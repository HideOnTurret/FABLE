{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQGQJapatIfF","outputId":"f6aa929a-a5aa-4a5b-b802-b2bd3fbf5693","executionInfo":{"status":"ok","timestamp":1739230241527,"user_tz":300,"elapsed":12319,"user":{"displayName":"Yue Deng","userId":"00346012823044562765"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: cartopy in /usr/local/lib/python3.11/dist-packages (0.24.1)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from cartopy) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.10.0)\n","Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.0.7)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from cartopy) (24.2)\n","Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.3.1)\n","Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (4.55.8)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (2.8.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=3.3.1->cartopy) (2025.1.31)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->cartopy) (1.17.0)\n","Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.12)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n","Requirement already satisfied: torchcde in /usr/local/lib/python3.11/dist-packages (0.2.5)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from torchcde) (2.5.1+cu124)\n","Requirement already satisfied: torchdiffeq>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from torchcde) (0.2.5)\n","Requirement already satisfied: torchsde>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from torchcde) (0.2.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchcde) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.0->torchcde) (1.3.0)\n","Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq>=0.2.0->torchcde) (1.13.1)\n","Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from torchsde>=0.2.5->torchcde) (1.26.4)\n","Requirement already satisfied: trampoline>=0.1.2 in /usr/local/lib/python3.11/dist-packages (from torchsde>=0.2.5->torchcde) (0.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.0->torchcde) (3.0.2)\n","Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (1.26.4)\n","Mounted at /content/drive\n"]}],"source":["# @title Env\n","!pip install cartopy\n","!pip install torch_geometric\n","!pip install torchcde\n","!pip install PyWavelets\n","\n","from google.colab import drive\n","\n","import pickle\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import argparse\n","import yaml\n","import torch\n","import os\n","import argparse\n","import yaml\n","from pathlib import Path\n","import json\n","import sys\n","import cartopy.crs as ccrs\n","import cartopy.feature as cfeature\n","import torch.nn.functional as F\n","from typing import Any, Callable, Tuple, Union\n","from torch import nn, optim\n","from torch.optim import Adam\n","import networkx as nx\n","import random\n","from torch.autograd import Variable\n","import gc\n","import torchcde\n","import math\n","from statsmodels.tsa.seasonal import STL\n","from tqdm import tqdm\n","import pickle\n","import logging\n","import time\n","import pywt\n","\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir('/content/drive/My Drive/CLCRN-main')\n","\n","# !pip install torch_scatter\n","# !mkdir -p /content/drive/MyDrive/colab_packages\n","# !cp -r /usr/local/lib/python3.*/dist-packages/torch_scatter /content/drive/MyDrive/colab_packages/\n","!cp -r /content/drive/MyDrive/colab_packages/torch_scatter /usr/local/lib/python3.*/dist-packages/  # Wheels have been built and saved in 'My Drive/colab_packages'\n","\n","from supervisor import Supervisor\n","\n","dataName = 'tmp2m'\n","dim = 0\n","if dataName == 'tmp2m':\n","    mean = 2.839279e+02\n","    std = 3.833667e+01\n","    max_std = (3.145732e+02 - mean) / std\n","    min_std = (2.325919e+02 - mean) / std\n","    p99 = (304.43316650390625 - mean) / std\n","    p95 = (301.278076171875 - mean) / std\n","    p90 = (298.9938659667969 - mean) / std\n","    p75 = (2.934141e+02 - mean) / std\n","    p50 = (2.853937e+02 - mean)/std\n","    p25 = (2.754702e+02 - mean) / std\n","\n","## Specify configuration for adversatial target construction\n","caseStudy = 0\n","\n","## Specify configuration for FABLE\n","padding = 'values'                                                              # ['values']\n","stepNum = 500\n","implementations = 1\n","clampEpsilon = 2.5                                                              # [0.2, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n","\n","lr = 0.01\n","batch_size = 48\n","seq_length = 12\n","H = 40\n","W = 30\n","device = 'cuda'\n","visualize = 1\n","test_sample_num = 0\n","use_optimizer = 'Adam' # ['Adam', 'PGD', 'pure']\n","reg = 1\n","\n","wavelet_decomposition_dimension = 3\n","if wavelet_decomposition_dimension == 2:\n","    target_key = ['ad', 'da', 'dd']                                             # For 2d transform, ['aa', 'ad', 'da', 'dd']\n","    if reg == 1:\n","        coeff_weights = {'LH': 0.3, 'HL': 0.3,'HH': 0.5}\n","        reg_p = 2\n","        lambda_reg = 0.000005\n","elif wavelet_decomposition_dimension == 3:\n","    target_key = ['LLH', 'LHL', 'LHH', 'HLL', 'HLH', 'HHL', 'HHH']                     # For 3d transform, ['LLL', 'LLH', 'LHL', 'LHH', 'HLL', 'HLH', 'HHL', 'HHH']\n","    if reg == 1:\n","        # coeff_weights = {'LLH': 0.9, 'LHL': 0.9, 'LHH': 0.5, 'HLH': 0.5, 'HHL': 0.5, 'HHH': 0.3}\n","        coeff_weights = {'LLH': 0.8, 'LHL': 0.8,  'HLL': 0.8, 'LHH': 0.5, 'HLH': 0.5, 'HHL': 0.5, 'HHH': 0.2}\n","        reg_p = 2\n","        lambda_reg = 0.000003\n","\n","fix = 'ablation_1'\n","\n","best_CLCRN_model_epoch = 84"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1I4TjebcEZzs"},"outputs":[],"source":["# @title FABLE: Based on 2d Wavelet Transform\n","!pip install PyWavelets\n","\n","import torch\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import gc\n","import yaml\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pywt\n","import pandas as pd\n","import pickle\n","import torch\n","from matplotlib.colors import LinearSegmentedColormap\n","from matplotlib.ticker import MaxNLocator\n","import cartopy.crs as ccrs\n","import cartopy.feature as cfeature\n","\n","def wavelet_transform_3d(data_3d, wavelet='haar'):\n","    \"\"\"\n","    data_3d: shape (Time, H, W)\n","    Returns a dict with keys {'aa','ad','da','dd'} after single-level transform.\n","    \"\"\"\n","    coeffs = pywt.dwtn(data_3d, wavelet=wavelet, axes=(1, 2))\n","    return coeffs\n","\n","def wavelet_inverse_transform_3d(coeffs, wavelet='haar'):\n","    \"\"\"\n","    Inverse of wavelet_transform_3d. Returns shape (Time, H, W).\n","    \"\"\"\n","    reconstructed_data = pywt.idwtn(coeffs, wavelet=wavelet, axes=(1, 2))\n","    return reconstructed_data\n","\n","class FABLE:\n","    def __init__(self, forward_func, wavelet, level, lons, lats):\n","        self.forward_func = forward_func\n","        self.wavelet = wavelet\n","        self.level = level\n","        self.lons = lons\n","        self.lats = lats\n","\n","        self.lr = lr\n","\n","    def rmse(self, pred, target):\n","        loss = F.mse_loss(pred, target)\n","        return torch.sqrt(loss)\n","\n","    def wavelet_decompose_3d(self, data_3d):\n","        return wavelet_transform_3d(data_3d, wavelet=self.wavelet)\n","\n","    def wavelet_reconstruct_3d(self, coeffs_dict):\n","        return wavelet_inverse_transform_3d(coeffs_dict, wavelet=self.wavelet)\n","\n","    def create_regular_padded(self, lons, lats, values, fill_value=0.0):\n","        unique_lons = sorted(list(set(lons)))\n","        unique_lats = sorted(list(set(lats)))\n","        W = len(unique_lons)\n","        H = len(unique_lats)\n","\n","        padded_data = np.full((H, W), fill_value, dtype=np.float32)             # '(0,0)' start from 'left-bottom' in the padded rectangle\n","        lon_to_x = {val: idx for idx, val in enumerate(unique_lons)}\n","        lat_to_y = {val: idx for idx, val in enumerate(unique_lats)}\n","\n","        for lon, lat, val in zip(lons, lats, values):\n","            ix = lon_to_x[lon]\n","            iy = lat_to_y[lat]\n","            padded_data[iy, ix] = val                                           # So, it should be (iy, ix) since '(0,0)', which starts from 'left-bottom' in the padded rectangle, should be converted to start from 'left-top'\n","\n","        return padded_data, unique_lons, unique_lats\n","\n","    def _forward_loss_given_params(\n","        self, param_vector,\n","        original_inputs,\n","        x_indices, y_indices,\n","        batch_size, seq_length,\n","        clampEpsilon, mean, std,\n","        target_torch, device\n","    ):\n","        \"\"\"\n","        Directly reconstruct X from param_vector and compute the loss with optimized unpad operation.\n","        \"\"\"\n","\n","        S1 = param_vector[:,:,0,:,:] + param_vector[:,:,1,:,:]\n","        S2 = param_vector[:,:,2,:,:] + param_vector[:,:,3,:,:]\n","        D1 = param_vector[:,:,0,:,:] - param_vector[:,:,1,:,:]\n","        D2 = param_vector[:,:,2,:,:] - param_vector[:,:,3,:,:]\n","\n","        reconstructed_inputs = torch.zeros(\n","            (batch_size, seq_length, param_vector[:,:,0,:,:].shape[2]*2, param_vector[:,:,0,:,:].shape[3]*2),\n","            device=param_vector.device,\n","            dtype=param_vector.dtype\n","        )\n","\n","        reconstructed_inputs[:, :, ::2, ::2]   = (S1 + S2) / 2.0\n","        reconstructed_inputs[:, :, 1::2, ::2]  = (S1 - S2) / 2.0\n","        reconstructed_inputs[:, :, ::2, 1::2]  = (D1 + D2) / 2.0\n","        reconstructed_inputs[:, :, 1::2, 1::2] = (D1 - D2) / 2.0\n","\n","        # Restore the input with valid locations\n","        reconstructed_inputs = reconstructed_inputs[:, :, y_indices, x_indices].unsqueeze(-1).permute(1, 0, 2, 3)\n","\n","        # Clip the reconstructed inputs to be within bounds\n","        lower_bound = torch.maximum(original_inputs - clampEpsilon, torch.tensor(min_std, device = original_inputs.device))\n","        upper_bound = torch.minimum(original_inputs + clampEpsilon, torch.tensor(max_std, device = original_inputs.device))\n","        perturbed_inputs = torch.clamp(reconstructed_inputs, lower_bound, upper_bound)\n","\n","        # Forward pass and calculate loss\n","        perturbed_inputs = perturbed_inputs.to(device)\n","        forecast = self.forward_func(perturbed_inputs)  # Shape: (seq_length, batch_size, ...)\n","        loss_val = self.rmse(forecast, target_torch)\n","\n","        return perturbed_inputs, loss_val\n","\n","    def calculate_regularization_loss(self, perturbed_param_vector, original_param_vector, coeff_weights):\n","        reg_loss = 0\n","        key_to_index = {'LL':0, 'LH': 1, 'HL': 2, 'HH': 3}                      # Coefficients indices\n","        for key, weight in coeff_weights.items():\n","            index = key_to_index[key]\n","            diff = (perturbed_param_vector[:, :, index, :, :] - original_param_vector[:, :, index, :, :])\n","            reg_loss += weight * torch.norm(diff, p=reg_p)\n","\n","        return reg_loss\n","\n","    def perturb(self, inputs, target, clampEpsilon, step_num, mean, std):\n","        \"\"\"\n","        Perturbs the inputs based on gradients computed on wavelet coefficients.\n","        inputs.shape: (seq_length, batch_size, locations, 1)\n","        \"\"\"\n","        device = inputs.device\n","        self.forward_func.eval()\n","\n","        seq_length, batch_size, locations, variables = inputs.shape\n","\n","        ulonlat_record = []\n","        padded_data_batch = []\n","\n","        # Loop over each batch to process all time steps\n","        for b_idx in range(batch_size):\n","            time_step_padded = []\n","            time_step_ulonlat = []\n","            for t in range(seq_length):                                         # Process each time step\n","                data_1d = inputs[t, b_idx, :, 0].detach().cpu().numpy()         # Extract (locations,) for this time step and batch\n","                padded_data, unique_lons, unique_lats = self.create_regular_padded(self.lons, self.lats, data_1d, fill_value=0.0)\n","                time_step_padded.append(padded_data)                            # Store padded data for this time step\n","                time_step_ulonlat.append((unique_lons, unique_lats))            # Record corresponding lon/lat info\n","            padded_data_batch.append(np.stack(time_step_padded, axis=0))        # Shape: (seq_length, H, W) for this batch\n","            ulonlat_record.append(time_step_ulonlat)\n","\n","        padded_data_batch = np.stack(padded_data_batch, axis=0)                 # Shape: (batch_size, seq_length, H, W)\n","\n","        # Record the valid indices in the tensor 'padded_data_batch'\n","        unique_lons, unique_lats = ulonlat_record[0][0]                         # Use mapping from the first batch and time step\n","        lon_to_x = {val: idx for idx, val in enumerate(unique_lons)}\n","        lat_to_y = {val: idx for idx, val in enumerate(unique_lats)}\n","        valid_indices = [\n","            (lon_to_x[lon], lat_to_y[lat])\n","            for lon, lat in zip(self.lons, self.lats)\n","            if lon in lon_to_x and lat in lat_to_y\n","        ]\n","        x_indices, y_indices = zip(*valid_indices)\n","        x_indices = torch.tensor(x_indices, dtype=torch.long, device=device)\n","        y_indices = torch.tensor(y_indices, dtype=torch.long, device=device)\n","\n","        # 'pywt.dwtn' can implement Wavelet Transform along side any dimensions, where here axes = (2, 3) corresponds to the dimensions of H and W for each (batch, seq_length)\n","        coeffs_list = pywt.dwtn(padded_data_batch, wavelet = 'Haar', axes = (2, 3))  # coeffs_list['xx'].shape: (32, 7, 14, 29)\n","\n","        # Create param_vector directly as a tensor of shape (batch_size, seq_length, 4, H/2, W/2), here, e.g., torch.Size([32, 7, 4, 14, 29])\n","        # //MARK (yue, 1/18/2025): the 'param_vector' is correct as it can be used to reconstruct the Original X, as I tested\n","        param_vector = torch.zeros((batch_size, seq_length, len(coeffs_list), coeffs_list['aa'].shape[2], coeffs_list['aa'].shape[3]), dtype=torch.float32, device=device)\n","        param_vector[:,:,0,:,:] = torch.tensor(coeffs_list['aa'], dtype=torch.float32).detach()\n","        param_vector[:,:,1,:,:] = torch.tensor(coeffs_list['ad'], dtype=torch.float32).detach()\n","        param_vector[:,:,2,:,:] = torch.tensor(coeffs_list['da'], dtype=torch.float32).detach()\n","        param_vector[:,:,3,:,:] = torch.tensor(coeffs_list['dd'], dtype=torch.float32).detach()\n","\n","        perturbed_param_vector = param_vector.detach().clone()\n","        perturbed_param_vector.requires_grad_(True)\n","        target_torch = target.clone().detach().to(device)\n","\n","        key_to_index = {'aa': 0, 'ad': 1, 'da': 2, 'dd': 3}\n","        target_indices = [key_to_index[key] for key in target_key]\n","        non_target_indices = [i for i in range(4) if i not in target_indices]\n","        unchanged_param = param_vector[:, :, non_target_indices, :, :].detach().clone()\n","\n","        if use_optimizer == 'Adam':\n","            optimizer = torch.optim.Adam([perturbed_param_vector], lr = lr)\n","\n","        best_loss = float('inf')\n","        for epoch in range(step_num):\n","            optimizer.zero_grad()\n","\n","            perturbed_inputs, loss = self._forward_loss_given_params(\n","                perturbed_param_vector,\n","                inputs,\n","                x_indices, y_indices,\n","                batch_size=batch_size,\n","                seq_length=seq_length,\n","                clampEpsilon=clampEpsilon,\n","                mean=mean,\n","                std=std,\n","                target_torch=target_torch,\n","                device=device\n","            )\n","\n","            ### regularizer\n","            if reg == 1:\n","                reg_loss = self.calculate_regularization_loss(perturbed_param_vector, param_vector, coeff_weights)\n","                total_loss = loss + lambda_reg * reg_loss\n","                print(f\"Epoch {epoch} | Total Loss: {total_loss.item():.6f}, Loss: {loss.item():.6f}, reg_loss: {reg_loss:.6f}\")\n","                total_loss.backward()\n","            else:\n","                print(f\"Epoch {epoch} | Loss: {loss.item():.6f}\")\n","                loss.backward()\n","\n","            if perturbed_param_vector.grad is not None:\n","                perturbed_param_vector.grad[:, :, non_target_indices, :, :] = 0\n","\n","            optimizer.step()\n","\n","            with torch.no_grad():\n","                perturbed_param_vector[:, :, non_target_indices, :, :] = unchanged_param\n","\n","            # print(torch.sum((perturbed_param_vector - param_vector)== 0).item() / param_vector.numel())\n","\n","            if loss.item() < best_loss:\n","                best_loss = loss.item()\n","                best_perturbed_inputs = perturbed_inputs\n","\n","\n","        return best_perturbed_inputs\n","\n","def main():\n","    lonlatAddr = \"data/tmp2m/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","    with open(\"experiments/config_clcrn_tmp2m.yaml\", 'r') as f:\n","        clcrnConfig = yaml.load(f, Loader=yaml.SafeLoader)\n","    clcrnModelTrained = Supervisor(**clcrnConfig)\n","    clcrnModelTrained.load_model(best_CLCRN_model_epoch)\n","    valIterator = clcrnModelTrained._data['{}_loader'.format('test')]\n","\n","    fable = FABLE(forward_func=clcrnModelTrained.model, wavelet='haar', level=1, lons=lons, lats=lats)\n","\n","    data_list = []\n","    breakFlag = 0\n","    for N in range(implementations):\n","        print(f'Random Implementation {N}')\n","        if caseStudy == 0:\n","            predList = torch.load(f'predList_tmp2m_{N}.pth')\n","            targetList = torch.load(f'targetList_tmp2m_{N}.pth')\n","        elif caseStudy == 1:\n","            predList = torch.load('case_predList_tmp2m.pth')\n","            targetList = torch.load('case_targetList_tmp2m.pth')\n","\n","        for idx, (x, y) in enumerate(valIterator):\n","            if x.shape[0] != batch_size:\n","                break\n","\n","            print(idx)\n","            # if idx == 1:\n","            #     break\n","\n","            thisX = x.permute(1, 0, 2, 3).to('cuda')\n","            clcrnModelTrained.model.eval()\n","\n","            yPred = predList[idx].permute(1, 0, 2, 3).to('cuda')\n","            yTarget = targetList[idx].permute(1, 0, 2, 3).to('cuda')\n","\n","            xPerturbed = fable.perturb(thisX, yTarget, clampEpsilon, stepNum, mean, std)  # thisX.shape: (time_step, batch_size, locations, 1), e.g., [7, 32, 1320, 1]\n","            yPerturbed = clcrnModelTrained.model(xPerturbed.to('cuda'))\n","\n","            for b in range(x.shape[0]):\n","                data_list.append({\n","                    \"thisX\": thisX[:, b, :, :].detach().cpu().numpy().tolist(),\n","                    \"yPred\": yPred[:, b, :, :].detach().cpu().numpy().tolist(),\n","                    \"yTarget\": yTarget[:, b, :, :].detach().cpu().numpy().tolist(),\n","                    \"xPerturbed\": xPerturbed[:, b, :, :].detach().cpu().numpy().tolist(),\n","                    \"yPerturbed\": yPerturbed[:, b, :, :].detach().cpu().numpy().tolist(),\n","                })\n","\n","            df = pd.DataFrame(data_list)\n","            if caseStudy == 0:\n","                df.to_csv(f\"results/FABLE_{dataName}_{clampEpsilon}_{stepNum}_2d_{fix}_{implementations}.csv\",index=False)\n","            else:\n","                df.to_csv(f\"results/FABLE_{dataName}_{clampEpsilon}_{stepNum}_2d_{fix}_caseStudy_{implementations}.csv\", index=False)\n","\n","            if caseStudy == 1:\n","                testBatchNum = 0\n","                if breakFlag == testBatchNum:\n","                    break\n","                breakFlag += 1\n","\n","            del thisX, yPred, yTarget, xPerturbed, yPerturbed\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":734442,"status":"ok","timestamp":1739230992506,"user":{"displayName":"Yue Deng","userId":"00346012823044562765"},"user_tz":300},"id":"gkUM2YfA3o_j","outputId":"2afcc1c8-7299-4aa6-890b-7090c9ccbcf6","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (1.26.4)\n","Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (1.26.4)\n"]},{"output_type":"stream","name":"stderr","text":["--- Logging error ---\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n","    self.flush()\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n","    self.stream.flush()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Call stack:\n"]},{"output_type":"stream","name":"stdout","text":["2025-02-10 23:31:02,742 - INFO - Log directory: experiments/tmp2m/tmp2m\n"]},{"output_type":"stream","name":"stderr","text":["  File \"<frozen runpy>\", line 198, in _run_module_as_main\n","  File \"<frozen runpy>\", line 88, in _run_code\n","  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n","    ColabKernelApp.launch_instance()\n","  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n","    self.io_loop.start()\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n","    self.asyncio_loop.run_forever()\n","  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n","    self._run_once()\n","  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n","    handle._run()\n","  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n","    self._context.run(self._callback, *self._args)\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 699, in <lambda>\n","    lambda f: self._run_callback(functools.partial(callback, f))\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 750, in _run_callback\n","    ret = callback()\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 824, in inner\n","    self.ctx_run(self.run)\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 785, in run\n","    yielded = self.gen.send(value)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n","    yield gen.maybe_future(dispatch(*args))\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n","    yielded = ctx_run(next, result)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n","    yield gen.maybe_future(handler(stream, idents, msg))\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n","    yielded = ctx_run(next, result)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n","    self.do_execute(\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n","    yielded = ctx_run(next, result)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n","    result = self._run_cell(\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-5-b13bbb68249f>\", line 367, in <cell line: 0>\n","    main()\n","  File \"<ipython-input-5-b13bbb68249f>\", line 299, in main\n","    clcrnModelTrained = Supervisor(**clcrnConfig)\n","  File \"/content/drive/MyDrive/CLCRN-main/supervisor.py\", line 37, in __init__\n","    self._logger = utils.get_logger(self._log_dir, __name__, 'info.log', level=log_level)\n","  File \"/content/drive/MyDrive/CLCRN-main/lib/utils.py\", line 104, in get_logger\n","    logger.info('Log directory: %s', log_dir)\n","Message: 'Log directory: %s'\n","Arguments: (PosixPath('experiments/tmp2m/tmp2m'),)\n"]},{"output_type":"stream","name":"stdout","text":["2025-02-10 23:31:02,742 - INFO - Log directory: experiments/tmp2m/tmp2m\n"]},{"output_type":"stream","name":"stderr","text":["INFO:supervisor:Log directory: experiments/tmp2m/tmp2m\n"]},{"output_type":"stream","name":"stdout","text":["loading data of trn set...\n","loading data of val set...\n","loading data of test set...\n","2025-02-10 23:31:07,743 - INFO - Model created\n","2025-02-10 23:31:07,743 - INFO - Model created\n"]},{"output_type":"stream","name":"stderr","text":["--- Logging error ---\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n","    self.flush()\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n","    self.stream.flush()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Call stack:\n","  File \"<frozen runpy>\", line 198, in _run_module_as_main\n","  File \"<frozen runpy>\", line 88, in _run_code\n","  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n","    ColabKernelApp.launch_instance()\n","  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n","    self.io_loop.start()\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n","    self.asyncio_loop.run_forever()\n","  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n","    self._run_once()\n","  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n","    handle._run()\n","  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n","    self._context.run(self._callback, *self._args)\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 699, in <lambda>\n","    lambda f: self._run_callback(functools.partial(callback, f))\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 750, in _run_callback\n","    ret = callback()\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 824, in inner\n","    self.ctx_run(self.run)\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 785, in run\n","    yielded = self.gen.send(value)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n","    yield gen.maybe_future(dispatch(*args))\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n","    yielded = ctx_run(next, result)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n","    yield gen.maybe_future(handler(stream, idents, msg))\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n","    yielded = ctx_run(next, result)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n","    self.do_execute(\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n","    yielded = ctx_run(next, result)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n","    result = self._run_cell(\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-5-b13bbb68249f>\", line 367, in <cell line: 0>\n","    main()\n","  File \"<ipython-input-5-b13bbb68249f>\", line 299, in main\n","    clcrnModelTrained = Supervisor(**clcrnConfig)\n","  File \"/content/drive/MyDrive/CLCRN-main/supervisor.py\", line 92, in __init__\n","    self._logger.info(\"Model created\")\n","Message: 'Model created'\n","Arguments: ()\n","INFO:supervisor:Model created\n"]},{"output_type":"stream","name":"stdout","text":["model loaded from:  experiments/tmp2m/tmp2m/saved_model\n","2025-02-10 23:31:08,246 - INFO - Loaded model at 84\n","2025-02-10 23:31:08,246 - INFO - Loaded model at 84\n"]},{"output_type":"stream","name":"stderr","text":["--- Logging error ---\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n","    self.flush()\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n","    self.stream.flush()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Call stack:\n","  File \"<frozen runpy>\", line 198, in _run_module_as_main\n","  File \"<frozen runpy>\", line 88, in _run_code\n","  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n","    ColabKernelApp.launch_instance()\n","  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n","    self.io_loop.start()\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n","    self.asyncio_loop.run_forever()\n","  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n","    self._run_once()\n","  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n","    handle._run()\n","  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n","    self._context.run(self._callback, *self._args)\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 699, in <lambda>\n","    lambda f: self._run_callback(functools.partial(callback, f))\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 750, in _run_callback\n","    ret = callback()\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 824, in inner\n","    self.ctx_run(self.run)\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 785, in run\n","    yielded = self.gen.send(value)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n","    yield gen.maybe_future(dispatch(*args))\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n","    yielded = ctx_run(next, result)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n","    yield gen.maybe_future(handler(stream, idents, msg))\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n","    yielded = ctx_run(next, result)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n","    self.do_execute(\n","  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n","    yielded = ctx_run(next, result)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n","    result = self._run_cell(\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-5-b13bbb68249f>\", line 367, in <cell line: 0>\n","    main()\n","  File \"<ipython-input-5-b13bbb68249f>\", line 300, in main\n","    clcrnModelTrained.load_model(best_CLCRN_model_epoch)\n","  File \"/content/drive/MyDrive/CLCRN-main/supervisor.py\", line 126, in load_model\n","    self._logger.info(\"Loaded model at {}\".format(epoch_num))\n","Message: 'Loaded model at 84'\n","Arguments: ()\n","INFO:supervisor:Loaded model at 84\n"]},{"output_type":"stream","name":"stdout","text":["total implpementations: 1\n","Random Implementation 0\n","0\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-b13bbb68249f>:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  predList = torch.load(f'predList_tmp2m_{N}.pth')\n","<ipython-input-5-b13bbb68249f>:330: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  targetList = torch.load(f'targetList_tmp2m_{N}.pth')\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0 | Total Loss: 0.005281, Loss: 0.005281, reg_loss: 0.000000\n","Epoch 1 | Total Loss: 0.005910, Loss: 0.005898, reg_loss: 4.125345\n","Epoch 2 | Total Loss: 0.006391, Loss: 0.006369, reg_loss: 7.320412\n","Epoch 3 | Total Loss: 0.005314, Loss: 0.005287, reg_loss: 8.824853\n","Epoch 4 | Total Loss: 0.005543, Loss: 0.005513, reg_loss: 10.108199\n","Epoch 5 | Total Loss: 0.005257, Loss: 0.005225, reg_loss: 10.647511\n","Epoch 6 | Total Loss: 0.004539, Loss: 0.004507, reg_loss: 10.624006\n","Epoch 7 | Total Loss: 0.004272, Loss: 0.004240, reg_loss: 10.706548\n","Epoch 8 | Total Loss: 0.004295, Loss: 0.004261, reg_loss: 11.291541\n","Epoch 9 | Total Loss: 0.004028, Loss: 0.003991, reg_loss: 12.125437\n","Epoch 10 | Total Loss: 0.003667, Loss: 0.003628, reg_loss: 12.943228\n","Epoch 11 | Total Loss: 0.003527, Loss: 0.003486, reg_loss: 13.747566\n","Epoch 12 | Total Loss: 0.003400, Loss: 0.003356, reg_loss: 14.580169\n","Epoch 13 | Total Loss: 0.003171, Loss: 0.003125, reg_loss: 15.409531\n","Epoch 14 | Total Loss: 0.003027, Loss: 0.002978, reg_loss: 16.214762\n","Epoch 15 | Total Loss: 0.002930, Loss: 0.002879, reg_loss: 17.022478\n","Epoch 16 | Total Loss: 0.002762, Loss: 0.002709, reg_loss: 17.865128\n","Epoch 17 | Total Loss: 0.002630, Loss: 0.002574, reg_loss: 18.746128\n","Epoch 18 | Total Loss: 0.002550, Loss: 0.002491, reg_loss: 19.639513\n","Epoch 19 | Total Loss: 0.002444, Loss: 0.002382, reg_loss: 20.520731\n","Epoch 20 | Total Loss: 0.002355, Loss: 0.002290, reg_loss: 21.385242\n","Epoch 21 | Total Loss: 0.002274, Loss: 0.002207, reg_loss: 22.241974\n","Epoch 22 | Total Loss: 0.002190, Loss: 0.002120, reg_loss: 23.103916\n","Epoch 23 | Total Loss: 0.002134, Loss: 0.002062, reg_loss: 23.969057\n","Epoch 24 | Total Loss: 0.002074, Loss: 0.002000, reg_loss: 24.815830\n","Epoch 25 | Total Loss: 0.002012, Loss: 0.001935, reg_loss: 25.628189\n","Epoch 26 | Total Loss: 0.001961, Loss: 0.001882, reg_loss: 26.409590\n","Epoch 27 | Total Loss: 0.001916, Loss: 0.001834, reg_loss: 27.172417\n","Epoch 28 | Total Loss: 0.001878, Loss: 0.001794, reg_loss: 27.919041\n","Epoch 29 | Total Loss: 0.001838, Loss: 0.001752, reg_loss: 28.634041\n","Epoch 30 | Total Loss: 0.001802, Loss: 0.001714, reg_loss: 29.301962\n","Epoch 31 | Total Loss: 0.001774, Loss: 0.001684, reg_loss: 29.922501\n","Epoch 32 | Total Loss: 0.001747, Loss: 0.001655, reg_loss: 30.503639\n","Epoch 33 | Total Loss: 0.001721, Loss: 0.001628, reg_loss: 31.046621\n","Epoch 34 | Total Loss: 0.001699, Loss: 0.001605, reg_loss: 31.542889\n","Epoch 35 | Total Loss: 0.001680, Loss: 0.001584, reg_loss: 31.986273\n","Epoch 36 | Total Loss: 0.001662, Loss: 0.001565, reg_loss: 32.380203\n","Epoch 37 | Total Loss: 0.001646, Loss: 0.001548, reg_loss: 32.730072\n","Epoch 38 | Total Loss: 0.001632, Loss: 0.001533, reg_loss: 33.034389\n","Epoch 39 | Total Loss: 0.001618, Loss: 0.001519, reg_loss: 33.291180\n","Epoch 40 | Total Loss: 0.001606, Loss: 0.001506, reg_loss: 33.505268\n","Epoch 41 | Total Loss: 0.001596, Loss: 0.001494, reg_loss: 33.680637\n","Epoch 42 | Total Loss: 0.001586, Loss: 0.001485, reg_loss: 33.818336\n","Epoch 43 | Total Loss: 0.001576, Loss: 0.001475, reg_loss: 33.920750\n","Epoch 44 | Total Loss: 0.001567, Loss: 0.001465, reg_loss: 33.994522\n","Epoch 45 | Total Loss: 0.001557, Loss: 0.001455, reg_loss: 34.043194\n","Epoch 46 | Total Loss: 0.001549, Loss: 0.001447, reg_loss: 34.068272\n","Epoch 47 | Total Loss: 0.001540, Loss: 0.001438, reg_loss: 34.074383\n","Epoch 48 | Total Loss: 0.001533, Loss: 0.001430, reg_loss: 34.066902\n","Epoch 49 | Total Loss: 0.001525, Loss: 0.001422, reg_loss: 34.049068\n","Epoch 50 | Total Loss: 0.001518, Loss: 0.001416, reg_loss: 34.021687\n","Epoch 51 | Total Loss: 0.001513, Loss: 0.001411, reg_loss: 33.988522\n","Epoch 52 | Total Loss: 0.001506, Loss: 0.001404, reg_loss: 33.952759\n","Epoch 53 | Total Loss: 0.001499, Loss: 0.001397, reg_loss: 33.914936\n","Epoch 54 | Total Loss: 0.001495, Loss: 0.001394, reg_loss: 33.874973\n","Epoch 55 | Total Loss: 0.001489, Loss: 0.001388, reg_loss: 33.835186\n","Epoch 56 | Total Loss: 0.001484, Loss: 0.001382, reg_loss: 33.797169\n","Epoch 57 | Total Loss: 0.001481, Loss: 0.001379, reg_loss: 33.759815\n","Epoch 58 | Total Loss: 0.001476, Loss: 0.001375, reg_loss: 33.725224\n","Epoch 59 | Total Loss: 0.001472, Loss: 0.001371, reg_loss: 33.694157\n","Epoch 60 | Total Loss: 0.001468, Loss: 0.001367, reg_loss: 33.665676\n","Epoch 61 | Total Loss: 0.001466, Loss: 0.001365, reg_loss: 33.640110\n","Epoch 62 | Total Loss: 0.001462, Loss: 0.001361, reg_loss: 33.617439\n","Epoch 63 | Total Loss: 0.001458, Loss: 0.001357, reg_loss: 33.597290\n","Epoch 64 | Total Loss: 0.001455, Loss: 0.001354, reg_loss: 33.578835\n","Epoch 65 | Total Loss: 0.001452, Loss: 0.001351, reg_loss: 33.562019\n","Epoch 66 | Total Loss: 0.001448, Loss: 0.001348, reg_loss: 33.546616\n","Epoch 67 | Total Loss: 0.001445, Loss: 0.001345, reg_loss: 33.533077\n","Epoch 68 | Total Loss: 0.001443, Loss: 0.001343, reg_loss: 33.519680\n","Epoch 69 | Total Loss: 0.001441, Loss: 0.001341, reg_loss: 33.507061\n","Epoch 70 | Total Loss: 0.001438, Loss: 0.001338, reg_loss: 33.494587\n","Epoch 71 | Total Loss: 0.001435, Loss: 0.001335, reg_loss: 33.482567\n","Epoch 72 | Total Loss: 0.001433, Loss: 0.001333, reg_loss: 33.471008\n","Epoch 73 | Total Loss: 0.001431, Loss: 0.001331, reg_loss: 33.460491\n","Epoch 74 | Total Loss: 0.001429, Loss: 0.001328, reg_loss: 33.450329\n","Epoch 75 | Total Loss: 0.001426, Loss: 0.001326, reg_loss: 33.440693\n","Epoch 76 | Total Loss: 0.001424, Loss: 0.001324, reg_loss: 33.431671\n","Epoch 77 | Total Loss: 0.001422, Loss: 0.001322, reg_loss: 33.423836\n","Epoch 78 | Total Loss: 0.001420, Loss: 0.001320, reg_loss: 33.417358\n","Epoch 79 | Total Loss: 0.001419, Loss: 0.001319, reg_loss: 33.411827\n","Epoch 80 | Total Loss: 0.001418, Loss: 0.001317, reg_loss: 33.408039\n","Epoch 81 | Total Loss: 0.001416, Loss: 0.001316, reg_loss: 33.405140\n","Epoch 82 | Total Loss: 0.001414, Loss: 0.001314, reg_loss: 33.403088\n","Epoch 83 | Total Loss: 0.001413, Loss: 0.001313, reg_loss: 33.404102\n","Epoch 84 | Total Loss: 0.001412, Loss: 0.001312, reg_loss: 33.404819\n","Epoch 85 | Total Loss: 0.001411, Loss: 0.001311, reg_loss: 33.408386\n","Epoch 86 | Total Loss: 0.001411, Loss: 0.001310, reg_loss: 33.411629\n","Epoch 87 | Total Loss: 0.001411, Loss: 0.001311, reg_loss: 33.416908\n","Epoch 88 | Total Loss: 0.001410, Loss: 0.001310, reg_loss: 33.423321\n","Epoch 89 | Total Loss: 0.001407, Loss: 0.001307, reg_loss: 33.430332\n","Epoch 90 | Total Loss: 0.001405, Loss: 0.001305, reg_loss: 33.439957\n","Epoch 91 | Total Loss: 0.001406, Loss: 0.001305, reg_loss: 33.449532\n","Epoch 92 | Total Loss: 0.001407, Loss: 0.001306, reg_loss: 33.461628\n","Epoch 93 | Total Loss: 0.001406, Loss: 0.001306, reg_loss: 33.474312\n","Epoch 94 | Total Loss: 0.001405, Loss: 0.001305, reg_loss: 33.488758\n","Epoch 95 | Total Loss: 0.001406, Loss: 0.001305, reg_loss: 33.504215\n","Epoch 96 | Total Loss: 0.001407, Loss: 0.001306, reg_loss: 33.521137\n","Epoch 97 | Total Loss: 0.001406, Loss: 0.001306, reg_loss: 33.537785\n","Epoch 98 | Total Loss: 0.001405, Loss: 0.001305, reg_loss: 33.556244\n","Epoch 99 | Total Loss: 0.001404, Loss: 0.001303, reg_loss: 33.574863\n","Epoch 100 | Total Loss: 0.001403, Loss: 0.001302, reg_loss: 33.594265\n","Epoch 101 | Total Loss: 0.001402, Loss: 0.001301, reg_loss: 33.615261\n","Epoch 102 | Total Loss: 0.001401, Loss: 0.001301, reg_loss: 33.635044\n","Epoch 103 | Total Loss: 0.001399, Loss: 0.001298, reg_loss: 33.657467\n","Epoch 104 | Total Loss: 0.001396, Loss: 0.001295, reg_loss: 33.677326\n","Epoch 105 | Total Loss: 0.001395, Loss: 0.001294, reg_loss: 33.699924\n","Epoch 106 | Total Loss: 0.001396, Loss: 0.001295, reg_loss: 33.721245\n","Epoch 107 | Total Loss: 0.001397, Loss: 0.001296, reg_loss: 33.743984\n","Epoch 108 | Total Loss: 0.001396, Loss: 0.001295, reg_loss: 33.766384\n","Epoch 109 | Total Loss: 0.001395, Loss: 0.001294, reg_loss: 33.789795\n","Epoch 110 | Total Loss: 0.001393, Loss: 0.001292, reg_loss: 33.812729\n","Epoch 111 | Total Loss: 0.001391, Loss: 0.001289, reg_loss: 33.836735\n","Epoch 112 | Total Loss: 0.001391, Loss: 0.001289, reg_loss: 33.859474\n","Epoch 113 | Total Loss: 0.001392, Loss: 0.001290, reg_loss: 33.884411\n","Epoch 114 | Total Loss: 0.001391, Loss: 0.001289, reg_loss: 33.907532\n","Epoch 115 | Total Loss: 0.001389, Loss: 0.001287, reg_loss: 33.932579\n","Epoch 116 | Total Loss: 0.001389, Loss: 0.001287, reg_loss: 33.956741\n","Epoch 117 | Total Loss: 0.001391, Loss: 0.001289, reg_loss: 33.982197\n","Epoch 118 | Total Loss: 0.001394, Loss: 0.001292, reg_loss: 34.007362\n","Epoch 119 | Total Loss: 0.001394, Loss: 0.001292, reg_loss: 34.032757\n","Epoch 120 | Total Loss: 0.001391, Loss: 0.001289, reg_loss: 34.057850\n","Epoch 121 | Total Loss: 0.001387, Loss: 0.001285, reg_loss: 34.082897\n","Epoch 122 | Total Loss: 0.001385, Loss: 0.001283, reg_loss: 34.108223\n","Epoch 123 | Total Loss: 0.001385, Loss: 0.001282, reg_loss: 34.133625\n","Epoch 124 | Total Loss: 0.001385, Loss: 0.001283, reg_loss: 34.160248\n","Epoch 125 | Total Loss: 0.001385, Loss: 0.001283, reg_loss: 34.185165\n","Epoch 126 | Total Loss: 0.001383, Loss: 0.001280, reg_loss: 34.213608\n","Epoch 127 | Total Loss: 0.001379, Loss: 0.001276, reg_loss: 34.238235\n","Epoch 128 | Total Loss: 0.001376, Loss: 0.001274, reg_loss: 34.267246\n","Epoch 129 | Total Loss: 0.001377, Loss: 0.001274, reg_loss: 34.292789\n","Epoch 130 | Total Loss: 0.001378, Loss: 0.001275, reg_loss: 34.321392\n","Epoch 131 | Total Loss: 0.001377, Loss: 0.001274, reg_loss: 34.348408\n","Epoch 132 | Total Loss: 0.001375, Loss: 0.001272, reg_loss: 34.375507\n","Epoch 133 | Total Loss: 0.001375, Loss: 0.001272, reg_loss: 34.403984\n","Epoch 134 | Total Loss: 0.001376, Loss: 0.001273, reg_loss: 34.431252\n","Epoch 135 | Total Loss: 0.001379, Loss: 0.001276, reg_loss: 34.458717\n","Epoch 136 | Total Loss: 0.001379, Loss: 0.001276, reg_loss: 34.486691\n","Epoch 137 | Total Loss: 0.001378, Loss: 0.001274, reg_loss: 34.512505\n","Epoch 138 | Total Loss: 0.001378, Loss: 0.001274, reg_loss: 34.540096\n","Epoch 139 | Total Loss: 0.001379, Loss: 0.001275, reg_loss: 34.567169\n","Epoch 140 | Total Loss: 0.001379, Loss: 0.001275, reg_loss: 34.594185\n","Epoch 141 | Total Loss: 0.001378, Loss: 0.001274, reg_loss: 34.621845\n","Epoch 142 | Total Loss: 0.001379, Loss: 0.001275, reg_loss: 34.649529\n","Epoch 143 | Total Loss: 0.001379, Loss: 0.001275, reg_loss: 34.676041\n","Epoch 144 | Total Loss: 0.001372, Loss: 0.001268, reg_loss: 34.704567\n","Epoch 145 | Total Loss: 0.001365, Loss: 0.001261, reg_loss: 34.731163\n","Epoch 146 | Total Loss: 0.001365, Loss: 0.001260, reg_loss: 34.759377\n","Epoch 147 | Total Loss: 0.001366, Loss: 0.001262, reg_loss: 34.788292\n","Epoch 148 | Total Loss: 0.001365, Loss: 0.001260, reg_loss: 34.815536\n","Epoch 149 | Total Loss: 0.001364, Loss: 0.001259, reg_loss: 34.845772\n","Epoch 150 | Total Loss: 0.001365, Loss: 0.001261, reg_loss: 34.873459\n","Epoch 151 | Total Loss: 0.001367, Loss: 0.001262, reg_loss: 34.903263\n","Epoch 152 | Total Loss: 0.001366, Loss: 0.001262, reg_loss: 34.930939\n","Epoch 153 | Total Loss: 0.001366, Loss: 0.001262, reg_loss: 34.959858\n","Epoch 154 | Total Loss: 0.001367, Loss: 0.001262, reg_loss: 34.987045\n","Epoch 155 | Total Loss: 0.001367, Loss: 0.001262, reg_loss: 35.015331\n","Epoch 156 | Total Loss: 0.001365, Loss: 0.001260, reg_loss: 35.043819\n","Epoch 157 | Total Loss: 0.001364, Loss: 0.001259, reg_loss: 35.071922\n","Epoch 158 | Total Loss: 0.001362, Loss: 0.001257, reg_loss: 35.100594\n","Epoch 159 | Total Loss: 0.001358, Loss: 0.001253, reg_loss: 35.129700\n","Epoch 160 | Total Loss: 0.001356, Loss: 0.001251, reg_loss: 35.157585\n","Epoch 161 | Total Loss: 0.001358, Loss: 0.001252, reg_loss: 35.186985\n","Epoch 162 | Total Loss: 0.001360, Loss: 0.001254, reg_loss: 35.215687\n","Epoch 163 | Total Loss: 0.001359, Loss: 0.001253, reg_loss: 35.244705\n","Epoch 164 | Total Loss: 0.001355, Loss: 0.001249, reg_loss: 35.272850\n","Epoch 165 | Total Loss: 0.001351, Loss: 0.001246, reg_loss: 35.301003\n","Epoch 166 | Total Loss: 0.001352, Loss: 0.001246, reg_loss: 35.328365\n","Epoch 167 | Total Loss: 0.001353, Loss: 0.001247, reg_loss: 35.357170\n","Epoch 168 | Total Loss: 0.001354, Loss: 0.001248, reg_loss: 35.384026\n","Epoch 169 | Total Loss: 0.001353, Loss: 0.001247, reg_loss: 35.411270\n","Epoch 170 | Total Loss: 0.001352, Loss: 0.001246, reg_loss: 35.439514\n","Epoch 171 | Total Loss: 0.001352, Loss: 0.001246, reg_loss: 35.465393\n","Epoch 172 | Total Loss: 0.001354, Loss: 0.001248, reg_loss: 35.494183\n","Epoch 173 | Total Loss: 0.001358, Loss: 0.001251, reg_loss: 35.519909\n","Epoch 174 | Total Loss: 0.001360, Loss: 0.001254, reg_loss: 35.547871\n","Epoch 175 | Total Loss: 0.001360, Loss: 0.001253, reg_loss: 35.574459\n","Epoch 176 | Total Loss: 0.001359, Loss: 0.001252, reg_loss: 35.600777\n","Epoch 177 | Total Loss: 0.001358, Loss: 0.001251, reg_loss: 35.627232\n","Epoch 178 | Total Loss: 0.001356, Loss: 0.001249, reg_loss: 35.652821\n","Epoch 179 | Total Loss: 0.001351, Loss: 0.001244, reg_loss: 35.678936\n","Epoch 180 | Total Loss: 0.001348, Loss: 0.001241, reg_loss: 35.704491\n","Epoch 181 | Total Loss: 0.001348, Loss: 0.001240, reg_loss: 35.730545\n","Epoch 182 | Total Loss: 0.001349, Loss: 0.001242, reg_loss: 35.756390\n","Epoch 183 | Total Loss: 0.001350, Loss: 0.001243, reg_loss: 35.781731\n","Epoch 184 | Total Loss: 0.001351, Loss: 0.001244, reg_loss: 35.807041\n","Epoch 185 | Total Loss: 0.001351, Loss: 0.001243, reg_loss: 35.832325\n","Epoch 186 | Total Loss: 0.001347, Loss: 0.001240, reg_loss: 35.858150\n","Epoch 187 | Total Loss: 0.001340, Loss: 0.001233, reg_loss: 35.883385\n","Epoch 188 | Total Loss: 0.001337, Loss: 0.001229, reg_loss: 35.909565\n","Epoch 189 | Total Loss: 0.001340, Loss: 0.001232, reg_loss: 35.935551\n","Epoch 190 | Total Loss: 0.001346, Loss: 0.001238, reg_loss: 35.961929\n","Epoch 191 | Total Loss: 0.001349, Loss: 0.001241, reg_loss: 35.987900\n","Epoch 192 | Total Loss: 0.001349, Loss: 0.001240, reg_loss: 36.013275\n","Epoch 193 | Total Loss: 0.001348, Loss: 0.001240, reg_loss: 36.038223\n","Epoch 194 | Total Loss: 0.001345, Loss: 0.001237, reg_loss: 36.063000\n","Epoch 195 | Total Loss: 0.001340, Loss: 0.001232, reg_loss: 36.086845\n","Epoch 196 | Total Loss: 0.001338, Loss: 0.001230, reg_loss: 36.111843\n","Epoch 197 | Total Loss: 0.001342, Loss: 0.001233, reg_loss: 36.135899\n","Epoch 198 | Total Loss: 0.001345, Loss: 0.001237, reg_loss: 36.160789\n","Epoch 199 | Total Loss: 0.001343, Loss: 0.001234, reg_loss: 36.184601\n","Epoch 200 | Total Loss: 0.001338, Loss: 0.001230, reg_loss: 36.207790\n","Epoch 201 | Total Loss: 0.001339, Loss: 0.001230, reg_loss: 36.232365\n","Epoch 202 | Total Loss: 0.001342, Loss: 0.001233, reg_loss: 36.255009\n","Epoch 203 | Total Loss: 0.001341, Loss: 0.001232, reg_loss: 36.278431\n","Epoch 204 | Total Loss: 0.001337, Loss: 0.001228, reg_loss: 36.301952\n","Epoch 205 | Total Loss: 0.001333, Loss: 0.001224, reg_loss: 36.323830\n","Epoch 206 | Total Loss: 0.001333, Loss: 0.001224, reg_loss: 36.348846\n","Epoch 207 | Total Loss: 0.001335, Loss: 0.001226, reg_loss: 36.370785\n","Epoch 208 | Total Loss: 0.001336, Loss: 0.001227, reg_loss: 36.395454\n","Epoch 209 | Total Loss: 0.001335, Loss: 0.001226, reg_loss: 36.417656\n","Epoch 210 | Total Loss: 0.001334, Loss: 0.001224, reg_loss: 36.440910\n","Epoch 211 | Total Loss: 0.001333, Loss: 0.001224, reg_loss: 36.463200\n","Epoch 212 | Total Loss: 0.001332, Loss: 0.001223, reg_loss: 36.486160\n","Epoch 213 | Total Loss: 0.001331, Loss: 0.001222, reg_loss: 36.508125\n","Epoch 214 | Total Loss: 0.001330, Loss: 0.001221, reg_loss: 36.531239\n","Epoch 215 | Total Loss: 0.001331, Loss: 0.001221, reg_loss: 36.553562\n","Epoch 216 | Total Loss: 0.001333, Loss: 0.001223, reg_loss: 36.575455\n","Epoch 217 | Total Loss: 0.001336, Loss: 0.001226, reg_loss: 36.597858\n","Epoch 218 | Total Loss: 0.001339, Loss: 0.001229, reg_loss: 36.618057\n","Epoch 219 | Total Loss: 0.001339, Loss: 0.001229, reg_loss: 36.639893\n","Epoch 220 | Total Loss: 0.001337, Loss: 0.001227, reg_loss: 36.660061\n","Epoch 221 | Total Loss: 0.001335, Loss: 0.001225, reg_loss: 36.681377\n","Epoch 222 | Total Loss: 0.001334, Loss: 0.001224, reg_loss: 36.701897\n","Epoch 223 | Total Loss: 0.001332, Loss: 0.001222, reg_loss: 36.722927\n","Epoch 224 | Total Loss: 0.001328, Loss: 0.001218, reg_loss: 36.744129\n","Epoch 225 | Total Loss: 0.001325, Loss: 0.001215, reg_loss: 36.764912\n","Epoch 226 | Total Loss: 0.001325, Loss: 0.001215, reg_loss: 36.786896\n","Epoch 227 | Total Loss: 0.001328, Loss: 0.001217, reg_loss: 36.807732\n","Epoch 228 | Total Loss: 0.001329, Loss: 0.001219, reg_loss: 36.829834\n","Epoch 229 | Total Loss: 0.001327, Loss: 0.001217, reg_loss: 36.850727\n","Epoch 230 | Total Loss: 0.001325, Loss: 0.001214, reg_loss: 36.872971\n","Epoch 231 | Total Loss: 0.001327, Loss: 0.001216, reg_loss: 36.893497\n","Epoch 232 | Total Loss: 0.001332, Loss: 0.001221, reg_loss: 36.916573\n","Epoch 233 | Total Loss: 0.001337, Loss: 0.001227, reg_loss: 36.936138\n","Epoch 234 | Total Loss: 0.001338, Loss: 0.001227, reg_loss: 36.957981\n","Epoch 235 | Total Loss: 0.001332, Loss: 0.001221, reg_loss: 36.976692\n","Epoch 236 | Total Loss: 0.001325, Loss: 0.001214, reg_loss: 36.997356\n","Epoch 237 | Total Loss: 0.001323, Loss: 0.001212, reg_loss: 37.017517\n","Epoch 238 | Total Loss: 0.001327, Loss: 0.001216, reg_loss: 37.038551\n","Epoch 239 | Total Loss: 0.001327, Loss: 0.001216, reg_loss: 37.059273\n","Epoch 240 | Total Loss: 0.001320, Loss: 0.001209, reg_loss: 37.079517\n","Epoch 241 | Total Loss: 0.001315, Loss: 0.001204, reg_loss: 37.100052\n","Epoch 242 | Total Loss: 0.001318, Loss: 0.001207, reg_loss: 37.120594\n","Epoch 243 | Total Loss: 0.001322, Loss: 0.001211, reg_loss: 37.141754\n","Epoch 244 | Total Loss: 0.001320, Loss: 0.001209, reg_loss: 37.161602\n","Epoch 245 | Total Loss: 0.001316, Loss: 0.001205, reg_loss: 37.182110\n","Epoch 246 | Total Loss: 0.001317, Loss: 0.001205, reg_loss: 37.202465\n","Epoch 247 | Total Loss: 0.001321, Loss: 0.001209, reg_loss: 37.222916\n","Epoch 248 | Total Loss: 0.001323, Loss: 0.001211, reg_loss: 37.243881\n","Epoch 249 | Total Loss: 0.001322, Loss: 0.001210, reg_loss: 37.263840\n","Epoch 250 | Total Loss: 0.001323, Loss: 0.001211, reg_loss: 37.284092\n","Epoch 251 | Total Loss: 0.001328, Loss: 0.001216, reg_loss: 37.304207\n","Epoch 252 | Total Loss: 0.001332, Loss: 0.001220, reg_loss: 37.322510\n","Epoch 253 | Total Loss: 0.001331, Loss: 0.001219, reg_loss: 37.341961\n","Epoch 254 | Total Loss: 0.001328, Loss: 0.001216, reg_loss: 37.359528\n","Epoch 255 | Total Loss: 0.001327, Loss: 0.001215, reg_loss: 37.378036\n","Epoch 256 | Total Loss: 0.001326, Loss: 0.001214, reg_loss: 37.397171\n","Epoch 257 | Total Loss: 0.001323, Loss: 0.001211, reg_loss: 37.414787\n","Epoch 258 | Total Loss: 0.001319, Loss: 0.001207, reg_loss: 37.434608\n","Epoch 259 | Total Loss: 0.001316, Loss: 0.001204, reg_loss: 37.452549\n","Epoch 260 | Total Loss: 0.001315, Loss: 0.001203, reg_loss: 37.472145\n","Epoch 261 | Total Loss: 0.001315, Loss: 0.001203, reg_loss: 37.491226\n","Epoch 262 | Total Loss: 0.001316, Loss: 0.001203, reg_loss: 37.510063\n","Epoch 263 | Total Loss: 0.001317, Loss: 0.001204, reg_loss: 37.529575\n","Epoch 264 | Total Loss: 0.001319, Loss: 0.001207, reg_loss: 37.548000\n","Epoch 265 | Total Loss: 0.001321, Loss: 0.001209, reg_loss: 37.567406\n","Epoch 266 | Total Loss: 0.001320, Loss: 0.001208, reg_loss: 37.585888\n","Epoch 267 | Total Loss: 0.001318, Loss: 0.001205, reg_loss: 37.604755\n","Epoch 268 | Total Loss: 0.001316, Loss: 0.001203, reg_loss: 37.623123\n","Epoch 269 | Total Loss: 0.001315, Loss: 0.001202, reg_loss: 37.641827\n","Epoch 270 | Total Loss: 0.001314, Loss: 0.001201, reg_loss: 37.660530\n","Epoch 271 | Total Loss: 0.001313, Loss: 0.001200, reg_loss: 37.679535\n","Epoch 272 | Total Loss: 0.001312, Loss: 0.001199, reg_loss: 37.698505\n","Epoch 273 | Total Loss: 0.001314, Loss: 0.001201, reg_loss: 37.717278\n","Epoch 274 | Total Loss: 0.001318, Loss: 0.001205, reg_loss: 37.736393\n","Epoch 275 | Total Loss: 0.001321, Loss: 0.001207, reg_loss: 37.754574\n","Epoch 276 | Total Loss: 0.001320, Loss: 0.001207, reg_loss: 37.772869\n","Epoch 277 | Total Loss: 0.001319, Loss: 0.001206, reg_loss: 37.790531\n","Epoch 278 | Total Loss: 0.001320, Loss: 0.001207, reg_loss: 37.808525\n","Epoch 279 | Total Loss: 0.001323, Loss: 0.001210, reg_loss: 37.825764\n","Epoch 280 | Total Loss: 0.001324, Loss: 0.001210, reg_loss: 37.842770\n","Epoch 281 | Total Loss: 0.001319, Loss: 0.001206, reg_loss: 37.858913\n","Epoch 282 | Total Loss: 0.001315, Loss: 0.001201, reg_loss: 37.876953\n","Epoch 283 | Total Loss: 0.001314, Loss: 0.001201, reg_loss: 37.892921\n","Epoch 284 | Total Loss: 0.001317, Loss: 0.001203, reg_loss: 37.910969\n","Epoch 285 | Total Loss: 0.001318, Loss: 0.001205, reg_loss: 37.926346\n","Epoch 286 | Total Loss: 0.001317, Loss: 0.001203, reg_loss: 37.942959\n","Epoch 287 | Total Loss: 0.001315, Loss: 0.001201, reg_loss: 37.958771\n","Epoch 288 | Total Loss: 0.001314, Loss: 0.001200, reg_loss: 37.974056\n","Epoch 289 | Total Loss: 0.001318, Loss: 0.001204, reg_loss: 37.990658\n","Epoch 290 | Total Loss: 0.001320, Loss: 0.001206, reg_loss: 38.005653\n","Epoch 291 | Total Loss: 0.001318, Loss: 0.001204, reg_loss: 38.021797\n","Epoch 292 | Total Loss: 0.001313, Loss: 0.001199, reg_loss: 38.037319\n","Epoch 293 | Total Loss: 0.001313, Loss: 0.001199, reg_loss: 38.052544\n","Epoch 294 | Total Loss: 0.001316, Loss: 0.001202, reg_loss: 38.068283\n","Epoch 295 | Total Loss: 0.001317, Loss: 0.001202, reg_loss: 38.083721\n","Epoch 296 | Total Loss: 0.001314, Loss: 0.001200, reg_loss: 38.097733\n","Epoch 297 | Total Loss: 0.001313, Loss: 0.001198, reg_loss: 38.114128\n","Epoch 298 | Total Loss: 0.001314, Loss: 0.001200, reg_loss: 38.127357\n","Epoch 299 | Total Loss: 0.001314, Loss: 0.001200, reg_loss: 38.143661\n","Epoch 300 | Total Loss: 0.001313, Loss: 0.001198, reg_loss: 38.157795\n","Epoch 301 | Total Loss: 0.001313, Loss: 0.001198, reg_loss: 38.172405\n","Epoch 302 | Total Loss: 0.001314, Loss: 0.001200, reg_loss: 38.187508\n","Epoch 303 | Total Loss: 0.001315, Loss: 0.001201, reg_loss: 38.201508\n","Epoch 304 | Total Loss: 0.001314, Loss: 0.001199, reg_loss: 38.215515\n","Epoch 305 | Total Loss: 0.001313, Loss: 0.001198, reg_loss: 38.229904\n","Epoch 306 | Total Loss: 0.001312, Loss: 0.001197, reg_loss: 38.243904\n","Epoch 307 | Total Loss: 0.001310, Loss: 0.001196, reg_loss: 38.258347\n","Epoch 308 | Total Loss: 0.001310, Loss: 0.001195, reg_loss: 38.272751\n","Epoch 309 | Total Loss: 0.001312, Loss: 0.001197, reg_loss: 38.286037\n","Epoch 310 | Total Loss: 0.001315, Loss: 0.001200, reg_loss: 38.300060\n","Epoch 311 | Total Loss: 0.001315, Loss: 0.001200, reg_loss: 38.313087\n","Epoch 312 | Total Loss: 0.001314, Loss: 0.001199, reg_loss: 38.326370\n","Epoch 313 | Total Loss: 0.001313, Loss: 0.001198, reg_loss: 38.340485\n","Epoch 314 | Total Loss: 0.001315, Loss: 0.001200, reg_loss: 38.353046\n","Epoch 315 | Total Loss: 0.001317, Loss: 0.001201, reg_loss: 38.367378\n","Epoch 316 | Total Loss: 0.001314, Loss: 0.001199, reg_loss: 38.380402\n","Epoch 317 | Total Loss: 0.001310, Loss: 0.001195, reg_loss: 38.393349\n","Epoch 318 | Total Loss: 0.001308, Loss: 0.001193, reg_loss: 38.407104\n","Epoch 319 | Total Loss: 0.001308, Loss: 0.001192, reg_loss: 38.419762\n","Epoch 320 | Total Loss: 0.001306, Loss: 0.001191, reg_loss: 38.434090\n","Epoch 321 | Total Loss: 0.001304, Loss: 0.001188, reg_loss: 38.446892\n","Epoch 322 | Total Loss: 0.001302, Loss: 0.001187, reg_loss: 38.460804\n","Epoch 323 | Total Loss: 0.001302, Loss: 0.001187, reg_loss: 38.473686\n","Epoch 324 | Total Loss: 0.001302, Loss: 0.001187, reg_loss: 38.487831\n","Epoch 325 | Total Loss: 0.001301, Loss: 0.001186, reg_loss: 38.500931\n","Epoch 326 | Total Loss: 0.001300, Loss: 0.001184, reg_loss: 38.515343\n","Epoch 327 | Total Loss: 0.001301, Loss: 0.001185, reg_loss: 38.528839\n","Epoch 328 | Total Loss: 0.001304, Loss: 0.001189, reg_loss: 38.542850\n","Epoch 329 | Total Loss: 0.001307, Loss: 0.001192, reg_loss: 38.556171\n","Epoch 330 | Total Loss: 0.001307, Loss: 0.001192, reg_loss: 38.569729\n","Epoch 331 | Total Loss: 0.001306, Loss: 0.001190, reg_loss: 38.582409\n","Epoch 332 | Total Loss: 0.001306, Loss: 0.001190, reg_loss: 38.596310\n","Epoch 333 | Total Loss: 0.001308, Loss: 0.001192, reg_loss: 38.608917\n","Epoch 334 | Total Loss: 0.001310, Loss: 0.001194, reg_loss: 38.623013\n","Epoch 335 | Total Loss: 0.001311, Loss: 0.001195, reg_loss: 38.635002\n","Epoch 336 | Total Loss: 0.001310, Loss: 0.001194, reg_loss: 38.648270\n","Epoch 337 | Total Loss: 0.001306, Loss: 0.001190, reg_loss: 38.660278\n","Epoch 338 | Total Loss: 0.001302, Loss: 0.001186, reg_loss: 38.672184\n","Epoch 339 | Total Loss: 0.001301, Loss: 0.001185, reg_loss: 38.685032\n","Epoch 340 | Total Loss: 0.001304, Loss: 0.001188, reg_loss: 38.696594\n","Epoch 341 | Total Loss: 0.001307, Loss: 0.001191, reg_loss: 38.710487\n","Epoch 342 | Total Loss: 0.001307, Loss: 0.001190, reg_loss: 38.721535\n","Epoch 343 | Total Loss: 0.001304, Loss: 0.001187, reg_loss: 38.734989\n","Epoch 344 | Total Loss: 0.001301, Loss: 0.001185, reg_loss: 38.746178\n","Epoch 345 | Total Loss: 0.001300, Loss: 0.001183, reg_loss: 38.758339\n","Epoch 346 | Total Loss: 0.001298, Loss: 0.001182, reg_loss: 38.770248\n","Epoch 347 | Total Loss: 0.001297, Loss: 0.001181, reg_loss: 38.781345\n","Epoch 348 | Total Loss: 0.001298, Loss: 0.001182, reg_loss: 38.794250\n","Epoch 349 | Total Loss: 0.001300, Loss: 0.001183, reg_loss: 38.805908\n","Epoch 350 | Total Loss: 0.001299, Loss: 0.001183, reg_loss: 38.818596\n","Epoch 351 | Total Loss: 0.001297, Loss: 0.001181, reg_loss: 38.830639\n","Epoch 352 | Total Loss: 0.001297, Loss: 0.001180, reg_loss: 38.842617\n","Epoch 353 | Total Loss: 0.001298, Loss: 0.001182, reg_loss: 38.855045\n","Epoch 354 | Total Loss: 0.001302, Loss: 0.001185, reg_loss: 38.867298\n","Epoch 355 | Total Loss: 0.001305, Loss: 0.001188, reg_loss: 38.879677\n","Epoch 356 | Total Loss: 0.001307, Loss: 0.001191, reg_loss: 38.891434\n","Epoch 357 | Total Loss: 0.001310, Loss: 0.001193, reg_loss: 38.903202\n","Epoch 358 | Total Loss: 0.001310, Loss: 0.001193, reg_loss: 38.914207\n","Epoch 359 | Total Loss: 0.001307, Loss: 0.001190, reg_loss: 38.925316\n","Epoch 360 | Total Loss: 0.001302, Loss: 0.001186, reg_loss: 38.935795\n","Epoch 361 | Total Loss: 0.001299, Loss: 0.001182, reg_loss: 38.946873\n","Epoch 362 | Total Loss: 0.001298, Loss: 0.001181, reg_loss: 38.957119\n","Epoch 363 | Total Loss: 0.001299, Loss: 0.001182, reg_loss: 38.969173\n","Epoch 364 | Total Loss: 0.001301, Loss: 0.001184, reg_loss: 38.979294\n","Epoch 365 | Total Loss: 0.001303, Loss: 0.001186, reg_loss: 38.992104\n","Epoch 366 | Total Loss: 0.001304, Loss: 0.001187, reg_loss: 39.002071\n","Epoch 367 | Total Loss: 0.001305, Loss: 0.001188, reg_loss: 39.014938\n","Epoch 368 | Total Loss: 0.001304, Loss: 0.001187, reg_loss: 39.024429\n","Epoch 369 | Total Loss: 0.001302, Loss: 0.001185, reg_loss: 39.036320\n","Epoch 370 | Total Loss: 0.001302, Loss: 0.001185, reg_loss: 39.045963\n","Epoch 371 | Total Loss: 0.001305, Loss: 0.001188, reg_loss: 39.057411\n","Epoch 372 | Total Loss: 0.001309, Loss: 0.001192, reg_loss: 39.067310\n","Epoch 373 | Total Loss: 0.001310, Loss: 0.001192, reg_loss: 39.077827\n","Epoch 374 | Total Loss: 0.001305, Loss: 0.001188, reg_loss: 39.087749\n","Epoch 375 | Total Loss: 0.001300, Loss: 0.001182, reg_loss: 39.098232\n","Epoch 376 | Total Loss: 0.001295, Loss: 0.001177, reg_loss: 39.107941\n","Epoch 377 | Total Loss: 0.001292, Loss: 0.001175, reg_loss: 39.118301\n","Epoch 378 | Total Loss: 0.001292, Loss: 0.001174, reg_loss: 39.128716\n","Epoch 379 | Total Loss: 0.001295, Loss: 0.001178, reg_loss: 39.138329\n","Epoch 380 | Total Loss: 0.001300, Loss: 0.001183, reg_loss: 39.149773\n","Epoch 381 | Total Loss: 0.001305, Loss: 0.001187, reg_loss: 39.159286\n","Epoch 382 | Total Loss: 0.001307, Loss: 0.001189, reg_loss: 39.170300\n","Epoch 383 | Total Loss: 0.001308, Loss: 0.001190, reg_loss: 39.179668\n","Epoch 384 | Total Loss: 0.001310, Loss: 0.001193, reg_loss: 39.189575\n","Epoch 385 | Total Loss: 0.001312, Loss: 0.001195, reg_loss: 39.198727\n","Epoch 386 | Total Loss: 0.001311, Loss: 0.001193, reg_loss: 39.207832\n","Epoch 387 | Total Loss: 0.001305, Loss: 0.001187, reg_loss: 39.217438\n","Epoch 388 | Total Loss: 0.001299, Loss: 0.001181, reg_loss: 39.226738\n","Epoch 389 | Total Loss: 0.001298, Loss: 0.001180, reg_loss: 39.236820\n","Epoch 390 | Total Loss: 0.001303, Loss: 0.001185, reg_loss: 39.246559\n","Epoch 391 | Total Loss: 0.001307, Loss: 0.001190, reg_loss: 39.256592\n","Epoch 392 | Total Loss: 0.001307, Loss: 0.001189, reg_loss: 39.266270\n","Epoch 393 | Total Loss: 0.001304, Loss: 0.001186, reg_loss: 39.275970\n","Epoch 394 | Total Loss: 0.001303, Loss: 0.001185, reg_loss: 39.285664\n","Epoch 395 | Total Loss: 0.001305, Loss: 0.001187, reg_loss: 39.295372\n","Epoch 396 | Total Loss: 0.001303, Loss: 0.001185, reg_loss: 39.304363\n","Epoch 397 | Total Loss: 0.001297, Loss: 0.001179, reg_loss: 39.313328\n","Epoch 398 | Total Loss: 0.001292, Loss: 0.001174, reg_loss: 39.322140\n","Epoch 399 | Total Loss: 0.001293, Loss: 0.001175, reg_loss: 39.331673\n","Epoch 400 | Total Loss: 0.001299, Loss: 0.001181, reg_loss: 39.341015\n","Epoch 401 | Total Loss: 0.001303, Loss: 0.001185, reg_loss: 39.351646\n","Epoch 402 | Total Loss: 0.001303, Loss: 0.001184, reg_loss: 39.360931\n","Epoch 403 | Total Loss: 0.001299, Loss: 0.001181, reg_loss: 39.370949\n","Epoch 404 | Total Loss: 0.001297, Loss: 0.001179, reg_loss: 39.380791\n","Epoch 405 | Total Loss: 0.001298, Loss: 0.001180, reg_loss: 39.390118\n","Epoch 406 | Total Loss: 0.001299, Loss: 0.001181, reg_loss: 39.400246\n","Epoch 407 | Total Loss: 0.001300, Loss: 0.001182, reg_loss: 39.409508\n","Epoch 408 | Total Loss: 0.001299, Loss: 0.001181, reg_loss: 39.419380\n","Epoch 409 | Total Loss: 0.001298, Loss: 0.001179, reg_loss: 39.430199\n","Epoch 410 | Total Loss: 0.001298, Loss: 0.001179, reg_loss: 39.439899\n","Epoch 411 | Total Loss: 0.001298, Loss: 0.001180, reg_loss: 39.451256\n","Epoch 412 | Total Loss: 0.001299, Loss: 0.001181, reg_loss: 39.460686\n","Epoch 413 | Total Loss: 0.001298, Loss: 0.001180, reg_loss: 39.471249\n","Epoch 414 | Total Loss: 0.001296, Loss: 0.001178, reg_loss: 39.480694\n","Epoch 415 | Total Loss: 0.001296, Loss: 0.001177, reg_loss: 39.491825\n","Epoch 416 | Total Loss: 0.001298, Loss: 0.001180, reg_loss: 39.501163\n","Epoch 417 | Total Loss: 0.001302, Loss: 0.001183, reg_loss: 39.512585\n","Epoch 418 | Total Loss: 0.001302, Loss: 0.001184, reg_loss: 39.521420\n","Epoch 419 | Total Loss: 0.001299, Loss: 0.001180, reg_loss: 39.532246\n","Epoch 420 | Total Loss: 0.001295, Loss: 0.001177, reg_loss: 39.540897\n","Epoch 421 | Total Loss: 0.001295, Loss: 0.001176, reg_loss: 39.551704\n","Epoch 422 | Total Loss: 0.001296, Loss: 0.001177, reg_loss: 39.560219\n","Epoch 423 | Total Loss: 0.001297, Loss: 0.001178, reg_loss: 39.570496\n","Epoch 424 | Total Loss: 0.001296, Loss: 0.001178, reg_loss: 39.578724\n","Epoch 425 | Total Loss: 0.001294, Loss: 0.001176, reg_loss: 39.588604\n","Epoch 426 | Total Loss: 0.001293, Loss: 0.001174, reg_loss: 39.597664\n","Epoch 427 | Total Loss: 0.001291, Loss: 0.001172, reg_loss: 39.606838\n","Epoch 428 | Total Loss: 0.001290, Loss: 0.001171, reg_loss: 39.617195\n","Epoch 429 | Total Loss: 0.001289, Loss: 0.001171, reg_loss: 39.626667\n","Epoch 430 | Total Loss: 0.001290, Loss: 0.001171, reg_loss: 39.638363\n","Epoch 431 | Total Loss: 0.001292, Loss: 0.001173, reg_loss: 39.648483\n","Epoch 432 | Total Loss: 0.001294, Loss: 0.001175, reg_loss: 39.658714\n","Epoch 433 | Total Loss: 0.001295, Loss: 0.001176, reg_loss: 39.669289\n","Epoch 434 | Total Loss: 0.001295, Loss: 0.001176, reg_loss: 39.678303\n","Epoch 435 | Total Loss: 0.001294, Loss: 0.001174, reg_loss: 39.689171\n","Epoch 436 | Total Loss: 0.001292, Loss: 0.001173, reg_loss: 39.697273\n","Epoch 437 | Total Loss: 0.001290, Loss: 0.001171, reg_loss: 39.708004\n","Epoch 438 | Total Loss: 0.001289, Loss: 0.001170, reg_loss: 39.716541\n","Epoch 439 | Total Loss: 0.001289, Loss: 0.001170, reg_loss: 39.726528\n","Epoch 440 | Total Loss: 0.001289, Loss: 0.001169, reg_loss: 39.737011\n","Epoch 441 | Total Loss: 0.001288, Loss: 0.001169, reg_loss: 39.747082\n","Epoch 442 | Total Loss: 0.001288, Loss: 0.001169, reg_loss: 39.758640\n","Epoch 443 | Total Loss: 0.001288, Loss: 0.001169, reg_loss: 39.768490\n","Epoch 444 | Total Loss: 0.001289, Loss: 0.001170, reg_loss: 39.779427\n","Epoch 445 | Total Loss: 0.001290, Loss: 0.001171, reg_loss: 39.789803\n","Epoch 446 | Total Loss: 0.001291, Loss: 0.001171, reg_loss: 39.799740\n","Epoch 447 | Total Loss: 0.001291, Loss: 0.001172, reg_loss: 39.810951\n","Epoch 448 | Total Loss: 0.001292, Loss: 0.001172, reg_loss: 39.820717\n","Epoch 449 | Total Loss: 0.001294, Loss: 0.001174, reg_loss: 39.831146\n","Epoch 450 | Total Loss: 0.001295, Loss: 0.001176, reg_loss: 39.840370\n","Epoch 451 | Total Loss: 0.001296, Loss: 0.001176, reg_loss: 39.849152\n","Epoch 452 | Total Loss: 0.001295, Loss: 0.001176, reg_loss: 39.859356\n","Epoch 453 | Total Loss: 0.001296, Loss: 0.001176, reg_loss: 39.867973\n","Epoch 454 | Total Loss: 0.001299, Loss: 0.001179, reg_loss: 39.878330\n","Epoch 455 | Total Loss: 0.001302, Loss: 0.001182, reg_loss: 39.886650\n","Epoch 456 | Total Loss: 0.001302, Loss: 0.001182, reg_loss: 39.895370\n","Epoch 457 | Total Loss: 0.001299, Loss: 0.001179, reg_loss: 39.904324\n","Epoch 458 | Total Loss: 0.001294, Loss: 0.001174, reg_loss: 39.912102\n","Epoch 459 | Total Loss: 0.001291, Loss: 0.001171, reg_loss: 39.921658\n","Epoch 460 | Total Loss: 0.001290, Loss: 0.001170, reg_loss: 39.929314\n","Epoch 461 | Total Loss: 0.001290, Loss: 0.001170, reg_loss: 39.939526\n","Epoch 462 | Total Loss: 0.001289, Loss: 0.001169, reg_loss: 39.947983\n","Epoch 463 | Total Loss: 0.001288, Loss: 0.001168, reg_loss: 39.957603\n","Epoch 464 | Total Loss: 0.001288, Loss: 0.001168, reg_loss: 39.966747\n","Epoch 465 | Total Loss: 0.001289, Loss: 0.001169, reg_loss: 39.975830\n","Epoch 466 | Total Loss: 0.001290, Loss: 0.001170, reg_loss: 39.985546\n","Epoch 467 | Total Loss: 0.001289, Loss: 0.001169, reg_loss: 39.994270\n","Epoch 468 | Total Loss: 0.001287, Loss: 0.001167, reg_loss: 40.004200\n","Epoch 469 | Total Loss: 0.001286, Loss: 0.001166, reg_loss: 40.012558\n","Epoch 470 | Total Loss: 0.001288, Loss: 0.001168, reg_loss: 40.022575\n","Epoch 471 | Total Loss: 0.001291, Loss: 0.001171, reg_loss: 40.031109\n","Epoch 472 | Total Loss: 0.001295, Loss: 0.001175, reg_loss: 40.041435\n","Epoch 473 | Total Loss: 0.001298, Loss: 0.001177, reg_loss: 40.049606\n","Epoch 474 | Total Loss: 0.001297, Loss: 0.001177, reg_loss: 40.059818\n","Epoch 475 | Total Loss: 0.001295, Loss: 0.001175, reg_loss: 40.066986\n","Epoch 476 | Total Loss: 0.001292, Loss: 0.001172, reg_loss: 40.077209\n","Epoch 477 | Total Loss: 0.001290, Loss: 0.001170, reg_loss: 40.084332\n","Epoch 478 | Total Loss: 0.001290, Loss: 0.001170, reg_loss: 40.094883\n","Epoch 479 | Total Loss: 0.001290, Loss: 0.001170, reg_loss: 40.102119\n","Epoch 480 | Total Loss: 0.001290, Loss: 0.001170, reg_loss: 40.111633\n","Epoch 481 | Total Loss: 0.001289, Loss: 0.001169, reg_loss: 40.119648\n","Epoch 482 | Total Loss: 0.001288, Loss: 0.001167, reg_loss: 40.128609\n","Epoch 483 | Total Loss: 0.001287, Loss: 0.001167, reg_loss: 40.137394\n","Epoch 484 | Total Loss: 0.001287, Loss: 0.001167, reg_loss: 40.146275\n","Epoch 485 | Total Loss: 0.001288, Loss: 0.001168, reg_loss: 40.155079\n","Epoch 486 | Total Loss: 0.001290, Loss: 0.001169, reg_loss: 40.164501\n","Epoch 487 | Total Loss: 0.001291, Loss: 0.001170, reg_loss: 40.172688\n","Epoch 488 | Total Loss: 0.001291, Loss: 0.001171, reg_loss: 40.182026\n","Epoch 489 | Total Loss: 0.001291, Loss: 0.001170, reg_loss: 40.189793\n","Epoch 490 | Total Loss: 0.001290, Loss: 0.001169, reg_loss: 40.198273\n","Epoch 491 | Total Loss: 0.001289, Loss: 0.001168, reg_loss: 40.206272\n","Epoch 492 | Total Loss: 0.001288, Loss: 0.001167, reg_loss: 40.214874\n","Epoch 493 | Total Loss: 0.001286, Loss: 0.001166, reg_loss: 40.223358\n","Epoch 494 | Total Loss: 0.001286, Loss: 0.001165, reg_loss: 40.231808\n","Epoch 495 | Total Loss: 0.001286, Loss: 0.001165, reg_loss: 40.240307\n","Epoch 496 | Total Loss: 0.001287, Loss: 0.001166, reg_loss: 40.248802\n","Epoch 497 | Total Loss: 0.001289, Loss: 0.001168, reg_loss: 40.257172\n","Epoch 498 | Total Loss: 0.001291, Loss: 0.001170, reg_loss: 40.265747\n","Epoch 499 | Total Loss: 0.001291, Loss: 0.001170, reg_loss: 40.273849\n","1\n","Epoch 0 | Total Loss: 0.005243, Loss: 0.005243, reg_loss: 0.000000\n","Epoch 1 | Total Loss: 0.005841, Loss: 0.005829, reg_loss: 4.118710\n","Epoch 2 | Total Loss: 0.006461, Loss: 0.006439, reg_loss: 7.325608\n","Epoch 3 | Total Loss: 0.005254, Loss: 0.005228, reg_loss: 8.800719\n","Epoch 4 | Total Loss: 0.005573, Loss: 0.005543, reg_loss: 10.106605\n","Epoch 5 | Total Loss: 0.005290, Loss: 0.005258, reg_loss: 10.615150\n","Epoch 6 | Total Loss: 0.004497, Loss: 0.004465, reg_loss: 10.546646\n","Epoch 7 | Total Loss: 0.004250, Loss: 0.004218, reg_loss: 10.606885\n","Epoch 8 | Total Loss: 0.004325, Loss: 0.004291, reg_loss: 11.195697\n","Epoch 9 | Total Loss: 0.004025, Loss: 0.003989, reg_loss: 12.027495\n","Epoch 10 | Total Loss: 0.003618, Loss: 0.003579, reg_loss: 12.847860\n","Epoch 11 | Total Loss: 0.003512, Loss: 0.003471, reg_loss: 13.667990\n","Epoch 12 | Total Loss: 0.003407, Loss: 0.003364, reg_loss: 14.514912\n","Epoch 13 | Total Loss: 0.003142, Loss: 0.003096, reg_loss: 15.348342\n","Epoch 14 | Total Loss: 0.002986, Loss: 0.002938, reg_loss: 16.154514\n","Epoch 15 | Total Loss: 0.002914, Loss: 0.002863, reg_loss: 16.971613\n","Epoch 16 | Total Loss: 0.002731, Loss: 0.002677, reg_loss: 17.830515\n","Epoch 17 | Total Loss: 0.002578, Loss: 0.002522, reg_loss: 18.728968\n","Epoch 18 | Total Loss: 0.002516, Loss: 0.002457, reg_loss: 19.637873\n","Epoch 19 | Total Loss: 0.002402, Loss: 0.002340, reg_loss: 20.532568\n","Epoch 20 | Total Loss: 0.002292, Loss: 0.002228, reg_loss: 21.411688\n","Epoch 21 | Total Loss: 0.002224, Loss: 0.002157, reg_loss: 22.287781\n","Epoch 22 | Total Loss: 0.002137, Loss: 0.002067, reg_loss: 23.172787\n","Epoch 23 | Total Loss: 0.002068, Loss: 0.001996, reg_loss: 24.061831\n","Epoch 24 | Total Loss: 0.002015, Loss: 0.001940, reg_loss: 24.933449\n","Epoch 25 | Total Loss: 0.001947, Loss: 0.001870, reg_loss: 25.771286\n","Epoch 26 | Total Loss: 0.001897, Loss: 0.001817, reg_loss: 26.576876\n","Epoch 27 | Total Loss: 0.001854, Loss: 0.001772, reg_loss: 27.361607\n","Epoch 28 | Total Loss: 0.001806, Loss: 0.001722, reg_loss: 28.128378\n","Epoch 29 | Total Loss: 0.001773, Loss: 0.001686, reg_loss: 28.862562\n","Epoch 30 | Total Loss: 0.001737, Loss: 0.001648, reg_loss: 29.546310\n","Epoch 31 | Total Loss: 0.001706, Loss: 0.001615, reg_loss: 30.179302\n","Epoch 32 | Total Loss: 0.001682, Loss: 0.001589, reg_loss: 30.769129\n","Epoch 33 | Total Loss: 0.001657, Loss: 0.001563, reg_loss: 31.316656\n","Epoch 34 | Total Loss: 0.001638, Loss: 0.001542, reg_loss: 31.811640\n","Epoch 35 | Total Loss: 0.001619, Loss: 0.001522, reg_loss: 32.248241\n","Epoch 36 | Total Loss: 0.001605, Loss: 0.001507, reg_loss: 32.630409\n","Epoch 37 | Total Loss: 0.001590, Loss: 0.001491, reg_loss: 32.960224\n","Epoch 38 | Total Loss: 0.001577, Loss: 0.001478, reg_loss: 33.234524\n","Epoch 39 | Total Loss: 0.001566, Loss: 0.001465, reg_loss: 33.453651\n","Epoch 40 | Total Loss: 0.001555, Loss: 0.001454, reg_loss: 33.624290\n","Epoch 41 | Total Loss: 0.001544, Loss: 0.001443, reg_loss: 33.754517\n","Epoch 42 | Total Loss: 0.001535, Loss: 0.001433, reg_loss: 33.844944\n","Epoch 43 | Total Loss: 0.001526, Loss: 0.001425, reg_loss: 33.895657\n","Epoch 44 | Total Loss: 0.001517, Loss: 0.001416, reg_loss: 33.914242\n","Epoch 45 | Total Loss: 0.001509, Loss: 0.001407, reg_loss: 33.907440\n","Epoch 46 | Total Loss: 0.001500, Loss: 0.001399, reg_loss: 33.880253\n","Epoch 47 | Total Loss: 0.001491, Loss: 0.001390, reg_loss: 33.834759\n","Epoch 48 | Total Loss: 0.001484, Loss: 0.001382, reg_loss: 33.776665\n","Epoch 49 | Total Loss: 0.001477, Loss: 0.001376, reg_loss: 33.712135\n","Epoch 50 | Total Loss: 0.001471, Loss: 0.001370, reg_loss: 33.644222\n","Epoch 51 | Total Loss: 0.001466, Loss: 0.001365, reg_loss: 33.574879\n","Epoch 52 | Total Loss: 0.001461, Loss: 0.001360, reg_loss: 33.505905\n","Epoch 53 | Total Loss: 0.001455, Loss: 0.001354, reg_loss: 33.437607\n","Epoch 54 | Total Loss: 0.001450, Loss: 0.001350, reg_loss: 33.372246\n","Epoch 55 | Total Loss: 0.001447, Loss: 0.001347, reg_loss: 33.309162\n","Epoch 56 | Total Loss: 0.001442, Loss: 0.001342, reg_loss: 33.249950\n","Epoch 57 | Total Loss: 0.001436, Loss: 0.001337, reg_loss: 33.195282\n","Epoch 58 | Total Loss: 0.001433, Loss: 0.001333, reg_loss: 33.145252\n","Epoch 59 | Total Loss: 0.001429, Loss: 0.001330, reg_loss: 33.098995\n","Epoch 60 | Total Loss: 0.001425, Loss: 0.001326, reg_loss: 33.055634\n","Epoch 61 | Total Loss: 0.001421, Loss: 0.001322, reg_loss: 33.016068\n","Epoch 62 | Total Loss: 0.001419, Loss: 0.001320, reg_loss: 32.978149\n","Epoch 63 | Total Loss: 0.001416, Loss: 0.001317, reg_loss: 32.942970\n","Epoch 64 | Total Loss: 0.001413, Loss: 0.001314, reg_loss: 32.910301\n","Epoch 65 | Total Loss: 0.001410, Loss: 0.001311, reg_loss: 32.879517\n","Epoch 66 | Total Loss: 0.001407, Loss: 0.001308, reg_loss: 32.850121\n","Epoch 67 | Total Loss: 0.001405, Loss: 0.001306, reg_loss: 32.821976\n","Epoch 68 | Total Loss: 0.001402, Loss: 0.001304, reg_loss: 32.795193\n","Epoch 69 | Total Loss: 0.001400, Loss: 0.001301, reg_loss: 32.768917\n","Epoch 70 | Total Loss: 0.001398, Loss: 0.001299, reg_loss: 32.744358\n","Epoch 71 | Total Loss: 0.001396, Loss: 0.001298, reg_loss: 32.720413\n","Epoch 72 | Total Loss: 0.001394, Loss: 0.001296, reg_loss: 32.698135\n","Epoch 73 | Total Loss: 0.001393, Loss: 0.001295, reg_loss: 32.676605\n","Epoch 74 | Total Loss: 0.001391, Loss: 0.001293, reg_loss: 32.656769\n","Epoch 75 | Total Loss: 0.001390, Loss: 0.001292, reg_loss: 32.637836\n","Epoch 76 | Total Loss: 0.001387, Loss: 0.001290, reg_loss: 32.620720\n","Epoch 77 | Total Loss: 0.001385, Loss: 0.001287, reg_loss: 32.603870\n","Epoch 78 | Total Loss: 0.001384, Loss: 0.001286, reg_loss: 32.589539\n","Epoch 79 | Total Loss: 0.001383, Loss: 0.001286, reg_loss: 32.576023\n","Epoch 80 | Total Loss: 0.001382, Loss: 0.001284, reg_loss: 32.565044\n","Epoch 81 | Total Loss: 0.001381, Loss: 0.001283, reg_loss: 32.554752\n","Epoch 82 | Total Loss: 0.001380, Loss: 0.001283, reg_loss: 32.546692\n","Epoch 83 | Total Loss: 0.001381, Loss: 0.001283, reg_loss: 32.539585\n","Epoch 84 | Total Loss: 0.001378, Loss: 0.001280, reg_loss: 32.534058\n","Epoch 85 | Total Loss: 0.001373, Loss: 0.001276, reg_loss: 32.530499\n","Epoch 86 | Total Loss: 0.001373, Loss: 0.001276, reg_loss: 32.527435\n","Epoch 87 | Total Loss: 0.001374, Loss: 0.001276, reg_loss: 32.526852\n","Epoch 88 | Total Loss: 0.001372, Loss: 0.001274, reg_loss: 32.526470\n","Epoch 89 | Total Loss: 0.001371, Loss: 0.001273, reg_loss: 32.528683\n","Epoch 90 | Total Loss: 0.001371, Loss: 0.001273, reg_loss: 32.530548\n","Epoch 91 | Total Loss: 0.001369, Loss: 0.001272, reg_loss: 32.535507\n","Epoch 92 | Total Loss: 0.001368, Loss: 0.001270, reg_loss: 32.539608\n","Epoch 93 | Total Loss: 0.001367, Loss: 0.001270, reg_loss: 32.546928\n","Epoch 94 | Total Loss: 0.001366, Loss: 0.001269, reg_loss: 32.553104\n","Epoch 95 | Total Loss: 0.001365, Loss: 0.001268, reg_loss: 32.562439\n","Epoch 96 | Total Loss: 0.001365, Loss: 0.001267, reg_loss: 32.571159\n","Epoch 97 | Total Loss: 0.001363, Loss: 0.001265, reg_loss: 32.581913\n","Epoch 98 | Total Loss: 0.001363, Loss: 0.001265, reg_loss: 32.593143\n","Epoch 99 | Total Loss: 0.001365, Loss: 0.001267, reg_loss: 32.605194\n","Epoch 100 | Total Loss: 0.001366, Loss: 0.001268, reg_loss: 32.618675\n","Epoch 101 | Total Loss: 0.001366, Loss: 0.001268, reg_loss: 32.632069\n","Epoch 102 | Total Loss: 0.001365, Loss: 0.001267, reg_loss: 32.646523\n","Epoch 103 | Total Loss: 0.001363, Loss: 0.001265, reg_loss: 32.660751\n","Epoch 104 | Total Loss: 0.001363, Loss: 0.001265, reg_loss: 32.675987\n","Epoch 105 | Total Loss: 0.001363, Loss: 0.001265, reg_loss: 32.691132\n","Epoch 106 | Total Loss: 0.001361, Loss: 0.001263, reg_loss: 32.706665\n","Epoch 107 | Total Loss: 0.001358, Loss: 0.001260, reg_loss: 32.722824\n","Epoch 108 | Total Loss: 0.001357, Loss: 0.001259, reg_loss: 32.739277\n","Epoch 109 | Total Loss: 0.001357, Loss: 0.001258, reg_loss: 32.756168\n","Epoch 110 | Total Loss: 0.001357, Loss: 0.001258, reg_loss: 32.773869\n","Epoch 111 | Total Loss: 0.001358, Loss: 0.001260, reg_loss: 32.791214\n","Epoch 112 | Total Loss: 0.001360, Loss: 0.001262, reg_loss: 32.809109\n","Epoch 113 | Total Loss: 0.001361, Loss: 0.001263, reg_loss: 32.827763\n","Epoch 114 | Total Loss: 0.001358, Loss: 0.001260, reg_loss: 32.845081\n","Epoch 115 | Total Loss: 0.001356, Loss: 0.001258, reg_loss: 32.866096\n","Epoch 116 | Total Loss: 0.001358, Loss: 0.001259, reg_loss: 32.882877\n","Epoch 117 | Total Loss: 0.001359, Loss: 0.001260, reg_loss: 32.904579\n","Epoch 118 | Total Loss: 0.001356, Loss: 0.001258, reg_loss: 32.922340\n","Epoch 119 | Total Loss: 0.001354, Loss: 0.001255, reg_loss: 32.942001\n","Epoch 120 | Total Loss: 0.001355, Loss: 0.001256, reg_loss: 32.961449\n","Epoch 121 | Total Loss: 0.001357, Loss: 0.001258, reg_loss: 32.980747\n","Epoch 122 | Total Loss: 0.001358, Loss: 0.001259, reg_loss: 33.000866\n","Epoch 123 | Total Loss: 0.001358, Loss: 0.001259, reg_loss: 33.020981\n","Epoch 124 | Total Loss: 0.001355, Loss: 0.001255, reg_loss: 33.039845\n","Epoch 125 | Total Loss: 0.001352, Loss: 0.001253, reg_loss: 33.061131\n","Epoch 126 | Total Loss: 0.001351, Loss: 0.001251, reg_loss: 33.080112\n","Epoch 127 | Total Loss: 0.001350, Loss: 0.001251, reg_loss: 33.100670\n","Epoch 128 | Total Loss: 0.001350, Loss: 0.001250, reg_loss: 33.121433\n","Epoch 129 | Total Loss: 0.001351, Loss: 0.001251, reg_loss: 33.141075\n","Epoch 130 | Total Loss: 0.001351, Loss: 0.001252, reg_loss: 33.162018\n","Epoch 131 | Total Loss: 0.001349, Loss: 0.001250, reg_loss: 33.182999\n","Epoch 132 | Total Loss: 0.001348, Loss: 0.001248, reg_loss: 33.203606\n","Epoch 133 | Total Loss: 0.001345, Loss: 0.001246, reg_loss: 33.224892\n","Epoch 134 | Total Loss: 0.001343, Loss: 0.001244, reg_loss: 33.247269\n","Epoch 135 | Total Loss: 0.001344, Loss: 0.001244, reg_loss: 33.266792\n","Epoch 136 | Total Loss: 0.001345, Loss: 0.001245, reg_loss: 33.290585\n","Epoch 137 | Total Loss: 0.001345, Loss: 0.001245, reg_loss: 33.310345\n","Epoch 138 | Total Loss: 0.001344, Loss: 0.001244, reg_loss: 33.331997\n","Epoch 139 | Total Loss: 0.001345, Loss: 0.001245, reg_loss: 33.354031\n","Epoch 140 | Total Loss: 0.001345, Loss: 0.001245, reg_loss: 33.374073\n","Epoch 141 | Total Loss: 0.001343, Loss: 0.001243, reg_loss: 33.396534\n","Epoch 142 | Total Loss: 0.001341, Loss: 0.001241, reg_loss: 33.417328\n","Epoch 143 | Total Loss: 0.001338, Loss: 0.001237, reg_loss: 33.438732\n","Epoch 144 | Total Loss: 0.001335, Loss: 0.001235, reg_loss: 33.460064\n","Epoch 145 | Total Loss: 0.001335, Loss: 0.001234, reg_loss: 33.482639\n","Epoch 146 | Total Loss: 0.001335, Loss: 0.001235, reg_loss: 33.503414\n","Epoch 147 | Total Loss: 0.001335, Loss: 0.001235, reg_loss: 33.526890\n","Epoch 148 | Total Loss: 0.001335, Loss: 0.001235, reg_loss: 33.547474\n","Epoch 149 | Total Loss: 0.001336, Loss: 0.001235, reg_loss: 33.570374\n","Epoch 150 | Total Loss: 0.001336, Loss: 0.001235, reg_loss: 33.592239\n","Epoch 151 | Total Loss: 0.001338, Loss: 0.001237, reg_loss: 33.614414\n","Epoch 152 | Total Loss: 0.001339, Loss: 0.001238, reg_loss: 33.636765\n","Epoch 153 | Total Loss: 0.001340, Loss: 0.001239, reg_loss: 33.658062\n","Epoch 154 | Total Loss: 0.001339, Loss: 0.001238, reg_loss: 33.680569\n","Epoch 155 | Total Loss: 0.001337, Loss: 0.001236, reg_loss: 33.700977\n","Epoch 156 | Total Loss: 0.001336, Loss: 0.001235, reg_loss: 33.724049\n","Epoch 157 | Total Loss: 0.001335, Loss: 0.001234, reg_loss: 33.744343\n","Epoch 158 | Total Loss: 0.001334, Loss: 0.001233, reg_loss: 33.767025\n","Epoch 159 | Total Loss: 0.001333, Loss: 0.001232, reg_loss: 33.787865\n","Epoch 160 | Total Loss: 0.001332, Loss: 0.001230, reg_loss: 33.809807\n","Epoch 161 | Total Loss: 0.001329, Loss: 0.001228, reg_loss: 33.831135\n","Epoch 162 | Total Loss: 0.001328, Loss: 0.001227, reg_loss: 33.853363\n","Epoch 163 | Total Loss: 0.001329, Loss: 0.001228, reg_loss: 33.874477\n","Epoch 164 | Total Loss: 0.001331, Loss: 0.001229, reg_loss: 33.897251\n","Epoch 165 | Total Loss: 0.001329, Loss: 0.001227, reg_loss: 33.918156\n","Epoch 166 | Total Loss: 0.001327, Loss: 0.001225, reg_loss: 33.940285\n","Epoch 167 | Total Loss: 0.001327, Loss: 0.001225, reg_loss: 33.961716\n","Epoch 168 | Total Loss: 0.001330, Loss: 0.001228, reg_loss: 33.983974\n","Epoch 169 | Total Loss: 0.001333, Loss: 0.001231, reg_loss: 34.005527\n","Epoch 170 | Total Loss: 0.001332, Loss: 0.001230, reg_loss: 34.028088\n","Epoch 171 | Total Loss: 0.001330, Loss: 0.001228, reg_loss: 34.048645\n","Epoch 172 | Total Loss: 0.001327, Loss: 0.001225, reg_loss: 34.071198\n","Epoch 173 | Total Loss: 0.001326, Loss: 0.001224, reg_loss: 34.092331\n","Epoch 174 | Total Loss: 0.001327, Loss: 0.001224, reg_loss: 34.114780\n","Epoch 175 | Total Loss: 0.001327, Loss: 0.001225, reg_loss: 34.136543\n","Epoch 176 | Total Loss: 0.001327, Loss: 0.001225, reg_loss: 34.158310\n","Epoch 177 | Total Loss: 0.001327, Loss: 0.001224, reg_loss: 34.180336\n","Epoch 178 | Total Loss: 0.001326, Loss: 0.001223, reg_loss: 34.201736\n","Epoch 179 | Total Loss: 0.001324, Loss: 0.001221, reg_loss: 34.223732\n","Epoch 180 | Total Loss: 0.001323, Loss: 0.001220, reg_loss: 34.245216\n","Epoch 181 | Total Loss: 0.001324, Loss: 0.001221, reg_loss: 34.267124\n","Epoch 182 | Total Loss: 0.001325, Loss: 0.001222, reg_loss: 34.288193\n","Epoch 183 | Total Loss: 0.001326, Loss: 0.001223, reg_loss: 34.310051\n","Epoch 184 | Total Loss: 0.001328, Loss: 0.001225, reg_loss: 34.330109\n","Epoch 185 | Total Loss: 0.001325, Loss: 0.001222, reg_loss: 34.350994\n","Epoch 186 | Total Loss: 0.001321, Loss: 0.001218, reg_loss: 34.370796\n","Epoch 187 | Total Loss: 0.001320, Loss: 0.001217, reg_loss: 34.390247\n","Epoch 188 | Total Loss: 0.001323, Loss: 0.001220, reg_loss: 34.410366\n","Epoch 189 | Total Loss: 0.001326, Loss: 0.001222, reg_loss: 34.428799\n","Epoch 190 | Total Loss: 0.001328, Loss: 0.001225, reg_loss: 34.447960\n","Epoch 191 | Total Loss: 0.001329, Loss: 0.001226, reg_loss: 34.466537\n","Epoch 192 | Total Loss: 0.001328, Loss: 0.001224, reg_loss: 34.484177\n","Epoch 193 | Total Loss: 0.001323, Loss: 0.001220, reg_loss: 34.502586\n","Epoch 194 | Total Loss: 0.001317, Loss: 0.001214, reg_loss: 34.519299\n","Epoch 195 | Total Loss: 0.001312, Loss: 0.001208, reg_loss: 34.537735\n","Epoch 196 | Total Loss: 0.001311, Loss: 0.001207, reg_loss: 34.555122\n","Epoch 197 | Total Loss: 0.001314, Loss: 0.001210, reg_loss: 34.574051\n","Epoch 198 | Total Loss: 0.001316, Loss: 0.001212, reg_loss: 34.592030\n","Epoch 199 | Total Loss: 0.001315, Loss: 0.001211, reg_loss: 34.610493\n","Epoch 200 | Total Loss: 0.001313, Loss: 0.001209, reg_loss: 34.628433\n","Epoch 201 | Total Loss: 0.001313, Loss: 0.001210, reg_loss: 34.646816\n","Epoch 202 | Total Loss: 0.001317, Loss: 0.001213, reg_loss: 34.664761\n","Epoch 203 | Total Loss: 0.001319, Loss: 0.001215, reg_loss: 34.683258\n","Epoch 204 | Total Loss: 0.001320, Loss: 0.001215, reg_loss: 34.700001\n","Epoch 205 | Total Loss: 0.001319, Loss: 0.001215, reg_loss: 34.718166\n","Epoch 206 | Total Loss: 0.001320, Loss: 0.001216, reg_loss: 34.734398\n","Epoch 207 | Total Loss: 0.001319, Loss: 0.001215, reg_loss: 34.752415\n","Epoch 208 | Total Loss: 0.001319, Loss: 0.001215, reg_loss: 34.768475\n","Epoch 209 | Total Loss: 0.001318, Loss: 0.001214, reg_loss: 34.785580\n","Epoch 210 | Total Loss: 0.001317, Loss: 0.001213, reg_loss: 34.802021\n","Epoch 211 | Total Loss: 0.001314, Loss: 0.001210, reg_loss: 34.818069\n","Epoch 212 | Total Loss: 0.001310, Loss: 0.001205, reg_loss: 34.834732\n","Epoch 213 | Total Loss: 0.001307, Loss: 0.001202, reg_loss: 34.850063\n","Epoch 214 | Total Loss: 0.001306, Loss: 0.001201, reg_loss: 34.867710\n","Epoch 215 | Total Loss: 0.001305, Loss: 0.001201, reg_loss: 34.882694\n","Epoch 216 | Total Loss: 0.001304, Loss: 0.001199, reg_loss: 34.900227\n","Epoch 217 | Total Loss: 0.001302, Loss: 0.001198, reg_loss: 34.916370\n","Epoch 218 | Total Loss: 0.001303, Loss: 0.001198, reg_loss: 34.932789\n","Epoch 219 | Total Loss: 0.001306, Loss: 0.001201, reg_loss: 34.950359\n","Epoch 220 | Total Loss: 0.001311, Loss: 0.001206, reg_loss: 34.966293\n","Epoch 221 | Total Loss: 0.001317, Loss: 0.001212, reg_loss: 34.982418\n","Epoch 222 | Total Loss: 0.001320, Loss: 0.001215, reg_loss: 34.999249\n","Epoch 223 | Total Loss: 0.001318, Loss: 0.001213, reg_loss: 35.013172\n","Epoch 224 | Total Loss: 0.001313, Loss: 0.001208, reg_loss: 35.030064\n","Epoch 225 | Total Loss: 0.001310, Loss: 0.001205, reg_loss: 35.044247\n","Epoch 226 | Total Loss: 0.001307, Loss: 0.001202, reg_loss: 35.059109\n","Epoch 227 | Total Loss: 0.001303, Loss: 0.001198, reg_loss: 35.075500\n","Epoch 228 | Total Loss: 0.001300, Loss: 0.001195, reg_loss: 35.088829\n","Epoch 229 | Total Loss: 0.001300, Loss: 0.001195, reg_loss: 35.106190\n","Epoch 230 | Total Loss: 0.001304, Loss: 0.001198, reg_loss: 35.120850\n","Epoch 231 | Total Loss: 0.001307, Loss: 0.001201, reg_loss: 35.136417\n","Epoch 232 | Total Loss: 0.001305, Loss: 0.001200, reg_loss: 35.152485\n","Epoch 233 | Total Loss: 0.001300, Loss: 0.001195, reg_loss: 35.166496\n","Epoch 234 | Total Loss: 0.001296, Loss: 0.001190, reg_loss: 35.182655\n","Epoch 235 | Total Loss: 0.001296, Loss: 0.001191, reg_loss: 35.197380\n","Epoch 236 | Total Loss: 0.001301, Loss: 0.001195, reg_loss: 35.213646\n","Epoch 237 | Total Loss: 0.001305, Loss: 0.001199, reg_loss: 35.228947\n","Epoch 238 | Total Loss: 0.001307, Loss: 0.001201, reg_loss: 35.245037\n","Epoch 239 | Total Loss: 0.001309, Loss: 0.001203, reg_loss: 35.259258\n","Epoch 240 | Total Loss: 0.001309, Loss: 0.001203, reg_loss: 35.274242\n","Epoch 241 | Total Loss: 0.001305, Loss: 0.001199, reg_loss: 35.288486\n","Epoch 242 | Total Loss: 0.001300, Loss: 0.001194, reg_loss: 35.302376\n","Epoch 243 | Total Loss: 0.001297, Loss: 0.001191, reg_loss: 35.317543\n","Epoch 244 | Total Loss: 0.001299, Loss: 0.001193, reg_loss: 35.331745\n","Epoch 245 | Total Loss: 0.001303, Loss: 0.001197, reg_loss: 35.347000\n","Epoch 246 | Total Loss: 0.001306, Loss: 0.001200, reg_loss: 35.361420\n","Epoch 247 | Total Loss: 0.001305, Loss: 0.001199, reg_loss: 35.375660\n","Epoch 248 | Total Loss: 0.001301, Loss: 0.001195, reg_loss: 35.389912\n","Epoch 249 | Total Loss: 0.001297, Loss: 0.001190, reg_loss: 35.404236\n","Epoch 250 | Total Loss: 0.001294, Loss: 0.001188, reg_loss: 35.418488\n","Epoch 251 | Total Loss: 0.001296, Loss: 0.001190, reg_loss: 35.432995\n","Epoch 252 | Total Loss: 0.001300, Loss: 0.001193, reg_loss: 35.447376\n","Epoch 253 | Total Loss: 0.001301, Loss: 0.001194, reg_loss: 35.461731\n","Epoch 254 | Total Loss: 0.001301, Loss: 0.001195, reg_loss: 35.476368\n","Epoch 255 | Total Loss: 0.001306, Loss: 0.001199, reg_loss: 35.490501\n","Epoch 256 | Total Loss: 0.001311, Loss: 0.001204, reg_loss: 35.504505\n","Epoch 257 | Total Loss: 0.001308, Loss: 0.001202, reg_loss: 35.518467\n","Epoch 258 | Total Loss: 0.001301, Loss: 0.001194, reg_loss: 35.531181\n","Epoch 259 | Total Loss: 0.001296, Loss: 0.001189, reg_loss: 35.545120\n","Epoch 260 | Total Loss: 0.001297, Loss: 0.001191, reg_loss: 35.558022\n","Epoch 261 | Total Loss: 0.001302, Loss: 0.001196, reg_loss: 35.571957\n","Epoch 262 | Total Loss: 0.001306, Loss: 0.001199, reg_loss: 35.584991\n","Epoch 263 | Total Loss: 0.001304, Loss: 0.001197, reg_loss: 35.598892\n","Epoch 264 | Total Loss: 0.001298, Loss: 0.001191, reg_loss: 35.610718\n","Epoch 265 | Total Loss: 0.001292, Loss: 0.001185, reg_loss: 35.625130\n","Epoch 266 | Total Loss: 0.001292, Loss: 0.001185, reg_loss: 35.636707\n","Epoch 267 | Total Loss: 0.001295, Loss: 0.001188, reg_loss: 35.651257\n","Epoch 268 | Total Loss: 0.001299, Loss: 0.001192, reg_loss: 35.663570\n","Epoch 269 | Total Loss: 0.001303, Loss: 0.001196, reg_loss: 35.677284\n","Epoch 270 | Total Loss: 0.001307, Loss: 0.001200, reg_loss: 35.690331\n","Epoch 271 | Total Loss: 0.001308, Loss: 0.001201, reg_loss: 35.702637\n","Epoch 272 | Total Loss: 0.001303, Loss: 0.001196, reg_loss: 35.715061\n","Epoch 273 | Total Loss: 0.001296, Loss: 0.001189, reg_loss: 35.726665\n","Epoch 274 | Total Loss: 0.001291, Loss: 0.001184, reg_loss: 35.738873\n","Epoch 275 | Total Loss: 0.001290, Loss: 0.001183, reg_loss: 35.751793\n","Epoch 276 | Total Loss: 0.001291, Loss: 0.001184, reg_loss: 35.763920\n","Epoch 277 | Total Loss: 0.001293, Loss: 0.001185, reg_loss: 35.777687\n","Epoch 278 | Total Loss: 0.001293, Loss: 0.001186, reg_loss: 35.789211\n","Epoch 279 | Total Loss: 0.001292, Loss: 0.001185, reg_loss: 35.802872\n","Epoch 280 | Total Loss: 0.001292, Loss: 0.001185, reg_loss: 35.814209\n","Epoch 281 | Total Loss: 0.001294, Loss: 0.001186, reg_loss: 35.827808\n","Epoch 282 | Total Loss: 0.001297, Loss: 0.001190, reg_loss: 35.839367\n","Epoch 283 | Total Loss: 0.001300, Loss: 0.001192, reg_loss: 35.852615\n","Epoch 284 | Total Loss: 0.001299, Loss: 0.001192, reg_loss: 35.864170\n","Epoch 285 | Total Loss: 0.001297, Loss: 0.001190, reg_loss: 35.876396\n","Epoch 286 | Total Loss: 0.001296, Loss: 0.001189, reg_loss: 35.888248\n","Epoch 287 | Total Loss: 0.001297, Loss: 0.001190, reg_loss: 35.899872\n","Epoch 288 | Total Loss: 0.001300, Loss: 0.001192, reg_loss: 35.911968\n","Epoch 289 | Total Loss: 0.001301, Loss: 0.001194, reg_loss: 35.923214\n","Epoch 290 | Total Loss: 0.001302, Loss: 0.001194, reg_loss: 35.934475\n","Epoch 291 | Total Loss: 0.001301, Loss: 0.001194, reg_loss: 35.945473\n","Epoch 292 | Total Loss: 0.001300, Loss: 0.001192, reg_loss: 35.955929\n","Epoch 293 | Total Loss: 0.001297, Loss: 0.001189, reg_loss: 35.966621\n","Epoch 294 | Total Loss: 0.001294, Loss: 0.001186, reg_loss: 35.976631\n","Epoch 295 | Total Loss: 0.001291, Loss: 0.001183, reg_loss: 35.987186\n","Epoch 296 | Total Loss: 0.001289, Loss: 0.001181, reg_loss: 35.997910\n","Epoch 297 | Total Loss: 0.001288, Loss: 0.001180, reg_loss: 36.008793\n","Epoch 298 | Total Loss: 0.001289, Loss: 0.001181, reg_loss: 36.019997\n","Epoch 299 | Total Loss: 0.001289, Loss: 0.001181, reg_loss: 36.031124\n","Epoch 300 | Total Loss: 0.001290, Loss: 0.001182, reg_loss: 36.042046\n","Epoch 301 | Total Loss: 0.001292, Loss: 0.001183, reg_loss: 36.053543\n","Epoch 302 | Total Loss: 0.001294, Loss: 0.001186, reg_loss: 36.063904\n","Epoch 303 | Total Loss: 0.001296, Loss: 0.001188, reg_loss: 36.075565\n","Epoch 304 | Total Loss: 0.001293, Loss: 0.001185, reg_loss: 36.084694\n","Epoch 305 | Total Loss: 0.001286, Loss: 0.001178, reg_loss: 36.096657\n","Epoch 306 | Total Loss: 0.001281, Loss: 0.001173, reg_loss: 36.105484\n","Epoch 307 | Total Loss: 0.001281, Loss: 0.001172, reg_loss: 36.117683\n","Epoch 308 | Total Loss: 0.001283, Loss: 0.001175, reg_loss: 36.127617\n","Epoch 309 | Total Loss: 0.001284, Loss: 0.001176, reg_loss: 36.139053\n","Epoch 310 | Total Loss: 0.001285, Loss: 0.001176, reg_loss: 36.149712\n","Epoch 311 | Total Loss: 0.001285, Loss: 0.001177, reg_loss: 36.159874\n","Epoch 312 | Total Loss: 0.001287, Loss: 0.001179, reg_loss: 36.171379\n","Epoch 313 | Total Loss: 0.001288, Loss: 0.001180, reg_loss: 36.181511\n","Epoch 314 | Total Loss: 0.001288, Loss: 0.001179, reg_loss: 36.192516\n","Epoch 315 | Total Loss: 0.001287, Loss: 0.001178, reg_loss: 36.202919\n","Epoch 316 | Total Loss: 0.001286, Loss: 0.001178, reg_loss: 36.213589\n","Epoch 317 | Total Loss: 0.001286, Loss: 0.001177, reg_loss: 36.223835\n","Epoch 318 | Total Loss: 0.001285, Loss: 0.001177, reg_loss: 36.234688\n","Epoch 319 | Total Loss: 0.001286, Loss: 0.001177, reg_loss: 36.245239\n","Epoch 320 | Total Loss: 0.001286, Loss: 0.001177, reg_loss: 36.255787\n","Epoch 321 | Total Loss: 0.001286, Loss: 0.001177, reg_loss: 36.266449\n","Epoch 322 | Total Loss: 0.001285, Loss: 0.001176, reg_loss: 36.276443\n","Epoch 323 | Total Loss: 0.001284, Loss: 0.001175, reg_loss: 36.287510\n","Epoch 324 | Total Loss: 0.001285, Loss: 0.001176, reg_loss: 36.297028\n","Epoch 325 | Total Loss: 0.001287, Loss: 0.001178, reg_loss: 36.307888\n","Epoch 326 | Total Loss: 0.001289, Loss: 0.001180, reg_loss: 36.317417\n","Epoch 327 | Total Loss: 0.001289, Loss: 0.001180, reg_loss: 36.327324\n","Epoch 328 | Total Loss: 0.001287, Loss: 0.001178, reg_loss: 36.337395\n","Epoch 329 | Total Loss: 0.001285, Loss: 0.001176, reg_loss: 36.347038\n","Epoch 330 | Total Loss: 0.001283, Loss: 0.001174, reg_loss: 36.357368\n","Epoch 331 | Total Loss: 0.001282, Loss: 0.001173, reg_loss: 36.367031\n","Epoch 332 | Total Loss: 0.001282, Loss: 0.001173, reg_loss: 36.377815\n","Epoch 333 | Total Loss: 0.001283, Loss: 0.001174, reg_loss: 36.387733\n","Epoch 334 | Total Loss: 0.001285, Loss: 0.001176, reg_loss: 36.398438\n","Epoch 335 | Total Loss: 0.001287, Loss: 0.001178, reg_loss: 36.408310\n","Epoch 336 | Total Loss: 0.001289, Loss: 0.001179, reg_loss: 36.418442\n","Epoch 337 | Total Loss: 0.001288, Loss: 0.001179, reg_loss: 36.427895\n","Epoch 338 | Total Loss: 0.001288, Loss: 0.001179, reg_loss: 36.437561\n","Epoch 339 | Total Loss: 0.001289, Loss: 0.001180, reg_loss: 36.446938\n","Epoch 340 | Total Loss: 0.001292, Loss: 0.001182, reg_loss: 36.456493\n","Epoch 341 | Total Loss: 0.001293, Loss: 0.001183, reg_loss: 36.466011\n","Epoch 342 | Total Loss: 0.001291, Loss: 0.001181, reg_loss: 36.474915\n","Epoch 343 | Total Loss: 0.001288, Loss: 0.001178, reg_loss: 36.484138\n","Epoch 344 | Total Loss: 0.001286, Loss: 0.001176, reg_loss: 36.493134\n","Epoch 345 | Total Loss: 0.001286, Loss: 0.001177, reg_loss: 36.502182\n","Epoch 346 | Total Loss: 0.001287, Loss: 0.001178, reg_loss: 36.511692\n","Epoch 347 | Total Loss: 0.001288, Loss: 0.001178, reg_loss: 36.519993\n","Epoch 348 | Total Loss: 0.001286, Loss: 0.001176, reg_loss: 36.529785\n","Epoch 349 | Total Loss: 0.001284, Loss: 0.001174, reg_loss: 36.537842\n","Epoch 350 | Total Loss: 0.001284, Loss: 0.001174, reg_loss: 36.547409\n","Epoch 351 | Total Loss: 0.001285, Loss: 0.001176, reg_loss: 36.555561\n","Epoch 352 | Total Loss: 0.001288, Loss: 0.001178, reg_loss: 36.564697\n","Epoch 353 | Total Loss: 0.001287, Loss: 0.001178, reg_loss: 36.573330\n","Epoch 354 | Total Loss: 0.001284, Loss: 0.001174, reg_loss: 36.581699\n","Epoch 355 | Total Loss: 0.001281, Loss: 0.001172, reg_loss: 36.590569\n","Epoch 356 | Total Loss: 0.001282, Loss: 0.001172, reg_loss: 36.599392\n","Epoch 357 | Total Loss: 0.001285, Loss: 0.001175, reg_loss: 36.608101\n","Epoch 358 | Total Loss: 0.001289, Loss: 0.001179, reg_loss: 36.617455\n","Epoch 359 | Total Loss: 0.001290, Loss: 0.001180, reg_loss: 36.624939\n","Epoch 360 | Total Loss: 0.001289, Loss: 0.001179, reg_loss: 36.634342\n","Epoch 361 | Total Loss: 0.001287, Loss: 0.001177, reg_loss: 36.641567\n","Epoch 362 | Total Loss: 0.001286, Loss: 0.001176, reg_loss: 36.650723\n","Epoch 363 | Total Loss: 0.001286, Loss: 0.001176, reg_loss: 36.658756\n","Epoch 364 | Total Loss: 0.001287, Loss: 0.001177, reg_loss: 36.667450\n","Epoch 365 | Total Loss: 0.001287, Loss: 0.001177, reg_loss: 36.676064\n","Epoch 366 | Total Loss: 0.001285, Loss: 0.001175, reg_loss: 36.684116\n","Epoch 367 | Total Loss: 0.001282, Loss: 0.001172, reg_loss: 36.693161\n","Epoch 368 | Total Loss: 0.001281, Loss: 0.001171, reg_loss: 36.701168\n","Epoch 369 | Total Loss: 0.001281, Loss: 0.001171, reg_loss: 36.710175\n","Epoch 370 | Total Loss: 0.001282, Loss: 0.001172, reg_loss: 36.718864\n","Epoch 371 | Total Loss: 0.001284, Loss: 0.001173, reg_loss: 36.727913\n","Epoch 372 | Total Loss: 0.001283, Loss: 0.001173, reg_loss: 36.736893\n","Epoch 373 | Total Loss: 0.001281, Loss: 0.001171, reg_loss: 36.745308\n","Epoch 374 | Total Loss: 0.001279, Loss: 0.001168, reg_loss: 36.754284\n","Epoch 375 | Total Loss: 0.001279, Loss: 0.001169, reg_loss: 36.762871\n","Epoch 376 | Total Loss: 0.001281, Loss: 0.001171, reg_loss: 36.771656\n","Epoch 377 | Total Loss: 0.001283, Loss: 0.001172, reg_loss: 36.780487\n","Epoch 378 | Total Loss: 0.001282, Loss: 0.001172, reg_loss: 36.788368\n","Epoch 379 | Total Loss: 0.001282, Loss: 0.001172, reg_loss: 36.796703\n","Epoch 380 | Total Loss: 0.001283, Loss: 0.001172, reg_loss: 36.804726\n","Epoch 381 | Total Loss: 0.001284, Loss: 0.001173, reg_loss: 36.812817\n","Epoch 382 | Total Loss: 0.001283, Loss: 0.001173, reg_loss: 36.821926\n","Epoch 383 | Total Loss: 0.001282, Loss: 0.001172, reg_loss: 36.829498\n","Epoch 384 | Total Loss: 0.001283, Loss: 0.001173, reg_loss: 36.838753\n","Epoch 385 | Total Loss: 0.001287, Loss: 0.001176, reg_loss: 36.846397\n","Epoch 386 | Total Loss: 0.001290, Loss: 0.001179, reg_loss: 36.855137\n","Epoch 387 | Total Loss: 0.001289, Loss: 0.001179, reg_loss: 36.862244\n","Epoch 388 | Total Loss: 0.001287, Loss: 0.001176, reg_loss: 36.870312\n","Epoch 389 | Total Loss: 0.001285, Loss: 0.001174, reg_loss: 36.877316\n","Epoch 390 | Total Loss: 0.001284, Loss: 0.001173, reg_loss: 36.885269\n","Epoch 391 | Total Loss: 0.001283, Loss: 0.001172, reg_loss: 36.892471\n","Epoch 392 | Total Loss: 0.001282, Loss: 0.001171, reg_loss: 36.899940\n","Epoch 393 | Total Loss: 0.001282, Loss: 0.001171, reg_loss: 36.908089\n","Epoch 394 | Total Loss: 0.001281, Loss: 0.001171, reg_loss: 36.914997\n","Epoch 395 | Total Loss: 0.001280, Loss: 0.001169, reg_loss: 36.923313\n","Epoch 396 | Total Loss: 0.001276, Loss: 0.001165, reg_loss: 36.930550\n","Epoch 397 | Total Loss: 0.001273, Loss: 0.001162, reg_loss: 36.938492\n","Epoch 398 | Total Loss: 0.001274, Loss: 0.001163, reg_loss: 36.946960\n","Epoch 399 | Total Loss: 0.001278, Loss: 0.001167, reg_loss: 36.954910\n","Epoch 400 | Total Loss: 0.001281, Loss: 0.001170, reg_loss: 36.963486\n","Epoch 401 | Total Loss: 0.001281, Loss: 0.001171, reg_loss: 36.971962\n","Epoch 402 | Total Loss: 0.001281, Loss: 0.001170, reg_loss: 36.979721\n","Epoch 403 | Total Loss: 0.001282, Loss: 0.001171, reg_loss: 36.988495\n","Epoch 404 | Total Loss: 0.001286, Loss: 0.001175, reg_loss: 36.996395\n","Epoch 405 | Total Loss: 0.001290, Loss: 0.001179, reg_loss: 37.004539\n","Epoch 406 | Total Loss: 0.001289, Loss: 0.001178, reg_loss: 37.012543\n","Epoch 407 | Total Loss: 0.001283, Loss: 0.001172, reg_loss: 37.019768\n","Epoch 408 | Total Loss: 0.001277, Loss: 0.001166, reg_loss: 37.027500\n","Epoch 409 | Total Loss: 0.001275, Loss: 0.001164, reg_loss: 37.035545\n","Epoch 410 | Total Loss: 0.001276, Loss: 0.001165, reg_loss: 37.043541\n","Epoch 411 | Total Loss: 0.001277, Loss: 0.001166, reg_loss: 37.052074\n","Epoch 412 | Total Loss: 0.001276, Loss: 0.001164, reg_loss: 37.060345\n","Epoch 413 | Total Loss: 0.001273, Loss: 0.001162, reg_loss: 37.068378\n","Epoch 414 | Total Loss: 0.001272, Loss: 0.001161, reg_loss: 37.077412\n","Epoch 415 | Total Loss: 0.001272, Loss: 0.001161, reg_loss: 37.084927\n","Epoch 416 | Total Loss: 0.001272, Loss: 0.001161, reg_loss: 37.094223\n","Epoch 417 | Total Loss: 0.001272, Loss: 0.001160, reg_loss: 37.101871\n","Epoch 418 | Total Loss: 0.001272, Loss: 0.001161, reg_loss: 37.110989\n","Epoch 419 | Total Loss: 0.001274, Loss: 0.001163, reg_loss: 37.119869\n","Epoch 420 | Total Loss: 0.001276, Loss: 0.001164, reg_loss: 37.128353\n","Epoch 421 | Total Loss: 0.001276, Loss: 0.001165, reg_loss: 37.137005\n","Epoch 422 | Total Loss: 0.001277, Loss: 0.001166, reg_loss: 37.144890\n","Epoch 423 | Total Loss: 0.001280, Loss: 0.001169, reg_loss: 37.153179\n","Epoch 424 | Total Loss: 0.001282, Loss: 0.001171, reg_loss: 37.160820\n","Epoch 425 | Total Loss: 0.001282, Loss: 0.001170, reg_loss: 37.168686\n","Epoch 426 | Total Loss: 0.001278, Loss: 0.001167, reg_loss: 37.175896\n","Epoch 427 | Total Loss: 0.001276, Loss: 0.001165, reg_loss: 37.183697\n","Epoch 428 | Total Loss: 0.001278, Loss: 0.001167, reg_loss: 37.191227\n","Epoch 429 | Total Loss: 0.001281, Loss: 0.001170, reg_loss: 37.199203\n","Epoch 430 | Total Loss: 0.001281, Loss: 0.001169, reg_loss: 37.206455\n","Epoch 431 | Total Loss: 0.001277, Loss: 0.001166, reg_loss: 37.214405\n","Epoch 432 | Total Loss: 0.001275, Loss: 0.001163, reg_loss: 37.221199\n","Epoch 433 | Total Loss: 0.001276, Loss: 0.001164, reg_loss: 37.229767\n","Epoch 434 | Total Loss: 0.001278, Loss: 0.001167, reg_loss: 37.236908\n","Epoch 435 | Total Loss: 0.001279, Loss: 0.001168, reg_loss: 37.244720\n","Epoch 436 | Total Loss: 0.001278, Loss: 0.001166, reg_loss: 37.252232\n","Epoch 437 | Total Loss: 0.001276, Loss: 0.001164, reg_loss: 37.259102\n","Epoch 438 | Total Loss: 0.001276, Loss: 0.001164, reg_loss: 37.267624\n","Epoch 439 | Total Loss: 0.001279, Loss: 0.001167, reg_loss: 37.274837\n","Epoch 440 | Total Loss: 0.001282, Loss: 0.001170, reg_loss: 37.282795\n","Epoch 441 | Total Loss: 0.001282, Loss: 0.001170, reg_loss: 37.289673\n","Epoch 442 | Total Loss: 0.001277, Loss: 0.001165, reg_loss: 37.296623\n","Epoch 443 | Total Loss: 0.001272, Loss: 0.001160, reg_loss: 37.304192\n","Epoch 444 | Total Loss: 0.001271, Loss: 0.001159, reg_loss: 37.310886\n","Epoch 445 | Total Loss: 0.001272, Loss: 0.001160, reg_loss: 37.318928\n","Epoch 446 | Total Loss: 0.001273, Loss: 0.001161, reg_loss: 37.325493\n","Epoch 447 | Total Loss: 0.001273, Loss: 0.001161, reg_loss: 37.333748\n","Epoch 448 | Total Loss: 0.001271, Loss: 0.001159, reg_loss: 37.341362\n","Epoch 449 | Total Loss: 0.001271, Loss: 0.001159, reg_loss: 37.349243\n","Epoch 450 | Total Loss: 0.001270, Loss: 0.001158, reg_loss: 37.357159\n","Epoch 451 | Total Loss: 0.001270, Loss: 0.001158, reg_loss: 37.363873\n","Epoch 452 | Total Loss: 0.001272, Loss: 0.001159, reg_loss: 37.371590\n","Epoch 453 | Total Loss: 0.001273, Loss: 0.001161, reg_loss: 37.378689\n","Epoch 454 | Total Loss: 0.001275, Loss: 0.001163, reg_loss: 37.386330\n","Epoch 455 | Total Loss: 0.001275, Loss: 0.001162, reg_loss: 37.394211\n","Epoch 456 | Total Loss: 0.001274, Loss: 0.001162, reg_loss: 37.400867\n","Epoch 457 | Total Loss: 0.001275, Loss: 0.001162, reg_loss: 37.409405\n","Epoch 458 | Total Loss: 0.001276, Loss: 0.001164, reg_loss: 37.415577\n","Epoch 459 | Total Loss: 0.001277, Loss: 0.001164, reg_loss: 37.424244\n","Epoch 460 | Total Loss: 0.001274, Loss: 0.001162, reg_loss: 37.430054\n","Epoch 461 | Total Loss: 0.001270, Loss: 0.001158, reg_loss: 37.437653\n","Epoch 462 | Total Loss: 0.001267, Loss: 0.001155, reg_loss: 37.444885\n","Epoch 463 | Total Loss: 0.001268, Loss: 0.001155, reg_loss: 37.452488\n","Epoch 464 | Total Loss: 0.001272, Loss: 0.001160, reg_loss: 37.460686\n","Epoch 465 | Total Loss: 0.001278, Loss: 0.001166, reg_loss: 37.467541\n","Epoch 466 | Total Loss: 0.001283, Loss: 0.001171, reg_loss: 37.475559\n","Epoch 467 | Total Loss: 0.001283, Loss: 0.001171, reg_loss: 37.482121\n","Epoch 468 | Total Loss: 0.001280, Loss: 0.001167, reg_loss: 37.489681\n","Epoch 469 | Total Loss: 0.001276, Loss: 0.001164, reg_loss: 37.496216\n","Epoch 470 | Total Loss: 0.001274, Loss: 0.001162, reg_loss: 37.503529\n","Epoch 471 | Total Loss: 0.001275, Loss: 0.001162, reg_loss: 37.510204\n","Epoch 472 | Total Loss: 0.001276, Loss: 0.001163, reg_loss: 37.517750\n","Epoch 473 | Total Loss: 0.001276, Loss: 0.001164, reg_loss: 37.524605\n","Epoch 474 | Total Loss: 0.001275, Loss: 0.001163, reg_loss: 37.531906\n","Epoch 475 | Total Loss: 0.001273, Loss: 0.001161, reg_loss: 37.538532\n","Epoch 476 | Total Loss: 0.001272, Loss: 0.001159, reg_loss: 37.546307\n","Epoch 477 | Total Loss: 0.001272, Loss: 0.001159, reg_loss: 37.552750\n","Epoch 478 | Total Loss: 0.001273, Loss: 0.001161, reg_loss: 37.560822\n","Epoch 479 | Total Loss: 0.001275, Loss: 0.001162, reg_loss: 37.566570\n","Epoch 480 | Total Loss: 0.001275, Loss: 0.001162, reg_loss: 37.574440\n","Epoch 481 | Total Loss: 0.001272, Loss: 0.001159, reg_loss: 37.580532\n","Epoch 482 | Total Loss: 0.001269, Loss: 0.001156, reg_loss: 37.587933\n","Epoch 483 | Total Loss: 0.001267, Loss: 0.001155, reg_loss: 37.595280\n","Epoch 484 | Total Loss: 0.001269, Loss: 0.001156, reg_loss: 37.602001\n","Epoch 485 | Total Loss: 0.001271, Loss: 0.001158, reg_loss: 37.609627\n","Epoch 486 | Total Loss: 0.001272, Loss: 0.001160, reg_loss: 37.616489\n","Epoch 487 | Total Loss: 0.001271, Loss: 0.001159, reg_loss: 37.624008\n","Epoch 488 | Total Loss: 0.001269, Loss: 0.001156, reg_loss: 37.631832\n","Epoch 489 | Total Loss: 0.001268, Loss: 0.001156, reg_loss: 37.638359\n","Epoch 490 | Total Loss: 0.001269, Loss: 0.001156, reg_loss: 37.646873\n","Epoch 491 | Total Loss: 0.001270, Loss: 0.001157, reg_loss: 37.652714\n","Epoch 492 | Total Loss: 0.001271, Loss: 0.001158, reg_loss: 37.660725\n","Epoch 493 | Total Loss: 0.001271, Loss: 0.001158, reg_loss: 37.666737\n","Epoch 494 | Total Loss: 0.001271, Loss: 0.001158, reg_loss: 37.674469\n","Epoch 495 | Total Loss: 0.001273, Loss: 0.001160, reg_loss: 37.681728\n","Epoch 496 | Total Loss: 0.001274, Loss: 0.001161, reg_loss: 37.688843\n","Epoch 497 | Total Loss: 0.001274, Loss: 0.001161, reg_loss: 37.696423\n","Epoch 498 | Total Loss: 0.001273, Loss: 0.001160, reg_loss: 37.703407\n","Epoch 499 | Total Loss: 0.001272, Loss: 0.001159, reg_loss: 37.710762\n"]}],"source":["# @title FABLE: Based on 3d Wavelet Transform\n","!pip install PyWavelets\n","\n","!pip install PyWavelets\n","\n","import torch\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import gc\n","import yaml\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pywt\n","import pandas as pd\n","import pickle\n","import torch\n","from matplotlib.colors import LinearSegmentedColormap\n","from matplotlib.ticker import MaxNLocator\n","import cartopy.crs as ccrs\n","import cartopy.feature as cfeature\n","\n","def haar_3d_inverse(LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH):\n","    \"\"\"\n","    Single-level 3D Haar inverse transform.\n","    Each sub-band has shape (B, T2, H2, W2).\n","    The output will have shape (B, T, H, W),\n","    where T=2*T2, H=2*H2, W=2*W2.\n","    \"\"\"\n","    B, T2, H2, W2 = LLL.shape\n","    T = T2 * 2\n","    H = H2 * 2\n","    W = W2 * 2\n","\n","    inv_sqrt8 = 1.0 / math.sqrt(8.0)\n","    x_recon = torch.zeros((B, T, H, W), device=LLL.device, dtype=LLL.dtype)\n","\n","    x_recon[:, ::2, ::2, ::2]   = (LLL + LLH + LHL + LHH + HLL + HLH + HHL + HHH) * inv_sqrt8\n","    x_recon[:, ::2, ::2, 1::2]  = (LLL - LLH + LHL - LHH + HLL - HLH + HHL - HHH) * inv_sqrt8\n","    x_recon[:, ::2, 1::2, ::2]  = (LLL + LLH - LHL - LHH + HLL + HLH - HHL - HHH) * inv_sqrt8\n","    x_recon[:, ::2, 1::2, 1::2] = (LLL - LLH - LHL + LHH + HLL - HLH - HHL + HHH) * inv_sqrt8\n","\n","    x_recon[:, 1::2, ::2, ::2]  = (LLL + LLH + LHL + LHH - HLL - HLH - HHL - HHH) * inv_sqrt8\n","    x_recon[:, 1::2, ::2, 1::2] = (LLL - LLH + LHL - LHH - HLL + HLH - HHL + HHH) * inv_sqrt8\n","    x_recon[:, 1::2, 1::2, ::2] = (LLL + LLH - LHL - LHH - HLL - HLH + HHL + HHH) * inv_sqrt8\n","    x_recon[:, 1::2, 1::2, 1::2] = (LLL - LLH - LHL + LHH - HLL + HLH + HHL - HHH) * inv_sqrt8\n","\n","    # x_recon.shape: torch.Size([32, 8, 28, 58])\n","\n","    return x_recon\n","\n","def wavelet_decompose_3d_pywt(x_4d):\n","    \"\"\"\n","    Single-level 3D wavelet transform with pywt.dwtn along (T,H,W).\n","    x_4d shape: (B, T, H, W).\n","    Returns dict of 8 sub-bands: 'LLL','LLH','LHL','LHH','HLL','HLH','HHL','HHH'.\n","    Each sub-band has shape (B, T/2, H/2, W/2).\n","    \"\"\"\n","    x_np = x_4d.detach().cpu().numpy()                                          # Convert to numpy if needed\n","    # PyWavelets forward 3D transform\n","    coeffs_dict = pywt.dwtn(x_np, wavelet='haar', axes=(1,2,3))\n","    # Convert back to torch\n","    LLL = torch.tensor(coeffs_dict['aaa'], device=x_4d.device, dtype=x_4d.dtype)\n","    LLH = torch.tensor(coeffs_dict['aad'], device=x_4d.device, dtype=x_4d.dtype)\n","    LHL = torch.tensor(coeffs_dict['ada'], device=x_4d.device, dtype=x_4d.dtype)\n","    LHH = torch.tensor(coeffs_dict['add'], device=x_4d.device, dtype=x_4d.dtype)\n","    HLL = torch.tensor(coeffs_dict['daa'], device=x_4d.device, dtype=x_4d.dtype)\n","    HLH = torch.tensor(coeffs_dict['dad'], device=x_4d.device, dtype=x_4d.dtype)\n","    HHL = torch.tensor(coeffs_dict['dda'], device=x_4d.device, dtype=x_4d.dtype)\n","    HHH = torch.tensor(coeffs_dict['ddd'], device=x_4d.device, dtype=x_4d.dtype)\n","\n","    return {'LLL': LLL, 'LLH': LLH, 'LHL': LHL, 'LHH': LHH, 'HLL': HLL, 'HLH': HLH, 'HHL': HHL, 'HHH': HHH}\n","\n","\n","class FABLE:\n","    def __init__(self, forward_func, wavelet, level, lons, lats):\n","        self.forward_func = forward_func\n","        self.wavelet = wavelet\n","        self.level = level\n","        self.lons = lons\n","        self.lats = lats\n","        self.lr = lr\n","\n","    def rmse(self, pred, target):\n","        loss = F.mse_loss(pred, target)\n","        return torch.sqrt(loss)\n","\n","    def wavelet_decompose_3d(self, data_4d):\n","        return wavelet_decompose_3d_pywt(data_4d)\n","\n","    def wavelet_reconstruct_3d(self, coeffs_dict):\n","        return haar_3d_inverse(\n","            coeffs_dict['LLL'], coeffs_dict['LLH'], coeffs_dict['LHL'], coeffs_dict['LHH'],\n","            coeffs_dict['HLL'], coeffs_dict['HLH'], coeffs_dict['HHL'], coeffs_dict['HHH']\n","        )\n","\n","    def create_regular_padded(self, lons, lats, values, fill_value=0.0):\n","        unique_lons = sorted(list(set(lons)))\n","        unique_lats = sorted(list(set(lats)))\n","        W = len(unique_lons)\n","        H = len(unique_lats)\n","\n","        padded_data = np.full((H, W), fill_value, dtype=np.float32)\n","        lon_to_x = {val: idx for idx, val in enumerate(unique_lons)}\n","        lat_to_y = {val: idx for idx, val in enumerate(unique_lats)}\n","\n","        for lon, lat, val in zip(lons, lats, values):\n","            ix = lon_to_x[lon]\n","            iy = lat_to_y[lat]\n","            padded_data[iy, ix] = val\n","\n","        return padded_data, unique_lons, unique_lats\n","\n","    def clamp_data(self, perturbed_data, original_data, clampEpsilon, mean, std):\n","        lower_bound = torch.maximum(original_data - clampEpsilon, torch.tensor(min_std, device=device))\n","        upper_bound = torch.minimum(original_data + clampEpsilon, torch.tensor(max_std, device=device))\n","\n","        return torch.clamp(perturbed_data, lower_bound, upper_bound)\n","\n","    def calculate_regularization_loss(self, perturbed_param_vector, original_param_vector, coeff_weights):\n","        reg_loss = 0\n","        key_to_index = {\n","            'LLL': 0, 'LLH': 1, 'LHL': 2, 'LHH': 3,\n","            'HLL': 4, 'HLH': 5, 'HHL': 6, 'HHH': 7\n","        }\n","        for key, weight in coeff_weights.items():\n","            index = key_to_index[key]\n","            diff = perturbed_param_vector[:, index, :, :, :] - original_param_vector[:, index, :, :, :]\n","            reg_loss += weight * torch.norm(diff, p=reg_p)\n","        return reg_loss\n","\n","    def _forward_loss_given_params(\n","        self, param_vector,\n","        original_inputs,\n","        x_indices, y_indices,\n","        batch_size, seq_length,\n","        clampEpsilon, mean, std,\n","        target_torch, device\n","    ):\n","        \"\"\"\n","        param_vector shape: (B, 8, T/2, H/2, W/2).\n","        Reconstruct with manual 3D inverse Haar.\n","        Map (B, T, H, W) -> (T, B, locations, 1), clamp, then compute loss.\n","        \"\"\"\n","        # Build a dict of sub-bands\n","        coeffs_dict = {\n","            'LLL': param_vector[:, 0, :, :, :],\n","            'LLH': param_vector[:, 1, :, :, :],\n","            'LHL': param_vector[:, 2, :, :, :],\n","            'LHH': param_vector[:, 3, :, :, :],\n","            'HLL': param_vector[:, 4, :, :, :],\n","            'HLH': param_vector[:, 5, :, :, :],\n","            'HHL': param_vector[:, 6, :, :, :],\n","            'HHH': param_vector[:, 7, :, :, :],\n","        }\n","\n","        reconstructed_4d = self.wavelet_reconstruct_3d(coeffs_dict)             # Reconstruct: shape (B, T, H, W)\n","        recon_perm = reconstructed_4d.permute(1, 0, 2, 3)                       # Permute to (T, B, H, W)\n","\n","        # Flatten so we can gather\n","        width = recon_perm.shape[-1]                                            # recon_perm: (T,B,H,W)\n","        recon_2d = recon_perm.reshape(seq_length, batch_size, -1)               # shape => (T,B,H*W)\n","\n","        # Gather only the valid indices\n","        linear_idx = y_indices * width + x_indices\n","        perturbed_inputs_2d = torch.gather(\n","            recon_2d, dim=2,\n","            index=linear_idx.unsqueeze(0).unsqueeze(0).expand(seq_length, batch_size, -1)\n","        )                                                                       # (T,B,locations)\n","        perturbed_inputs_4d = perturbed_inputs_2d.unsqueeze(-1)                 # (T,B,locations,1)\n","\n","        # Clamp\n","        perturbed_inputs_4d = self.clamp_data(perturbed_inputs_4d, original_inputs, clampEpsilon, mean, std)\n","\n","        # Forward pass\n","        forecast = self.forward_func(perturbed_inputs_4d.to(device))\n","        loss_val = self.rmse(forecast, target_torch)\n","        # print(forecast.shape, target_torch.shape)\n","        loss_val = self.rmse(forecast, target_torch)\n","        return perturbed_inputs_4d, loss_val\n","\n","    def perturb(self, inputs, target, clampEpsilon, step_num, mean, std):\n","        \"\"\"\n","        inputs: (T,B,locations,1)\n","        target: (T,B,locations,1)\n","        \"\"\"\n","        device = inputs.device\n","        self.forward_func.eval()\n","\n","        seq_length, batch_size, locations, _ = inputs.shape\n","\n","        # Example: record lon/lat for each time step in each batch\n","        ulonlat_record = []\n","        padded_data_batch = []\n","\n","        # Build a (B,T,H,W) data array\n","        for b_idx in range(batch_size):\n","            time_step_padded = []\n","            time_step_ulonlat = []\n","            for t in range(seq_length):\n","                data_1d = inputs[t, b_idx, :, 0].detach().cpu().numpy()\n","                padded_data, unique_lons, unique_lats = self.create_regular_padded(\n","                    self.lons, self.lats, data_1d, fill_value=0.0\n","                )\n","                time_step_padded.append(padded_data)\n","                time_step_ulonlat.append((unique_lons, unique_lats))\n","            padded_data_batch.append(np.stack(time_step_padded, axis=0))\n","            ulonlat_record.append(time_step_ulonlat)\n","\n","        padded_data_batch = np.stack(padded_data_batch, axis=0)  # (B, T, H, W)\n","        padded_data_batch_torch = torch.tensor(padded_data_batch, dtype=torch.float32, device=device)\n","\n","        # Record the valid indices in the tensor 'padded_data_batch'\n","        unique_lons, unique_lats = ulonlat_record[0][0]  # use the first sample's mapping\n","        lon_to_x = {val: idx for idx, val in enumerate(unique_lons)}\n","        lat_to_y = {val: idx for idx, val in enumerate(unique_lats)}\n","        valid_indices = [\n","            (lon_to_x[lon], lat_to_y[lat])\n","            for lon, lat in zip(self.lons, self.lats)\n","            if lon in lon_to_x and lat in lat_to_y\n","        ]\n","        x_indices, y_indices = zip(*valid_indices)\n","        x_indices = torch.tensor(x_indices, dtype=torch.long, device=device)\n","        y_indices = torch.tensor(y_indices, dtype=torch.long, device=device)\n","\n","        # 3D forward wavelet transform with pywt\n","        coeffs = self.wavelet_decompose_3d(padded_data_batch_torch)\n","        # Build param_vector => shape (B, 8, T/2, H/2, W/2)\n","        param_vector = torch.stack([\n","            coeffs['LLL'], coeffs['LLH'], coeffs['LHL'], coeffs['LHH'],\n","            coeffs['HLL'], coeffs['HLH'], coeffs['HHL'], coeffs['HHH']\n","        ], dim=1)\n","\n","        key_to_index = {'LLL': 0, 'LLH': 1, 'LHL': 2, 'LHH': 3, 'HLL': 4, 'HLH': 5, 'HHL': 6, 'HHH': 7}\n","        # Suppose we only want to attack LLL, LHH => for example\n","        target_indices = [key_to_index[k] for k in target_key]\n","        non_target_indices = [i for i in range(8) if i not in target_indices]\n","\n","        # Keep a copy of the original sub-bands for the non-target ones\n","        unchanged_param = param_vector[:, non_target_indices, :, :, :].detach().clone()\n","\n","        perturbed_param_vector = param_vector.detach().clone()\n","        perturbed_param_vector.requires_grad_(True)\n","\n","        target_torch = target.clone().detach().to(device)\n","\n","        # For example, use optimizer = 'Adam'\n","        if use_optimizer == 'Adam':\n","            optimizer = torch.optim.Adam([perturbed_param_vector], lr=self.lr)\n","\n","        best_loss = float('inf')\n","        best_perturbed_inputs = None\n","\n","        for epoch in range(step_num):\n","            optimizer.zero_grad()\n","            perturbed_inputs, loss = self._forward_loss_given_params(\n","                perturbed_param_vector,\n","                inputs,\n","                x_indices, y_indices,\n","                batch_size=batch_size,\n","                seq_length=seq_length,\n","                clampEpsilon=clampEpsilon,\n","                mean=mean,\n","                std=std,\n","                target_torch=target_torch,\n","                device=device\n","            )\n","\n","            ### regularizer\n","            if reg == 1:\n","                reg_loss = self.calculate_regularization_loss(perturbed_param_vector, param_vector, coeff_weights)\n","                total_loss = loss + lambda_reg * reg_loss\n","                print(f\"Epoch {epoch} | Total Loss: {total_loss.item():.6f}, Loss: {loss.item():.6f}, reg_loss: {reg_loss:.6f}\")\n","                total_loss.backward()\n","            else:\n","                print(f'Epoch {epoch}', ' | Loss:', loss.item(), ' | GradNorm:', torch.norm(perturbed_param_vector.grad.data).item())\n","                loss.backward()\n","\n","            # Zero out grad for non-target sub-bands\n","            if perturbed_param_vector.grad is not None:\n","                perturbed_param_vector.grad[:, non_target_indices, :, :, :] = 0\n","\n","            optimizer.step()\n","\n","            # Restore non-target sub-bands\n","            with torch.no_grad():\n","                perturbed_param_vector[:, non_target_indices, :, :, :] = unchanged_param\n","\n","            if loss.item() < best_loss:\n","                best_loss = loss.item()\n","                best_perturbed_inputs = perturbed_inputs\n","\n","        return best_perturbed_inputs\n","\n","def main():\n","    with open(\"experiments/config_clcrn_tmp2m.yaml\", 'r') as f:\n","        clcrnConfig = yaml.load(f, Loader=yaml.SafeLoader)\n","\n","    clcrnModelTrained = Supervisor(**clcrnConfig)\n","    clcrnModelTrained.load_model(best_CLCRN_model_epoch)\n","    valIterator = clcrnModelTrained._data['{}_loader'.format('test')]\n","\n","    lonlatAddr = \"data/tmp2m/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","\n","    fable = FABLE(forward_func=clcrnModelTrained.model, wavelet='haar', level=1, lons=lons, lats=lats)\n","\n","    data_list = []\n","    breakFlag = 0\n","\n","    print(f'total implpementations: {implementations}')\n","    for N in range(implementations):\n","        print(f'Random Implementation {N}')\n","        for idx, (x, y) in enumerate(valIterator):\n","            if x.shape[0] != batch_size:\n","                break\n","\n","            print(idx)\n","            # if idx == 1:\n","            #     break\n","\n","            thisX = x.permute(1, 0, 2, 3).to('cuda')\n","            clcrnModelTrained.model.eval()\n","\n","            if caseStudy == 0:\n","                predList = torch.load(f'predList_tmp2m_{N}.pth')\n","                targetList = torch.load(f'targetList_tmp2m_{N}.pth')\n","            else:\n","                predList = torch.load(f'case_predList_tmp2m_{N}.pth')\n","                targetList = torch.load(f'case_targetList_tmp2m_{N}.pth')\n","\n","            yPred = predList[idx].permute(1, 0, 2, 3).to('cuda')\n","            yTarget = targetList[idx].permute(1, 0, 2, 3).to('cuda')\n","\n","            xPerturbed = fable.perturb(thisX, yTarget, clampEpsilon, stepNum, mean, std)  # thisX.shape: (time_step, batch_size, locations, 1), e.g., [7, 32, 1320, 1]\n","            yPerturbed = clcrnModelTrained.model(xPerturbed.to('cuda'))\n","\n","            for b in range(x.shape[0]):\n","                data_list.append({\n","                    \"thisX\": thisX[:, b, :, :].detach().cpu().numpy().tolist(),\n","                    \"yPred\": yPred[:, b, :, :].detach().cpu().numpy().tolist(),\n","                    \"yTarget\": yTarget[:, b, :, :].detach().cpu().numpy().tolist(),\n","                    \"xPerturbed\": xPerturbed[:, b, :, :].detach().cpu().numpy().tolist(),\n","                    \"yPerturbed\": yPerturbed[:, b, :, :].detach().cpu().numpy().tolist(),\n","                })\n","\n","            df = pd.DataFrame(data_list)\n","            if caseStudy == 0:\n","                df.to_csv(f\"results/FABLE_{dataName}_{clampEpsilon}_{stepNum}_3d_{fix}_{implementations}.csv\", index=False)\n","            else:\n","                df.to_csv(f\"results/FABLE_{dataName}_{clampEpsilon}_{stepNum}_3d_{fix}_caseStudy_{implementations}.csv\", index=False)\n","\n","            if caseStudy == 1:\n","                testBatchNum = 0\n","                if breakFlag == testBatchNum:\n","                    break\n","                breakFlag += 1\n","\n","            del thisX, yPred, yTarget, xPerturbed, yPerturbed\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XXGMsFBI6RNP"},"outputs":[],"source":["# @title Visualize 2D Decomposition based on 2D Wavelet Transform on NLDAS-apcpsfc + Padding\n","# for NLDAS apcpsfc\n","# lon: 58; lat: 28\n","\n","# //MARK (yue, 1/2/2025): Address the boundary issues when doing wavelet transform is a contribution. (do not speculate but see the equation and collect mathematical proof)\n","\n","!pip install cartopy\n","!pip install torch_geometric\n","!pip install PyWavelets\n","\n","from google.colab import drive\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import LinearSegmentedColormap\n","from matplotlib.ticker import MaxNLocator\n","import pickle\n","import yaml\n","import torch\n","import torch.nn.functional as F\n","import re\n","import cartopy.crs as ccrs\n","import cartopy.feature as cfeature\n","import pywt\n","\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir('/content/drive/My Drive/CLCRN-main')\n","\n","# Updated color map for precipitation-like data\n","colors = [(0, \"white\"), (0.5, \"blue\"), (1, \"red\")]\n","custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n","\n","def create_regular_padded(lons, lats, values, fill_value=0.0):\n","    unique_lons = sorted(list(set(lons)))\n","    unique_lats = sorted(list(set(lats)))\n","    W = len(unique_lons)\n","    H = len(unique_lats)\n","\n","    padded_data = np.full((H, W), fill_value, dtype=np.float32)\n","\n","    lon_to_x = {val: idx for idx, val in enumerate(unique_lons)}\n","    lat_to_y = {val: idx for idx, val in enumerate(unique_lats)}\n","\n","    for lon, lat, val in zip(lons, lats, values):\n","        ix = lon_to_x[lon]\n","        iy = lat_to_y[lat]\n","        padded_data[iy, ix] = val\n","\n","    return padded_data, unique_lons, unique_lats\n","\n","def plot_2d_map(matrix_2d, vmin, vmax, title):\n","    fig = plt.figure(figsize=(6, 3))\n","    ax = fig.add_subplot(111)\n","    im = ax.imshow(matrix_2d, cmap=custom_cmap, origin='lower', vmin=vmin, vmax=vmax)\n","    cbar = plt.colorbar(im, orientation='vertical', pad=0.02, aspect=30)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","    plt.title(title)\n","    plt.show()\n","\n","def plotForecastingMap(thisData, vmin, vmax):\n","    lonlatAddr = \"data/apcpsfc/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","\n","    fig_width = 6\n","    fig_height = 3\n","    fig = plt.figure(figsize=(fig_width, fig_height))\n","    ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n","    ax.add_feature(cfeature.STATES, edgecolor='black')\n","    ax.coastlines()\n","    ax.set_extent([-125, -66, 24, 50], crs=ccrs.PlateCarree())\n","\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","\n","    sc = ax.scatter(\n","        lons,\n","        lats,\n","        c=thisData,\n","        cmap=custom_cmap,\n","        marker=',',\n","        edgecolor='none',\n","        linewidth=0,\n","        vmin=vmin,\n","        vmax=vmax\n","    )\n","\n","    cbar = plt.colorbar(sc, orientation='vertical', pad=0.02, aspect=15)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","\n","    plt.title(\"Original X (Raw)\", fontsize=12)\n","    plt.show()\n","\n","def wavelet_decompose_2d(data_2d, wavelet='haar', level=1):\n","    coeffs_2d = pywt.wavedec2(data_2d, wavelet=wavelet, level=level)\n","    return coeffs_2d\n","\n","def wavelet_reconstruct_2d(coeffs_2d, wavelet='haar'):\n","    return pywt.waverec2(coeffs_2d, wavelet=wavelet)\n","\n","def plot_wavelet_components(coeffs_2d):\n","    cA = coeffs_2d[0]\n","    details = coeffs_2d[1:]\n","\n","    fig, axes = plt.subplots(2, 2, figsize=(6, 3), gridspec_kw={'hspace': 0.05, 'wspace': 0.05})\n","    vmin = min(cA.min(), *(min(cH.min(), cV.min(), cD.min()) for cH, cV, cD in details))\n","    vmax = max(cA.max(), *(max(cH.max(), cV.max(), cD.max()) for cH, cV, cD in details))\n","\n","    im = axes[0, 0].imshow(cA, cmap=custom_cmap, origin='lower', vmin=vmin, vmax=vmax)\n","    axes[0, 0].axis('off')\n","    cH, cV, cD = details[0]\n","    axes[0, 1].imshow(cH, cmap=custom_cmap, origin='lower', vmin=vmin, vmax=vmax)\n","    axes[0, 1].axis('off')\n","    axes[1, 0].imshow(cV, cmap=custom_cmap, origin='lower', vmin=vmin, vmax=vmax)\n","    axes[1, 0].axis('off')\n","    axes[1, 1].imshow(cD, cmap=custom_cmap, origin='lower', vmin=vmin, vmax=vmax)\n","    axes[1, 1].axis('off')\n","\n","    # Shared colorbar for all components\n","    fig.subplots_adjust(right=0.92)\n","    cbar_ax = fig.add_axes([0.93, 0.15, 0.02, 0.7])\n","    fig.colorbar(im, cax=cbar_ax)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def perturb_wavelet_details(coeffs_2d, original_mask):\n","    random_noise_magnitude = 0.5\n","    perturbed_coeffs = coeffs_2d.copy()\n","    details = perturbed_coeffs[1:]\n","\n","    current_mask = original_mask\n","    for i, (cH, cV, cD) in enumerate(details):\n","        current_mask = current_mask[::2, ::2]                                   # Downsample by 2 (Haar wavelet effect)\n","\n","        cH += np.random.uniform(-random_noise_magnitude, random_noise_magnitude, size=cH.shape) * current_mask\n","        cV += np.random.uniform(-random_noise_magnitude, random_noise_magnitude, size=cV.shape) * current_mask\n","        cD += np.random.uniform(-random_noise_magnitude, random_noise_magnitude, size=cD.shape) * current_mask\n","\n","        details[i] = (cH, cV, cD)\n","\n","    perturbed_coeffs[1:] = details\n","    return perturbed_coeffs\n","\n","def compare_decomposed_and_perturbed(X_perturbed_padded, perturbed_coeffs_2d, wavelet, level):\n","    perturbed_coeffs_2d_again = wavelet_decompose_2d(X_perturbed_padded, wavelet=wavelet, level=level)\n","    plot_wavelet_components(perturbed_coeffs_2d_again)\n","\n","    cA_diff = np.abs(perturbed_coeffs_2d_again[0] - perturbed_coeffs_2d[0])\n","    details_diff = []\n","    for decomposed_detail, perturbed_detail in zip(perturbed_coeffs_2d_again[1:], perturbed_coeffs_2d[1:]):\n","        diff_cH = np.abs(decomposed_detail[0] - perturbed_detail[0])\n","        diff_cV = np.abs(decomposed_detail[1] - perturbed_detail[1])\n","        diff_cD = np.abs(decomposed_detail[2] - perturbed_detail[2])\n","        details_diff.append((diff_cH, diff_cV, diff_cD))\n","\n","    fig, axes = plt.subplots(2, 2, figsize=(6, 3), gridspec_kw={'hspace': 0.05, 'wspace': 0.05})\n","    im_cA = axes[0, 0].imshow(cA_diff, cmap=\"viridis\", origin=\"lower\")\n","    axes[0, 0].axis('off')\n","    im_cH = axes[0, 1].imshow(details_diff[0][0], cmap=\"viridis\", origin=\"lower\")\n","    axes[0, 1].axis('off')\n","    im_cV = axes[1, 0].imshow(details_diff[0][1], cmap=\"viridis\", origin=\"lower\")\n","    axes[1, 0].axis('off')\n","    im_cD = axes[1, 1].imshow(details_diff[0][2], cmap=\"viridis\", origin=\"lower\")\n","    axes[1, 1].axis('off')\n","    fig.subplots_adjust(right=0.9)\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n","    fig.colorbar(im_cA, cax=cbar_ax)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def main():\n","    df = pd.read_csv(\"results/FABLE_tmp2m_2.5_01000_2d_all.csv\")\n","    sampleNum = 10\n","\n","    X_original = torch.tensor(eval(df['thisX'][sampleNum]))[0, :, dim].cpu().numpy()\n","    lonlatAddr = \"data/tmp2m/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","\n","    vmax = X_original.max()\n","    vmin = X_original.min()\n","    plotForecastingMap(X_original, vmin, vmax)\n","\n","    if padding == 'values':\n","        X_original_padded, unique_lons, unique_lats = create_regular_padded(lons, lats, X_original, fill_value=0.0)\n","    vmax = X_original_padded.max()\n","    vmin = X_original_padded.min()\n","    plot_2d_map(X_original_padded, vmin, vmax, \"Original X (Padded)\")\n","\n","    # wavelet and level\n","    wavelet = 'haar'\n","    level = 1\n","    coeffs_2d = wavelet_decompose_2d(X_original_padded, wavelet=wavelet, level=level)\n","    plot_wavelet_components(coeffs_2d)\n","\n","    mask = (X_original_padded != 0).astype(np.float32)\n","    perturbed_coeffs_2d = perturb_wavelet_details(coeffs_2d, mask)\n","    plot_wavelet_components(perturbed_coeffs_2d)\n","\n","    X_perturbed_padded = wavelet_reconstruct_2d(perturbed_coeffs_2d, wavelet=wavelet)\n","    vmax = X_perturbed_padded.max()\n","    vmin = X_perturbed_padded.min()\n","    plot_2d_map(X_perturbed_padded, vmin, vmax, \"Perturbed X (Padded)\")\n","\n","    # To check whether 'X_perturbed_padded', which has been reconstructed with 'perturbed_coeffs_2d', would reproduce the same 'perturbed_coeffs_2d'\n","    compare_decomposed_and_perturbed(X_perturbed_padded, perturbed_coeffs_2d, wavelet=wavelet, level=level)\n","\n","if __name__ == '__main__':\n","    main()\n","\n","# //TODO:\n","# The relationship between A, D1, D2, D3? What will happen if we add them together?\n","# The decomposition unique? A, D1, D2', D3? A, D1', D2, D3?... Is there any randomness we concern about?\n","#   e.g. X->A D1 D2 D3 -> A D1' D2 D3 -> X' -> A'' D1'' D2'' D3'', (A D1' D2 D3)=(A'' D1'' D2'' D3'')?? Do not assume the whole code will give you the correct thing\n","#   May be Di has some autocorrelation so that we should not perturb them independently\n","# How about the 2d decomposition? Check it one by one\n","# Do we have a way to sum up D1 D2 D3 so we can write X=A+D? Do we better perturb D or perturb Di separately?\n","# What is the reconstruction process in terms of 2d Wavelet decomposition? How it will affect the results?\n","# To understand what is the best to put them all together.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mIBWH_6NSn1W"},"outputs":[],"source":["# @title Visualize 3D Decomposition based on 2D Wavelet Transform on NLDAS-apcpsfc + Padding\n","# for NLDAS apcpsfc\n","# lon: 58; lat: 28\n","\n","!pip install cartopy\n","!pip install torch_geometric\n","!pip install PyWavelets\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pywt\n","import pandas as pd\n","import pickle\n","import torch\n","from matplotlib.colors import LinearSegmentedColormap\n","from matplotlib.ticker import MaxNLocator\n","import cartopy.crs as ccrs\n","import cartopy.feature as cfeature\n","\n","colors = [(0, \"white\"), (0.5, \"blue\"), (1, \"red\")]\n","custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n","\n","def create_regular_padded(lons, lats, values, fill_value=0.0):\n","    \"\"\"\n","    Returns (padded_data, mask_data, unique_lons, unique_lats).\n","    mask_data has the same shape as padded_data, with 1 where actual data is placed,\n","    and 0 where fill_value is placed.\n","    \"\"\"\n","    unique_lons = sorted(list(set(lons)))\n","    unique_lats = sorted(list(set(lats)))\n","    W = len(unique_lons)\n","    H = len(unique_lats)\n","\n","    padded_data = np.full((H, W), fill_value, dtype=np.float32)\n","    mask_data   = np.zeros((H, W), dtype=np.float32)\n","\n","    lon_to_x = {val: idx for idx, val in enumerate(unique_lons)}\n","    lat_to_y = {val: idx for idx, val in enumerate(unique_lats)}\n","\n","    for lon, lat, val in zip(lons, lats, values):\n","        ix = lon_to_x[lon]\n","        iy = lat_to_y[lat]\n","        padded_data[iy, ix] = val\n","        mask_data[iy, ix]   = 1.0\n","\n","    return padded_data, mask_data, unique_lons, unique_lats\n","\n","def plotForecastingMapMultipleSteps(data_list, vmin, vmax, title):\n","    lonlatAddr = \"data/apcpsfc/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","\n","    fig_width = 6 * len(data_list)\n","    fig_height = 3\n","    fig, axes = plt.subplots(\n","        1, len(data_list),\n","        figsize=(fig_width, fig_height),\n","        subplot_kw={'projection': ccrs.PlateCarree()}\n","    )\n","\n","    if len(data_list) == 1:\n","        axes = [axes]\n","\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","\n","    for idx, (thisData, ax) in enumerate(zip(data_list, axes)):\n","        ax.add_feature(cfeature.STATES, edgecolor='black')\n","        ax.coastlines()\n","        ax.set_extent([-125, -66, 24, 50], crs=ccrs.PlateCarree())\n","\n","        sc = ax.scatter(\n","            lons,\n","            lats,\n","            c=thisData,\n","            cmap=custom_cmap,\n","            marker=',',\n","            edgecolor='none',\n","            linewidth=0,\n","            vmin=vmin,\n","            vmax=vmax\n","        )\n","        ax.set_title(f\"{title} T{idx+1}\", fontsize=10)\n","\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n","    cbar = fig.colorbar(sc, cax=cbar_ax)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","\n","    plt.tight_layout(rect=[0,0,0.9,1])\n","    plt.show()\n","\n","def plot_3d_map(matrix_3d, vmin, vmax, title):\n","    if isinstance(matrix_3d, list):\n","        matrix_3d = np.array(matrix_3d)\n","\n","    fig_width = 6 * matrix_3d.shape[0]\n","    fig_height = 3\n","    fig, axes = plt.subplots(\n","        1, matrix_3d.shape[0],\n","        figsize=(fig_width, fig_height)\n","    )\n","\n","    if matrix_3d.shape[0] == 1:\n","        axes = [axes]\n","\n","    for idx, (thisData, ax) in enumerate(zip(matrix_3d, axes)):\n","        im = ax.imshow(thisData, cmap=custom_cmap, origin='lower',\n","                       vmin=vmin, vmax=vmax)\n","        ax.set_title(f\"{title} T{idx+1}\", fontsize=10)\n","        ax.axis('off')\n","\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n","    cbar = fig.colorbar(im, cax=cbar_ax)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","\n","    plt.tight_layout(rect=[0,0,0.9,1])\n","    plt.show()\n","\n","def wavelet_transform_3d(data_3d, wavelet='haar'):\n","    \"\"\"\n","    Perform a 2D wavelet transform over axes=(1, 2) of data shaped (Time, H, W).\n","    Returns a dict of sub-band coefficients.\n","    Example of returned keys: 'aa','ad','da','dd'\n","    \"\"\"\n","    coeffs = pywt.dwtn(data_3d, wavelet=wavelet, axes=(1, 2))\n","    return coeffs\n","\n","def wavelet_inverse_transform_3d(coeffs, wavelet='haar'):\n","    \"\"\"\n","    Inverse of wavelet_transform_3d.\n","    Reconstructs data of shape (Time, H, W).\n","    \"\"\"\n","    reconstructed_data = pywt.idwtn(coeffs, wavelet=wavelet, axes=(1, 2))\n","    return reconstructed_data\n","\n","def plot_wavelet_coeffs(coeffs_dict, title):\n","    \"\"\"\n","    Plots a dictionary of wavelet coefficients in separate figures.\n","    Each subband is placed in its own figure, with time slices shown as subplots.\n","    \"\"\"\n","    subband_keys = list(coeffs_dict.keys())\n","    subband_keys.sort()\n","\n","    for sb_key in subband_keys:\n","        subband_data = coeffs_dict[sb_key]\n","        vmin_value = subband_data.min()\n","        vmax_value = subband_data.max()\n","\n","        time_len = subband_data.shape[0]\n","\n","        fig_width = 4.5 * time_len\n","        fig_height = 4.0\n","        fig, axes = plt.subplots(1, time_len, figsize=(fig_width, fig_height))\n","\n","        if time_len == 1:\n","            axes = np.array([axes])\n","\n","        for t in range(time_len):\n","            ax = axes[t]\n","            im = ax.imshow(\n","                subband_data[t],\n","                cmap=custom_cmap,\n","                origin='lower',\n","                vmin=vmin_value,\n","                vmax=vmax_value\n","            )\n","            ax.set_title(f\"{title} - {sb_key} (T{t+1})\", fontsize=8)\n","            ax.axis('off')\n","\n","        cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.5])\n","        cbar = fig.colorbar(im, cax=cbar_ax)\n","        cbar.locator = MaxNLocator(nbins=5)\n","        cbar.update_ticks()\n","\n","        plt.tight_layout(rect=[0,0,0.9,1])\n","        plt.show()\n","\n","def plot_wavelet_coeffs_difference(coeffs_dict_1, coeffs_dict_2, title):\n","    subband_keys = sorted(coeffs_dict_1.keys())\n","    time_len = coeffs_dict_1[subband_keys[0]].shape[0]\n","\n","    fig_width = 4.5 * time_len\n","    fig_height = 9\n","    fig, axes = plt.subplots(len(subband_keys), time_len, figsize=(fig_width, fig_height))\n","\n","    if time_len == 1:\n","        axes = np.expand_dims(axes, axis=1)\n","\n","    for row_idx, sb_key in enumerate(subband_keys):\n","        arr1 = coeffs_dict_1[sb_key]\n","        arr2 = coeffs_dict_2[sb_key]\n","        diff_arr = arr1 - arr2\n","        for t in range(time_len):\n","            ax = axes[row_idx, t]\n","            im = ax.imshow(diff_arr[t], cmap='seismic', origin='lower')\n","            ax.set_title(f\"{title} - {sb_key} (T{t+1})\", fontsize=8)\n","            ax.axis('off')\n","\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n","    cbar = fig.colorbar(im, cax=cbar_ax)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","\n","    plt.tight_layout(rect=[0,0,0.9,1])\n","    plt.show()\n","\n","def perturb_wavelet_coeffs_3d_with_mask(coeffs_dict, mask_3d, random_noise_magnitude=0.5):\n","    \"\"\"\n","    Perturb only detail sub-bands ('ad','da','dd') using a provided mask_3d\n","    (shape: (Time, H, W)), ignoring approximation sub-band ('aa').\n","    The mask is downsampled by factor 2 in spatial dimensions to match sub-band shapes.\n","    \"\"\"\n","    perturbed_coeffs = {}\n","    for sb_key, sb_data in coeffs_dict.items():\n","        if sb_key == 'aa':\n","            perturbed_coeffs[sb_key] = sb_data.copy()\n","            continue\n","\n","        time_len, h_sub, w_sub = sb_data.shape\n","        # Downsample the mask in spatial dimensions by factor 2\n","        mask_sub = mask_3d[:time_len, ::2, ::2]\n","\n","        noise = np.random.uniform(-random_noise_magnitude,\n","                                  random_noise_magnitude,\n","                                  size=sb_data.shape)\n","        new_data = sb_data + noise * mask_sub\n","        perturbed_coeffs[sb_key] = new_data\n","\n","    return perturbed_coeffs\n","\n","def main():\n","    df = pd.read_csv(\"results/fable_tmp2m_2.5_01000.csv\")\n","    sampleNum = 10\n","\n","    dim = 0\n","    X_original = torch.tensor(eval(df['thisX'][sampleNum]))[:, :, dim].cpu().numpy()\n","    time_steps = X_original.shape[0]\n","    vmin = X_original.min()\n","    vmax = X_original.max()\n","\n","    # Read lon/lat\n","    lonlatAddr = \"data/tmp2m/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","\n","    # Plot raw data\n","    plotForecastingMapMultipleSteps(\n","        [X_original[t] for t in range(time_steps)],\n","        vmin, vmax,\n","        title=\"Original X (Raw)\"\n","    )\n","\n","    # Create padded data and mask\n","    X_padded_list = []\n","    mask_list = []\n","    for t in range(time_steps):\n","        X_padded, mask_padded, _, _ = create_regular_padded(\n","            lons, lats, X_original[t], fill_value=0.0\n","        )\n","        X_padded_list.append(X_padded)\n","        mask_list.append(mask_padded)\n","\n","    X_original_padded = np.stack(X_padded_list)  # shape (Time, H, W)\n","    mask_3d = np.stack(mask_list)                # shape (Time, H, W)\n","\n","    plot_3d_map(X_original_padded, vmin, vmax, \"Original X (Padded)\")\n","\n","    # 2D wavelet transform over (H, W) for each time step\n","    wavelet = 'haar'\n","    coeffs_original = wavelet_transform_3d(X_original_padded, wavelet=wavelet)\n","    plot_wavelet_coeffs(coeffs_original, title=\"Original Coeffs\")\n","\n","    print(np.sum(abs(X_original_padded - pywt.idwtn(coeffs_original, wavelet = 'haar', axes = (1,2)))))\n","\n","    # Perturb only detail sub-bands using mask_3d\n","    coeffs_perturbed = perturb_wavelet_coeffs_3d_with_mask(coeffs_original, mask_3d, random_noise_magnitude=0.5)\n","    plot_wavelet_coeffs(coeffs_perturbed, title=\"Perturbed Coeffs\")\n","\n","    # Reconstruct\n","    X_perturbed_padded = wavelet_inverse_transform_3d(coeffs_perturbed, wavelet=wavelet)\n","    plot_3d_map(X_perturbed_padded, vmin, vmax, \"Perturbed X (Padded)\")\n","\n","    # Transform again\n","    coeffs_perturbed_again = wavelet_transform_3d(X_perturbed_padded, wavelet=wavelet)\n","    plot_wavelet_coeffs(coeffs_perturbed_again, title=\"Perturbed Coeffs Again\")\n","\n","    # Compare difference\n","    plot_wavelet_coeffs_difference(\n","        coeffs_perturbed, coeffs_perturbed_again,\n","        title=\"Difference\"\n","    )\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XbVWNnrOv1Co"},"outputs":[],"source":["# @title Visualize 3D Decomposition based on 3D Wavelet Transform on NLDAS-apcpsfc + Padding\n","# for NLDAS apcpsfc\n","# lon: 58; lat: 28\n","\n","# MARK (yue, 1/11/2025): Since the Haar Wavelet Transform requires the dimension of original data is 2*, padding will be automatically implemented by 'pywt.dwtn()' if any dimension if odd.\n","#                        For example, if the temporal dimension is 7, the original data will be automatically padded as 8 dimension in temporal by using the last time step's data\n","#                        As I have tested, the original data will be perfectly reconstructed using 'pywt.idwtn()' even with the decomposed coefficients based on the padded original data\n","#                        NOTE: DO NOT INPUT THE ODD DIMENSIONAL DATA INTO THE FUNCTION SINCE THE AUTOMATICAL PADDING WILL MAKE THE WAVELET TRANSFORM NOT UNIQUE!!!\n","#                        SOLUTION: pywt.dwtn(data_3d, wavelet=wavelet, mode='zero'), where the parameter 'mode' can be specified\n","\n","!pip install cartopy\n","!pip install torch_geometric\n","!pip install PyWavelets\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pywt\n","import pandas as pd\n","import pickle\n","import torch\n","from matplotlib.colors import LinearSegmentedColormap\n","from matplotlib.ticker import MaxNLocator\n","import cartopy.crs as ccrs\n","import cartopy.feature as cfeature\n","\n","wavelet_mode = 'smooth' # ['smooth', 'zero', 'constant', 'symmetric', 'reflect', 'periodic', 'antisymmetric', 'antireflect']\n","\n","\n","colors = [(0, \"white\"), (0.5, \"blue\"), (1, \"red\")]\n","custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n","\n","def create_regular_padded(lons, lats, values, fill_value=0.0):\n","    \"\"\"\n","    Create padded data and a mask of the same size.\n","    The mask indicates valid data (mask=1) vs. padded data (mask=0).\n","    \"\"\"\n","    unique_lons = sorted(list(set(lons)))\n","    unique_lats = sorted(list(set(lats)))\n","    W = len(unique_lons)\n","    H = len(unique_lats)\n","\n","    padded_data = np.full((H, W), fill_value, dtype=np.float32)\n","    mask_data = np.zeros((H, W), dtype=np.float32)\n","\n","    lon_to_x = {val: idx for idx, val in enumerate(unique_lons)}\n","    lat_to_y = {val: idx for idx, val in enumerate(unique_lats)}\n","\n","    for lon, lat, val in zip(lons, lats, values):\n","        ix = lon_to_x[lon]\n","        iy = lat_to_y[lat]\n","        padded_data[iy, ix] = val\n","        mask_data[iy, ix] = 1.0\n","\n","    return padded_data, mask_data, unique_lons, unique_lats\n","\n","def plotForecastingMapMultipleSteps(data_list, vmin, vmax, title):\n","    lonlatAddr = \"data/tmp2m/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","\n","    fig_width = 6 * len(data_list)\n","    fig_height = 3\n","    fig, axes = plt.subplots(\n","        1, len(data_list),\n","        figsize=(fig_width, fig_height),\n","        subplot_kw={'projection': ccrs.PlateCarree()}\n","    )\n","\n","    if len(data_list) == 1:\n","        axes = [axes]\n","\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","\n","    for idx, (thisData, ax) in enumerate(zip(data_list, axes)):\n","        ax.add_feature(cfeature.STATES, edgecolor='black')\n","        ax.coastlines()\n","        ax.set_extent([-125, -66, 24, 50], crs=ccrs.PlateCarree())\n","\n","        sc = ax.scatter(\n","            lons,\n","            lats,\n","            c=thisData,\n","            cmap=custom_cmap,\n","            marker=',',\n","            edgecolor='none',\n","            linewidth=0,\n","            vmin=vmin,\n","            vmax=vmax\n","        )\n","        ax.set_title(f\"{title} T{idx+1}\", fontsize=10)\n","\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n","    cbar = fig.colorbar(sc, cax=cbar_ax)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","\n","    plt.tight_layout(rect=[0, 0, 0.9, 1])\n","    plt.show()\n","\n","def plot_3d_map(matrix_3d, vmin, vmax, title):\n","    if isinstance(matrix_3d, list):\n","        matrix_3d = np.array(matrix_3d)\n","\n","    fig_width = 6 * matrix_3d.shape[0]\n","    fig_height = 3\n","    fig, axes = plt.subplots(\n","        1, matrix_3d.shape[0],\n","        figsize=(fig_width, fig_height)\n","    )\n","\n","    if matrix_3d.shape[0] == 1:\n","        axes = [axes]\n","\n","    for idx, (thisData, ax) in enumerate(zip(matrix_3d, axes)):\n","        im = ax.imshow(thisData, cmap=custom_cmap, origin='lower',\n","                       vmin=vmin, vmax=vmax)\n","        ax.set_title(f\"{title} T{idx+1}\", fontsize=10)\n","        ax.axis('off')\n","\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n","    cbar = fig.colorbar(im, cax=cbar_ax)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","\n","    plt.tight_layout(rect=[0, 0, 0.9, 1])\n","    plt.show()\n","\n","def wavelet_transform_3d(data_3d, wavelet='haar'):\n","    coeffs = pywt.dwtn(data_3d, wavelet=wavelet, mode=wavelet_mode)\n","    return coeffs\n","\n","def wavelet_inverse_transform_3d(coeffs, wavelet='haar'):\n","    reconstructed_data = pywt.idwtn(coeffs, wavelet=wavelet, mode=wavelet_mode)\n","    return reconstructed_data\n","\n","def plot_wavelet_coeffs(coeffs_dict, title):\n","    \"\"\"\n","    Plots a dictionary of wavelet coefficients in separate figures.\n","    Each subband is placed in its own figure, with time slices shown as subplots.\n","    \"\"\"\n","    subband_keys = list(coeffs_dict.keys())\n","    subband_keys.sort()\n","\n","    for sb_key in subband_keys:\n","        subband_data = coeffs_dict[sb_key]\n","        vmin_value = subband_data.min()\n","        vmax_value = subband_data.max()\n","\n","        time_len = subband_data.shape[0]\n","\n","        fig_width = 2.0 * time_len\n","        fig_height = 1.5\n","        fig, axes = plt.subplots(1, time_len, figsize=(fig_width, fig_height))\n","\n","        if time_len == 1:\n","            axes = np.array([axes])\n","\n","        for t in range(time_len):\n","            ax = axes[t]\n","            im = ax.imshow(\n","                subband_data[t],\n","                cmap=custom_cmap,\n","                origin='lower',\n","                vmin=vmin_value,\n","                vmax=vmax_value\n","            )\n","            ax.set_title(f\"{title} - {sb_key} (T{t+1})\", fontsize=8)\n","            ax.axis('off')\n","\n","        cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.5])\n","        cbar = fig.colorbar(im, cax=cbar_ax)\n","        cbar.locator = MaxNLocator(nbins=5)\n","        cbar.update_ticks()\n","\n","        plt.tight_layout(rect=[0,0,0.9,1])\n","        plt.show()\n","\n","def plot_wavelet_coeffs_difference(coeffs_dict_1, coeffs_dict_2, title):\n","    subband_keys = sorted(coeffs_dict_1.keys())\n","    time_len = coeffs_dict_1[subband_keys[0]].shape[0]\n","\n","    fig_width = 4.5 * time_len\n","    fig_height = 9\n","    fig, axes = plt.subplots(len(subband_keys), time_len, figsize=(fig_width, fig_height))\n","\n","    if time_len == 1:\n","        axes = np.expand_dims(axes, axis=1)\n","\n","    for row_idx, sb_key in enumerate(subband_keys):\n","        arr1 = coeffs_dict_1[sb_key]\n","        arr2 = coeffs_dict_2[sb_key]\n","        diff_arr = abs(arr1 - arr2)\n","        for t in range(time_len):\n","            ax = axes[row_idx, t]\n","            im = ax.imshow(diff_arr[t], cmap='seismic', origin='lower')\n","            ax.set_title(f\"{title} - {sb_key} (T{t+1})\", fontsize=8)\n","            ax.axis('off')\n","\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n","    cbar = fig.colorbar(im, cax=cbar_ax)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","\n","    plt.tight_layout(rect=[0, 0, 0.9, 1])\n","    plt.show()\n","\n","def perturb_wavelet_coeffs_3d_with_mask(coeffs_dict, mask_3d, random_noise_magnitude=0.5):\n","    \"\"\"\n","    Perturb wavelet coefficients in detail sub-bands using a mask.\n","    Approximation coefficients (key='aaa') are skipped.\n","    Only masked (valid) locations receive noise.\n","    \"\"\"\n","    perturbed_coeffs = {}\n","\n","    for sb_key, sb_data in coeffs_dict.items():\n","        if sb_key == 'aaa':\n","            perturbed_coeffs[sb_key] = sb_data.copy()\n","            continue\n","\n","        time_len, h_sub, w_sub = sb_data.shape\n","\n","        # Example: downsample the mask in spatial dimensions by factor of 2\n","        # You may adjust this based on actual wavelet transform axes\n","        mask_sub = mask_3d[:time_len, ::2, ::2]\n","\n","        noise = np.random.uniform(-random_noise_magnitude,\n","                                  random_noise_magnitude,\n","                                  size=sb_data.shape)\n","        new_data = sb_data + noise * mask_sub\n","        perturbed_coeffs[sb_key] = new_data\n","\n","    return perturbed_coeffs\n","\n","def main():\n","\n","    # read original X\n","    df = pd.read_csv(\"results/fable_tmp2m_2.5_01000.csv\")\n","    sampleNum = 10\n","    dim = 0\n","    X_original = torch.tensor(eval(df['thisX'][sampleNum]))[:, :, dim].cpu().numpy()\n","\n","    # print(X_original.shape)\n","    # X_original = np.concatenate([X_original, X_original[-1:]], axis=0)          # Haar Wavelet must take data with even dimensions as the input\n","    # print(X_original.shape)\n","\n","    lonlatAddr = \"data/tmp2m/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","\n","    # plot original X\n","    time_steps = X_original.shape[0]\n","    vmin = X_original.min()\n","    vmax = X_original.max()\n","\n","    plotForecastingMapMultipleSteps([X_original[t] for t in range(time_steps)], vmin, vmax, title=\"Original X (Raw)\")\n","\n","    # get and plot the padded X based on original X\n","    X_padded_list = []\n","    mask_list = []\n","    for t in range(time_steps):\n","        padded_data, mask_data, _, _ = create_regular_padded(lons, lats, X_original[t], fill_value=0.0)\n","        X_padded_list.append(padded_data)\n","        mask_list.append(mask_data)\n","\n","    X_original_padded = np.stack(X_padded_list)                                 # as the input of 'pywt.dwt()' is type 'ndarray'\n","    mask_3d = np.stack(mask_list)\n","\n","    plot_3d_map(X_original_padded, vmin, vmax, \"Original X (Padded)\")\n","\n","    # 3d Wavelet Transform based on padded X\n","    wavelet = 'haar'\n","    coeffs_original = wavelet_transform_3d(X_original_padded, wavelet=wavelet)\n","    plot_wavelet_coeffs(coeffs_original, title=\"Original Coeffs\")\n","\n","    # reconstruct padded X and plot the difference between padded X and reconstructed padded X\n","    X_original_padded_again = wavelet_inverse_transform_3d(coeffs_original, wavelet=wavelet)\n","    plot_3d_map(X_original_padded_again, vmin, vmax, \"Original X (Padded) Again\")\n","    plot_3d_map(abs(X_original_padded_again[:7] - X_original_padded),vmin, vmax, \"Difference on X\")\n","\n","    # perturb 3d Wavelet Transform components based on padded X\n","    coeffs_perturbed = perturb_wavelet_coeffs_3d_with_mask(coeffs_original, mask_3d, random_noise_magnitude = 0.5)\n","    plot_wavelet_coeffs(coeffs_perturbed, title=\"Perturbed Coeffs\")\n","\n","    # reconstruct perturbed padded X\n","    X_perturbed_padded = wavelet_inverse_transform_3d(coeffs_perturbed, wavelet=wavelet)\n","    plot_3d_map(X_perturbed_padded, vmin, vmax, \"Perturbed X (Padded)\")\n","\n","    # 3d Wavelet Transform based on reconstructed perturbed padded X\n","    coeffs_perturbed_again = wavelet_transform_3d(X_perturbed_padded, wavelet=wavelet)\n","    plot_wavelet_coeffs(coeffs_perturbed_again, title=\"Perturbed Coeffs Again\")\n","\n","    # Compare the difference\n","    plot_wavelet_coeffs_difference(coeffs_perturbed, coeffs_perturbed_again, title=\"Difference\")\n","\n","if __name__ == '__main__':\n","    main()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7f_cUfbRaOuK"},"outputs":[],"source":["# @title Test My 1D Inverse Haar Wavelet Transform\n","\n","!pip install PyWavelets\n","import pywt\n","import numpy as np\n","import torch\n","import math\n","\n","\n","# generate the data\n","Original_x_np = np.random.rand(batch_size, seq_length)\n","Original_x = torch.tensor(Original_x_np)\n","\n","# just test one sample from the batch\n","sample_num = 0\n","sample_original_x_np = Original_x_np[sample_num, :]\n","\n","### Haar wavelet Transform\n","# coefficients by 'pywt'\n","coeffs_list = pywt.dwtn(Original_x, wavelet='haar', axes=(1,))\n","print('a by pywt: ', coeffs_list['a'][sample_num,:])\n","print('d by pywt: ', coeffs_list['d'][sample_num,:])\n","# coefficients by me\n","a_by_me = []\n","d_by_me = []\n","for k in range(0, math.floor(len(sample_original_x_np)/2)):\n","    a_by_me.append((sample_original_x_np[2*k] + sample_original_x_np[2*k+1])/math.sqrt(2))\n","    d_by_me.append((sample_original_x_np[2*k] - sample_original_x_np[2*k+1])/math.sqrt(2))\n","print('a by me: ', a_by_me)\n","print('d by me: ', d_by_me)\n","\n","### inverse Haar wavelet transform\n","# reconstruct by 'pywt'\n","reconstruct_x_np = pywt.idwtn(coeffs_list, wavelet='haar', axes = (1,))\n","print('original X: ', Original_x[sample_num, :])\n","print('reconstructed X by pywt: ', reconstruct_x_np[sample_num, :])\n","# reconstruct by me\n","reconstructed_x_by_me = torch.zeros(Original_x.shape)\n","reconstructed_x_by_me[:, ::2] = (torch.tensor(coeffs_list['a']) + torch.tensor(coeffs_list['d']))/math.sqrt(2) # g(2k)\n","reconstructed_x_by_me[:, 1::2] = (torch.tensor(coeffs_list['a']) - torch.tensor(coeffs_list['d']))/math.sqrt(2) # g(2k+1)\n","print('reconstructed X by me: ', reconstructed_x_by_me[sample_num, :])"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"OD7cJcAI7kbJ"},"outputs":[],"source":["# @title Test My 2D Inverse Haar Wavelet Transform\n","\n","!pip install PyWavelets\n","import pywt\n","import numpy as np\n","import torch\n","import math\n","\n","# Generate random data\n","Original_x_np = np.random.rand(batch_size, seq_length, H, W)\n","Original_x = torch.tensor(Original_x_np)\n","\n","# Forward Haar wavelet transform\n","def haar_2d_forward(x):\n","    B, S, H, W = x.shape\n","    LL = torch.zeros((B, S, H // 2, W // 2), device=x.device, dtype=x.dtype)\n","    LH = torch.zeros((B, S, H // 2, W // 2), device=x.device, dtype=x.dtype)\n","    HL = torch.zeros((B, S, H // 2, W // 2), device=x.device, dtype=x.dtype)\n","    HH = torch.zeros((B, S, H // 2, W // 2), device=x.device, dtype=x.dtype)\n","\n","    for k_i in range(H // 2):\n","        for k_j in range(W // 2):\n","            LL[:, :, k_i, k_j] = (\n","                x[:, :, 2 * k_i, 2 * k_j]\n","                + x[:, :, 2 * k_i + 1, 2 * k_j]\n","                + x[:, :, 2 * k_i, 2 * k_j + 1]\n","                + x[:, :, 2 * k_i + 1, 2 * k_j + 1]\n","            ) / 2.0\n","            LH[:, :, k_i, k_j] = (\n","                x[:, :, 2 * k_i, 2 * k_j]\n","                + x[:, :, 2 * k_i + 1, 2 * k_j]\n","                - x[:, :, 2 * k_i, 2 * k_j + 1]\n","                - x[:, :, 2 * k_i + 1, 2 * k_j + 1]\n","            ) / 2.0\n","            HL[:, :, k_i, k_j] = (\n","                x[:, :, 2 * k_i, 2 * k_j]\n","                - x[:, :, 2 * k_i + 1, 2 * k_j]\n","                + x[:, :, 2 * k_i, 2 * k_j + 1]\n","                - x[:, :, 2 * k_i + 1, 2 * k_j + 1]\n","            ) / 2.0\n","            HH[:, :, k_i, k_j] = (\n","                x[:, :, 2 * k_i, 2 * k_j]\n","                - x[:, :, 2 * k_i + 1, 2 * k_j]\n","                - x[:, :, 2 * k_i, 2 * k_j + 1]\n","                + x[:, :, 2 * k_i + 1, 2 * k_j + 1]\n","            ) / 2.0\n","\n","    return LL, LH, HL, HH\n","\n","# Perform forward Haar wavelet transform\n","LL_my, LH_my, HL_my, HH_my = haar_2d_forward(Original_x.to(device))\n","\n","# Perform forward Haar transform using PyWavelets\n","coeffs_list = pywt.dwtn(Original_x_np, wavelet='haar', axes=(2, 3))\n","LL_pywt = torch.tensor(coeffs_list['aa'], dtype=torch.float32).to(device)\n","LH_pywt = torch.tensor(coeffs_list['ad'], dtype=torch.float32).to(device)\n","HL_pywt = torch.tensor(coeffs_list['da'], dtype=torch.float32).to(device)\n","HH_pywt = torch.tensor(coeffs_list['dd'], dtype=torch.float32).to(device)\n","\n","# Compare coefficients\n","print(\"Comparing coefficients:\")\n","print(\"LL difference mean:\", torch.abs(LL_my - LL_pywt).mean().item())\n","print(\"LH difference mean:\", torch.abs(LH_my - LH_pywt).mean().item())\n","print(\"HL difference mean:\", torch.abs(HL_my - HL_pywt).mean().item())\n","print(\"HH difference mean:\", torch.abs(HH_my - HH_pywt).mean().item())\n","\n","# Inverse Haar wavelet transform\n","Reconstructed_x_by_me = torch.zeros(\n","    (batch_size, seq_length, H, W),\n","    device=Original_x.device,\n","    dtype=Original_x.dtype\n",")\n","\n","Reconstructed_x_by_me[:, :, ::2, ::2] = (LL_my + LH_my + HL_my + HH_my) / 2.0\n","Reconstructed_x_by_me[:, :, ::2, 1::2] = (LL_my - LH_my + HL_my - HH_my) / 2.0\n","Reconstructed_x_by_me[:, :, 1::2, ::2] = (LL_my + LH_my - HL_my - HH_my) / 2.0\n","Reconstructed_x_by_me[:, :, 1::2, 1::2] = (LL_my - LH_my - HL_my + HH_my) / 2.0\n","\n","# Inverse using PyWavelets\n","Reconstructed_x_by_pywt = pywt.idwtn(coeffs_list, wavelet='haar', axes=(2, 3))\n","\n","# Calculate differences\n","print(\"mean error by pywt:\", (torch.abs(Original_x - torch.tensor(Reconstructed_x_by_pywt))).mean().item())\n","abs_diff_tensor = torch.abs(Original_x - Reconstructed_x_by_me)\n","print(\"max error by me:\", abs_diff_tensor.max().item())\n","print(\"min error by me)::\", abs_diff_tensor.min().item())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IwOORSUYYhcq"},"outputs":[],"source":["# @title Test My 3D Inverse Haar Wavelet Transform\n","\n","# NLDAS: temperature (this is for sure having it), sea level pressure (?, to check with Dr. Luo or Shane, if it has daily autocorrelation)\n","# do not use daily 'precipitation'(not strong enough, we used to do it by month) and 'wind speed'.\n","# take a time series and do the autocorrelation to check it\n","# six-hourly better autocorrelation?\n","\n","B = batch_size\n","T = seq_length\n","\n","def haar_3d_forward(x):\n","    B, T, H, W = x.shape\n","    inv_sqrt8 = 1.0 / math.sqrt(8.0)\n","    out_shape = (B, T // 2, H // 2, W // 2)\n","\n","    LLL = torch.zeros(out_shape, device=x.device, dtype=x.dtype)\n","    LLH = torch.zeros(out_shape, device=x.device, dtype=x.dtype)\n","    LHL = torch.zeros(out_shape, device=x.device, dtype=x.dtype)\n","    LHH = torch.zeros(out_shape, device=x.device, dtype=x.dtype)\n","    HLL = torch.zeros(out_shape, device=x.device, dtype=x.dtype)\n","    HLH = torch.zeros(out_shape, device=x.device, dtype=x.dtype)\n","    HHL = torch.zeros(out_shape, device=x.device, dtype=x.dtype)\n","    HHH = torch.zeros(out_shape, device=x.device, dtype=x.dtype)\n","\n","    for k_t in range(T // 2):\n","        for k_i in range(H // 2):\n","            for k_j in range(W // 2):\n","                X000 = x[:, 2*k_t,   2*k_i,   2*k_j   ]  # t=0,i=0,j=0\n","                X001 = x[:, 2*k_t,   2*k_i,   2*k_j+1 ]  # t=0,i=0,j=1\n","                X010 = x[:, 2*k_t,   2*k_i+1, 2*k_j   ]  # t=0,i=1,j=0\n","                X011 = x[:, 2*k_t,   2*k_i+1, 2*k_j+1 ]  # t=0,i=1,j=1\n","                X100 = x[:, 2*k_t+1, 2*k_i,   2*k_j   ]  # t=1,i=0,j=0\n","                X101 = x[:, 2*k_t+1, 2*k_i,   2*k_j+1 ]  # t=1,i=0,j=1\n","                X110 = x[:, 2*k_t+1, 2*k_i+1, 2*k_j   ]  # t=1,i=1,j=0\n","                X111 = x[:, 2*k_t+1, 2*k_i+1, 2*k_j+1 ]  # t=1,i=1,j=1\n","\n","                LLL[:, k_t, k_i, k_j] = (X000 + X001 + X010 + X011 + X100 + X101 + X110 + X111) * inv_sqrt8\n","                LLH[:, k_t, k_i, k_j] = (X000 - X001 + X010 - X011 + X100 - X101 + X110 - X111) * inv_sqrt8\n","                LHL[:, k_t, k_i, k_j] = (X000 + X001 - X010 - X011 + X100 + X101 - X110 - X111) * inv_sqrt8\n","                LHH[:, k_t, k_i, k_j] = (X000 - X001 - X010 + X011 + X100 - X101 - X110 + X111) * inv_sqrt8\n","                HLL[:, k_t, k_i, k_j] = (X000 + X001 + X010 + X011 - X100 - X101 - X110 - X111) * inv_sqrt8\n","                HLH[:, k_t, k_i, k_j] = (X000 - X001 + X010 - X011 - X100 + X101 - X110 + X111) * inv_sqrt8\n","                HHL[:, k_t, k_i, k_j] = (X000 + X001 - X010 - X011 - X100 - X101 + X110 + X111) * inv_sqrt8\n","                HHH[:, k_t, k_i, k_j] = (X000 - X001 - X010 + X011 - X100 + X101 + X110 - X111) * inv_sqrt8\n","\n","    return LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH\n","\n","def haar_3d_inverse(LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH):\n","    B, T2, H2, W2 = LLL.shape\n","    T = T2 * 2\n","    H = H2 * 2\n","    W = W2 * 2\n","    x_recon = torch.zeros((B, T, H, W), device=LLL.device, dtype=LLL.dtype)\n","    inv_sqrt8 = 1.0 / math.sqrt(8.0)\n","\n","    x_recon[:, ::2, ::2, ::2] = (LLL + LLH + LHL + LHH + HLL + HLH + HHL + HHH) * inv_sqrt8\n","    x_recon[:, ::2, ::2, 1::2] = (LLL - LLH + LHL - LHH + HLL - HLH + HHL - HHH) * inv_sqrt8\n","    x_recon[:, ::2, 1::2, ::2] = (LLL + LLH - LHL - LHH + HLL + HLH - HHL - HHH) * inv_sqrt8\n","    x_recon[:, ::2, 1::2, 1::2] = (LLL - LLH - LHL + LHH + HLL - HLH - HHL + HHH) * inv_sqrt8\n","    x_recon[:, 1::2, ::2, ::2] = (LLL + LLH + LHL + LHH - HLL - HLH - HHL - HHH) * inv_sqrt8\n","    x_recon[:, 1::2, ::2, 1::2] = (LLL - LLH + LHL - LHH - HLL + HLH - HHL + HHH) * inv_sqrt8\n","    x_recon[:, 1::2, 1::2, ::2] = (LLL + LLH - LHL - LHH - HLL - HLH + HHL + HHH) * inv_sqrt8\n","    x_recon[:, 1::2, 1::2, 1::2] = (LLL - LLH - LHL + LHH - HLL + HLH + HHL - HHH) * inv_sqrt8\n","\n","    return x_recon\n","\n","### forward\n","x_np = np.random.rand(B, T, H, W)\n","\n","print(x_np.shape)\n","\n","x = torch.tensor(x_np, dtype=torch.float32)\n","pywt_coeffs = pywt.dwtn(x_np, wavelet='haar', axes=(1, 2, 3))\n","LLL_my, LLH_my, LHL_my, LHH_my, HLL_my, HLH_my, HHL_my, HHH_my = haar_3d_forward(x)\n","\n","LLL_diff = torch.abs(LLL_my - torch.tensor(pywt_coeffs['aaa'], dtype=torch.float32)).mean().item()\n","LLH_diff = torch.abs(LLH_my - torch.tensor(pywt_coeffs['aad'], dtype=torch.float32)).mean().item()\n","LHL_diff = torch.abs(LHL_my - torch.tensor(pywt_coeffs['ada'], dtype=torch.float32)).mean().item()\n","LHH_diff = torch.abs(LHH_my - torch.tensor(pywt_coeffs['add'], dtype=torch.float32)).mean().item()\n","HLL_diff = torch.abs(HLL_my - torch.tensor(pywt_coeffs['daa'], dtype=torch.float32)).mean().item()\n","HLH_diff = torch.abs(HLH_my - torch.tensor(pywt_coeffs['dad'], dtype=torch.float32)).mean().item()\n","HHL_diff = torch.abs(HHL_my - torch.tensor(pywt_coeffs['dda'], dtype=torch.float32)).mean().item()\n","HHH_diff = torch.abs(HHH_my - torch.tensor(pywt_coeffs['ddd'], dtype=torch.float32)).mean().item()\n","\n","print(\"LLL difference:\", LLL_diff)\n","print(\"LLH difference:\", LLH_diff)\n","print(\"LHL difference:\", LHL_diff)\n","print(\"LHH difference:\", LHH_diff)\n","print(\"HLL difference:\", HLL_diff)\n","print(\"HLH difference:\", HLH_diff)\n","print(\"HHL difference:\", HHL_diff)\n","print(\"HHH difference:\", HHH_diff)\n","\n","### inverse\n","coeffs = pywt.dwtn(x_np, wavelet='haar', axes=(1,2,3))\n","LLL = torch.tensor(coeffs['aaa'], dtype=torch.float32)\n","LLH = torch.tensor(coeffs['aad'], dtype=torch.float32)\n","LHL = torch.tensor(coeffs['ada'], dtype=torch.float32)\n","LHH = torch.tensor(coeffs['add'], dtype=torch.float32)\n","HLL = torch.tensor(coeffs['daa'], dtype=torch.float32)\n","HLH = torch.tensor(coeffs['dad'], dtype=torch.float32)\n","HHL = torch.tensor(coeffs['dda'], dtype=torch.float32)\n","HHH = torch.tensor(coeffs['ddd'], dtype=torch.float32)\n","\n","Reconstructed_x_by_me = haar_3d_inverse(LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH)\n","Reconstructed_x_by_pywt = pywt.idwtn(coeffs, wavelet='haar', axes=(1,2,3))\n","err_by_me = (x - Reconstructed_x_by_me).abs().mean().item()\n","err_by_pywt = (x - Reconstructed_x_by_pywt).abs().mean().item()\n","diff = (Reconstructed_x_by_me - Reconstructed_x_by_pywt).abs().mean().item()\n","\n","print(\"mean error by pywt:\", err_by_pywt)\n","print(\"max error by me:\", (x - Reconstructed_x_by_me).abs().max().item())\n","print(\"min error by me:\", (x - Reconstructed_x_by_me).abs().min().item())"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Lu_7Fwr8Dikj"},"outputs":[],"source":["# @title Analysize the perturbation magnitute added on different coefficients for 2d Haar wavelet\n","time_steps_shown_in_plot = 6                                                    # they are the first X time steps in X\n","\n","!pip install cartopy\n","!pip install torch_geometric\n","!pip install PyWavelets\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pywt\n","import pandas as pd\n","import pickle\n","import torch\n","from matplotlib.colors import LinearSegmentedColormap\n","from matplotlib.ticker import MaxNLocator\n","import cartopy.crs as ccrs\n","import cartopy.feature as cfeature\n","\n","colors = [(0, \"white\"), (0.5, \"blue\"), (1, \"red\")]\n","custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n","\n","def create_regular_padded(lons, lats, values, fill_value=0.0):\n","    \"\"\"\n","    Returns (padded_data, mask_data, unique_lons, unique_lats).\n","    mask_data has the same shape as padded_data, with 1 where actual data is placed,\n","    and 0 where fill_value is placed.\n","    \"\"\"\n","    unique_lons = sorted(list(set(lons)))\n","    unique_lats = sorted(list(set(lats)))\n","    W = len(unique_lons)\n","    H = len(unique_lats)\n","\n","    padded_data = np.full((H, W), fill_value, dtype=np.float32)\n","    mask_data   = np.zeros((H, W), dtype=np.float32)\n","\n","    lon_to_x = {val: idx for idx, val in enumerate(unique_lons)}                # {-124.5: 0, -123.5: 1, -122.5: 2, -121.5: 3, -120.5: 4, -119.5: 5,  ...}\n","    lat_to_y = {val: idx for idx, val in enumerate(unique_lats)}                # {25.5: 0, 26.5: 1, 27.5: 2, 28.5: 3, 29.5: 4, 30.5: 5, ...}\n","\n","    for lon, lat, val in zip(lons, lats, values):\n","        ix = lon_to_x[lon]\n","        iy = lat_to_y[lat]\n","        padded_data[iy, ix] = val\n","        mask_data[iy, ix]   = 1.0\n","\n","    return padded_data, mask_data, unique_lons, unique_lats\n","\n","def plotForecastingMapMultipleSteps(data_list, vmin, vmax, title):\n","    lonlatAddr = \"data/apcpsfc/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","\n","    fig_width = 6 * time_steps_shown_in_plot\n","    fig_height = 3\n","    fig, axes = plt.subplots(\n","        1, len(data_list),\n","        figsize=(fig_width, fig_height),\n","        subplot_kw={'projection': ccrs.PlateCarree()}\n","    )\n","\n","    if len(data_list) == 1:\n","        axes = [axes]\n","\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","\n","    for idx, (thisData, ax) in enumerate(zip(data_list, axes)):\n","        ax.add_feature(cfeature.STATES, edgecolor='black')\n","        ax.coastlines()\n","        ax.set_extent([-125, -66, 24, 50], crs=ccrs.PlateCarree())\n","\n","        sc = ax.scatter(\n","            lons,\n","            lats,\n","            c=thisData,\n","            cmap=custom_cmap,\n","            marker=',',\n","            edgecolor='none',\n","            linewidth=0,\n","            vmin=vmin,\n","            vmax=vmax\n","        )\n","        ax.set_title(f\"{title} T{idx+1}\", fontsize=10)\n","\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n","    cbar = fig.colorbar(sc, cax=cbar_ax)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","\n","    plt.tight_layout(rect=[0,0,0.9,1])\n","    plt.show()\n","\n","def plot_3d_map(matrix_3d, vmin, vmax, title):\n","    if isinstance(matrix_3d, list):\n","        matrix_3d = np.array(matrix_3d)\n","\n","    # fig_width = 6 * matrix_3d.shape[0]\n","    fig_width = 6 * time_steps_shown_in_plot\n","    fig_height = 3\n","    fig, axes = plt.subplots(\n","        1, time_steps_shown_in_plot,\n","        figsize=(fig_width, fig_height)\n","    )\n","\n","    if matrix_3d.shape[0] == 1:\n","        axes = [axes]\n","\n","    for idx, (thisData, ax) in enumerate(zip(matrix_3d, axes)):\n","        if idx < time_steps_shown_in_plot:\n","\n","            im = ax.imshow(thisData, cmap=custom_cmap, origin='lower',\n","                          vmin=vmin, vmax=vmax)\n","            ax.set_title(f\"{title} T{idx+1}\", fontsize=10)\n","            ax.axis('off')\n","\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n","    cbar = fig.colorbar(im, cax=cbar_ax)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","\n","    plt.tight_layout(rect=[0,0,0.9,1])\n","    plt.show()\n","\n","def wavelet_transform_3d(data_3d, wavelet='haar'):\n","    \"\"\"\n","    Perform a 2D wavelet transform over axes=(1, 2) of data shaped (Time, H, W).\n","    Returns a dict of sub-band coefficients.\n","    Example of returned keys: 'aa','ad','da','dd'\n","    \"\"\"\n","    coeffs = pywt.dwtn(data_3d, wavelet=wavelet, axes=(0, 1, 2))\n","    return coeffs\n","\n","def wavelet_inverse_transform_3d(coeffs, wavelet='haar'):\n","    \"\"\"\n","    Inverse of wavelet_transform_3d.\n","    Reconstructs data of shape (Time, H, W).\n","    \"\"\"\n","    reconstructed_data = pywt.idwtn(coeffs, wavelet=wavelet, axes=(0, 1, 2))\n","    return reconstructed_data\n","\n","def plot_wavelet_coeffs(coeffs_dict, title):\n","    \"\"\"\n","    Plots a dictionary of wavelet coefficients in separate figures.\n","    Each subband is placed in its own figure, with time slices shown as subplots.\n","    \"\"\"\n","    subband_keys = list(coeffs_dict.keys())\n","    subband_keys.sort()\n","\n","    for sb_key in subband_keys:\n","        subband_data = coeffs_dict[sb_key]\n","        vmin_value = subband_data.min()\n","        vmax_value = subband_data.max()\n","\n","        # time_len = subband_data.shape[0]\n","\n","\n","        fig_width = 4.5 * time_steps_shown_in_plot\n","        fig_height = 4.0\n","        fig, axes = plt.subplots(1, time_steps_shown_in_plot, figsize=(fig_width, fig_height))\n","\n","        if time_steps_shown_in_plot == 1:\n","            axes = np.array([axes])\n","\n","        for t in range(time_steps_shown_in_plot):\n","            ax = axes[t]\n","            im = ax.imshow(\n","                subband_data[t],\n","                cmap=custom_cmap,\n","                origin='lower',\n","                vmin=vmin_value,\n","                vmax=vmax_value\n","            )\n","            ax.set_title(f\"{title} - {sb_key} (T{t+1})\", fontsize=8)\n","            ax.axis('off')\n","\n","        cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.5])\n","        cbar = fig.colorbar(im, cax=cbar_ax)\n","        cbar.locator = MaxNLocator(nbins=5)\n","        cbar.update_ticks()\n","\n","        plt.tight_layout(rect=[0,0,0.9,1])\n","        plt.show()\n","\n","def main():\n","    df = pd.read_csv(\"results/FABLE_tmp2m_2.5_01000_2d_LL+HL+HH.csv\")\n","    lonlatAddr = \"data/tmp2m/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","    dim = 0\n","\n","    LL_diff = []\n","    LH_diff = []\n","    HL_diff = []\n","    HH_diff = []\n","    LL = []\n","    LH = []\n","    HL = []\n","    HH = []\n","\n","    for sampleNum in range(len(df['thisX'])):\n","        # print(sampleNum)\n","\n","        ## original\n","        # print('original')\n","        X_original = torch.tensor(eval(df['thisX'][sampleNum]))[:, :, dim].cpu().numpy()\n","        time_steps = X_original.shape[0]\n","        vmin = X_original.min()\n","        vmax = X_original.max()\n","        X_padded_list = []\n","        mask_list = []\n","        for t in range(time_steps):\n","            X_padded, mask_padded, _, _ = create_regular_padded(lons, lats, X_original[t], fill_value=0.0)\n","            X_padded_list.append(X_padded)\n","            mask_list.append(mask_padded)\n","        X_original_padded = np.stack(X_padded_list)\n","        mask_3d = np.stack(mask_list)\n","        # plot_3d_map(X_original_padded, vmin, vmax, \"Original X (Padded)\")\n","        wavelet = 'haar'\n","        coeffs_original = wavelet_transform_3d(X_original_padded, wavelet=wavelet)\n","        # plot_wavelet_coeffs(coeffs_original, title=\"Original Coeffs\")\n","\n","        # ## perturbed\n","        # print('perturbed')\n","        X_perturbed = torch.tensor(eval(df['xPerturbed'][sampleNum]))[:, :, dim].cpu().numpy()\n","        time_steps = X_perturbed.shape[0]\n","        vmin = X_perturbed.min()\n","        vmax = X_perturbed.max()\n","        X_perturbed_padded_list = []\n","        mask_list = []\n","        for t in range(time_steps):\n","            X_perturbed_padded, mask_padded, _, _ = create_regular_padded(\n","                lons, lats, X_perturbed[t], fill_value=0.0\n","            )\n","            X_perturbed_padded_list.append(X_perturbed_padded)\n","            mask_list.append(mask_padded)\n","        X_perturbed_padded = np.stack(X_perturbed_padded_list)\n","        mask_3d = np.stack(mask_list)\n","        # plot_3d_map(X_perturbed_padded, vmin, vmax, \"Perturbed X (Padded)\")\n","        wavelet = 'haar'\n","        coeffs_perturbed = wavelet_transform_3d(X_perturbed_padded, wavelet=wavelet)\n","        # plot_wavelet_coeffs(coeffs_perturbed, title=\"Perturbed Coeffs\")\n","\n","        ## diff\n","        # print('diff')\n","        X_diff = X_original - X_perturbed\n","        time_steps = X_diff.shape[0]\n","        vmin = X_diff.min()\n","        vmax = X_diff.max()\n","        X_diff_padded_list = []\n","        mask_list = []\n","        for t in range(time_steps):\n","            X_diff_padded, mask_padded, _, _ = create_regular_padded(\n","                lons, lats, X_diff[t], fill_value=0.0\n","            )\n","            X_diff_padded_list.append(X_diff_padded)\n","            mask_list.append(mask_padded)\n","        X_diff_padded = np.stack(X_diff_padded_list)\n","        mask_3d = np.stack(mask_list)\n","        # plot_3d_map(X_diff_padded, vmin, vmax, \"Diff X (Padded)\")\n","        coeffs_diff = {}\n","        for key in coeffs_original.keys():\n","            coeffs_diff[key] = coeffs_original[key] - coeffs_perturbed[key]\n","        # plot_wavelet_coeffs(coeffs_diff, title=\"Coeffs diff\")\n","\n","\n","        ###\n","        LL.append(np.mean(abs(coeffs_original['aa'])))\n","        LH.append(np.mean(abs(coeffs_original['ad'])))\n","        HL.append(np.mean(abs(coeffs_original['da'])))\n","        HH.append(np.mean(abs(coeffs_original['dd'])))\n","\n","        LL_diff.append(np.mean(abs(coeffs_diff['aa'])))\n","        LH_diff.append(np.mean(abs(coeffs_diff['ad'])))\n","        HL_diff.append(np.mean(abs(coeffs_diff['da'])))\n","        HH_diff.append(np.mean(abs(coeffs_diff['dd'])))\n","\n","    # print(np.mean(LL), np.std(LL))\n","    # print(np.mean(LH), np.std(LH))\n","    # print(np.mean(HL), np.std(HL))\n","    # print(np.mean(HH), np.std(HH))\n","\n","    print(np.mean(LL_diff), np.std(LL_diff))\n","    print(np.mean(LH_diff), np.std(LH_diff))\n","    print(np.mean(HL_diff), np.std(HL_diff))\n","    print(np.mean(HH_diff), np.std(HH_diff))\n","\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"cjnOK83774X5"},"outputs":[],"source":["# @title Analysize the perturbation magnitute added on different coefficients for 3d Haar wavelet\n","time_steps_shown_in_plot = 6                                                    # they are the first X time steps in X\n","\n","!pip install cartopy\n","!pip install torch_geometric\n","!pip install PyWavelets\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pywt\n","import pandas as pd\n","import pickle\n","import torch\n","from matplotlib.colors import LinearSegmentedColormap\n","from matplotlib.ticker import MaxNLocator\n","import cartopy.crs as ccrs\n","import cartopy.feature as cfeature\n","\n","colors = [(0, \"white\"), (0.5, \"blue\"), (1, \"red\")]\n","custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n","\n","def create_regular_padded(lons, lats, values, fill_value=0.0):\n","    \"\"\"\n","    Returns (padded_data, mask_data, unique_lons, unique_lats).\n","    mask_data has the same shape as padded_data, with 1 where actual data is placed,\n","    and 0 where fill_value is placed.\n","    \"\"\"\n","    unique_lons = sorted(list(set(lons)))\n","    unique_lats = sorted(list(set(lats)))\n","    W = len(unique_lons)\n","    H = len(unique_lats)\n","\n","    padded_data = np.full((H, W), fill_value, dtype=np.float32)\n","    mask_data   = np.zeros((H, W), dtype=np.float32)\n","\n","    lon_to_x = {val: idx for idx, val in enumerate(unique_lons)}                # {-124.5: 0, -123.5: 1, -122.5: 2, -121.5: 3, -120.5: 4, -119.5: 5,  ...}\n","    lat_to_y = {val: idx for idx, val in enumerate(unique_lats)}                # {25.5: 0, 26.5: 1, 27.5: 2, 28.5: 3, 29.5: 4, 30.5: 5, ...}\n","\n","    for lon, lat, val in zip(lons, lats, values):\n","        ix = lon_to_x[lon]\n","        iy = lat_to_y[lat]\n","        padded_data[iy, ix] = val\n","        mask_data[iy, ix]   = 1.0\n","\n","    return padded_data, mask_data, unique_lons, unique_lats\n","\n","def plotForecastingMapMultipleSteps(data_list, vmin, vmax, title):\n","    lonlatAddr = \"data/apcpsfc/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","\n","    fig_width = 6 * time_steps_shown_in_plot\n","    fig_height = 3\n","    fig, axes = plt.subplots(\n","        1, len(data_list),\n","        figsize=(fig_width, fig_height),\n","        subplot_kw={'projection': ccrs.PlateCarree()}\n","    )\n","\n","    if len(data_list) == 1:\n","        axes = [axes]\n","\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","\n","    for idx, (thisData, ax) in enumerate(zip(data_list, axes)):\n","        ax.add_feature(cfeature.STATES, edgecolor='black')\n","        ax.coastlines()\n","        ax.set_extent([-125, -66, 24, 50], crs=ccrs.PlateCarree())\n","\n","        sc = ax.scatter(\n","            lons,\n","            lats,\n","            c=thisData,\n","            cmap=custom_cmap,\n","            marker=',',\n","            edgecolor='none',\n","            linewidth=0,\n","            vmin=vmin,\n","            vmax=vmax\n","        )\n","        ax.set_title(f\"{title} T{idx+1}\", fontsize=10)\n","\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n","    cbar = fig.colorbar(sc, cax=cbar_ax)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","\n","    plt.tight_layout(rect=[0,0,0.9,1])\n","    plt.show()\n","\n","def plot_3d_map(matrix_3d, vmin, vmax, title):\n","    if isinstance(matrix_3d, list):\n","        matrix_3d = np.array(matrix_3d)\n","\n","    # fig_width = 6 * matrix_3d.shape[0]\n","    fig_width = 6 * time_steps_shown_in_plot\n","    fig_height = 3\n","    fig, axes = plt.subplots(\n","        1, time_steps_shown_in_plot,\n","        figsize=(fig_width, fig_height)\n","    )\n","\n","    if matrix_3d.shape[0] == 1:\n","        axes = [axes]\n","\n","    for idx, (thisData, ax) in enumerate(zip(matrix_3d, axes)):\n","        if idx < time_steps_shown_in_plot:\n","\n","            im = ax.imshow(thisData, cmap=custom_cmap, origin='lower',\n","                          vmin=vmin, vmax=vmax)\n","            ax.set_title(f\"{title} T{idx+1}\", fontsize=10)\n","            ax.axis('off')\n","\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n","    cbar = fig.colorbar(im, cax=cbar_ax)\n","    cbar.locator = MaxNLocator(nbins=5)\n","    cbar.update_ticks()\n","\n","    plt.tight_layout(rect=[0,0,0.9,1])\n","    plt.show()\n","\n","def wavelet_transform_3d(data_3d, wavelet='haar'):\n","    \"\"\"\n","    Perform a 2D wavelet transform over axes=(1, 2) of data shaped (Time, H, W).\n","    Returns a dict of sub-band coefficients.\n","    Example of returned keys: 'aa','ad','da','dd'\n","    \"\"\"\n","    coeffs = pywt.dwtn(data_3d, wavelet=wavelet)\n","    return coeffs\n","\n","def wavelet_inverse_transform_3d(coeffs, wavelet='haar'):\n","    \"\"\"\n","    Inverse of wavelet_transform_3d.\n","    Reconstructs data of shape (Time, H, W).\n","    \"\"\"\n","    reconstructed_data = pywt.idwtn(coeffs, wavelet=wavelet)\n","    return reconstructed_data\n","\n","def plot_wavelet_coeffs(coeffs_dict, title):\n","    \"\"\"\n","    Plots a dictionary of wavelet coefficients in separate figures.\n","    Each subband is placed in its own figure, with time slices shown as subplots.\n","    \"\"\"\n","    subband_keys = list(coeffs_dict.keys())\n","    subband_keys.sort()\n","\n","    for sb_key in subband_keys:\n","        subband_data = coeffs_dict[sb_key]\n","        vmin_value = subband_data.min()\n","        vmax_value = subband_data.max()\n","\n","        # time_len = subband_data.shape[0]\n","\n","\n","        fig_width = 4.5 * time_steps_shown_in_plot\n","        fig_height = 4.0\n","        fig, axes = plt.subplots(1, time_steps_shown_in_plot, figsize=(fig_width, fig_height))\n","\n","        if time_steps_shown_in_plot == 1:\n","            axes = np.array([axes])\n","\n","        for t in range(time_steps_shown_in_plot):\n","            ax = axes[t]\n","            im = ax.imshow(\n","                subband_data[t],\n","                cmap=custom_cmap,\n","                origin='lower',\n","                vmin=vmin_value,\n","                vmax=vmax_value\n","            )\n","            ax.set_title(f\"{title} - {sb_key} (T{t+1})\", fontsize=8)\n","            ax.axis('off')\n","\n","        cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.5])\n","        cbar = fig.colorbar(im, cax=cbar_ax)\n","        cbar.locator = MaxNLocator(nbins=5)\n","        cbar.update_ticks()\n","\n","        plt.tight_layout(rect=[0,0,0.9,1])\n","        plt.show()\n","\n","def main():\n","    df = pd.read_csv(\"results/FABLE_tmp2m_2.5_01000_3d_2L1H_HHH.csv\")\n","    lonlatAddr = \"data/tmp2m/position_info.pkl\"\n","    with open(lonlatAddr, 'rb') as lonLatPkl:\n","        lonLatDict = pickle.load(lonLatPkl)\n","    lons = lonLatDict['lonlat'][:, 0]\n","    lats = lonLatDict['lonlat'][:, 1]\n","    dim = 0\n","\n","    LLL_diff = []\n","    LLH_diff = []\n","    LHL_diff = []\n","    LHH_diff = []\n","    HLL_diff = []\n","    HLH_diff = []\n","    HHL_diff = []\n","    HHH_diff = []\n","    LLL = []\n","    LLH = []\n","    LHL = []\n","    LHH = []\n","    HLL = []\n","    HLH = []\n","    HHL = []\n","    HHH = []\n","\n","\n","    for sampleNum in range(len(df['thisX'])):\n","        # print(sampleNum)\n","\n","        ## original\n","        # print('original')\n","        X_original = torch.tensor(eval(df['thisX'][sampleNum]))[:, :, dim].cpu().numpy()\n","        time_steps = X_original.shape[0]\n","        vmin = X_original.min()\n","        vmax = X_original.max()\n","        X_padded_list = []\n","        mask_list = []\n","        for t in range(time_steps):\n","            X_padded, mask_padded, _, _ = create_regular_padded(lons, lats, X_original[t], fill_value=0.0)\n","            X_padded_list.append(X_padded)\n","            mask_list.append(mask_padded)\n","        X_original_padded = np.stack(X_padded_list)\n","        mask_3d = np.stack(mask_list)\n","        # plot_3d_map(X_original_padded, vmin, vmax, \"Original X (Padded)\")\n","        wavelet = 'haar'\n","        coeffs_original = wavelet_transform_3d(X_original_padded, wavelet=wavelet)\n","        # plot_wavelet_coeffs(coeffs_original, title=\"Original Coeffs\")\n","\n","        # ## perturbed\n","        # print('perturbed')\n","        X_perturbed = torch.tensor(eval(df['xPerturbed'][sampleNum]))[:, :, dim].cpu().numpy()\n","        time_steps = X_perturbed.shape[0]\n","        vmin = X_perturbed.min()\n","        vmax = X_perturbed.max()\n","        X_perturbed_padded_list = []\n","        mask_list = []\n","        for t in range(time_steps):\n","            X_perturbed_padded, mask_padded, _, _ = create_regular_padded(\n","                lons, lats, X_perturbed[t], fill_value=0.0\n","            )\n","            X_perturbed_padded_list.append(X_perturbed_padded)\n","            mask_list.append(mask_padded)\n","        X_perturbed_padded = np.stack(X_perturbed_padded_list)\n","        mask_3d = np.stack(mask_list)\n","        # plot_3d_map(X_perturbed_padded, vmin, vmax, \"Perturbed X (Padded)\")\n","        wavelet = 'haar'\n","        coeffs_perturbed = wavelet_transform_3d(X_perturbed_padded, wavelet=wavelet)\n","        # plot_wavelet_coeffs(coeffs_perturbed, title=\"Perturbed Coeffs\")\n","\n","        ## diff\n","        # print('diff')\n","        X_diff = X_original - X_perturbed\n","        time_steps = X_diff.shape[0]\n","        vmin = X_diff.min()\n","        vmax = X_diff.max()\n","        X_diff_padded_list = []\n","        mask_list = []\n","        for t in range(time_steps):\n","            X_diff_padded, mask_padded, _, _ = create_regular_padded(\n","                lons, lats, X_diff[t], fill_value=0.0\n","            )\n","            X_diff_padded_list.append(X_diff_padded)\n","            mask_list.append(mask_padded)\n","        X_diff_padded = np.stack(X_diff_padded_list)\n","        mask_3d = np.stack(mask_list)\n","        # plot_3d_map(X_diff_padded, vmin, vmax, \"Diff X (Padded)\")\n","        coeffs_diff = {}\n","        for key in coeffs_original.keys():\n","            coeffs_diff[key] = coeffs_original[key] - coeffs_perturbed[key]\n","        # plot_wavelet_coeffs(coeffs_diff, title=\"Coeffs diff\")\n","\n","\n","        ###\n","        LLL.append(np.mean(abs(coeffs_original['aaa'])))\n","        LLH.append(np.mean(abs(coeffs_original['aad'])))\n","        LHL.append(np.mean(abs(coeffs_original['ada'])))\n","        LHH.append(np.mean(abs(coeffs_original['add'])))\n","        HLL.append(np.mean(abs(coeffs_original['daa'])))\n","        HLH.append(np.mean(abs(coeffs_original['dad'])))\n","        HHL.append(np.mean(abs(coeffs_original['dda'])))\n","        HHH.append(np.mean(abs(coeffs_original['ddd'])))\n","\n","        LLL_diff.append(np.mean(abs(coeffs_diff['aaa'])))\n","        LLH_diff.append(np.mean(abs(coeffs_diff['aad'])))\n","        LHL_diff.append(np.mean(abs(coeffs_diff['ada'])))\n","        LHH_diff.append(np.mean(abs(coeffs_diff['add'])))\n","        HLL_diff.append(np.mean(abs(coeffs_diff['daa'])))\n","        HLH_diff.append(np.mean(abs(coeffs_diff['dad'])))\n","        HHL_diff.append(np.mean(abs(coeffs_diff['dda'])))\n","        HHH_diff.append(np.mean(abs(coeffs_diff['ddd'])))\n","\n","    # print(np.mean(LLL), np.std(LLL))\n","    # print(np.mean(LLH), np.std(LLH))\n","    # print(np.mean(LHL), np.std(LHL))\n","    # print(np.mean(LHH), np.std(LHH))\n","    # print(np.mean(HLL), np.std(HLL))\n","    # print(np.mean(HLH), np.std(HLH))\n","    # print(np.mean(HHL), np.std(HHL))\n","    # print(np.mean(HHH), np.std(HHH))\n","\n","    print(np.mean(LLL_diff), np.std(LLL_diff))\n","    print(np.mean(LLH_diff), np.std(LLH_diff))\n","    print(np.mean(LHL_diff), np.std(LHL_diff))\n","    print(np.mean(LHH_diff), np.std(LHH_diff))\n","    print(np.mean(HLL_diff), np.std(HLL_diff))\n","    print(np.mean(HLH_diff), np.std(HLH_diff))\n","    print(np.mean(HHL_diff), np.std(HHL_diff))\n","    print(np.mean(HHH_diff), np.std(HHH_diff))\n","\n","\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VgpdO4zCnNWG","outputId":"cde5c800-ff59-4de6-aca4-1f91537c8381","executionInfo":{"status":"ok","timestamp":1739233301065,"user_tz":300,"elapsed":284410,"user":{"displayName":"Yue Deng","userId":"00346012823044562765"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pysal\n","  Downloading pysal-25.1-py3-none-any.whl.metadata (15 kB)\n","Collecting esda\n","  Downloading esda-2.7.0-py3-none-any.whl.metadata (2.0 kB)\n","Collecting libpysal\n","  Downloading libpysal-4.12.1-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: beautifulsoup4>=4.10 in /usr/local/lib/python3.11/dist-packages (from pysal) (4.13.3)\n","Requirement already satisfied: geopandas>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pysal) (1.0.1)\n","Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from pysal) (1.26.4)\n","Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.11/dist-packages (from pysal) (24.2)\n","Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.11/dist-packages (from pysal) (2.2.2)\n","Requirement already satisfied: platformdirs>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pysal) (4.3.6)\n","Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.11/dist-packages (from pysal) (2.32.3)\n","Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from pysal) (1.13.1)\n","Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from pysal) (2.0.7)\n","Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.11/dist-packages (from pysal) (1.6.1)\n","Collecting access>=1.1.9 (from pysal)\n","  Downloading access-1.1.9-py3-none-any.whl.metadata (2.4 kB)\n","Collecting giddy>=2.3.6 (from pysal)\n","  Downloading giddy-2.3.6-py3-none-any.whl.metadata (6.3 kB)\n","Collecting inequality>=1.1.1 (from pysal)\n","  Downloading inequality-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n","Collecting pointpats>=2.5.1 (from pysal)\n","  Downloading pointpats-2.5.1-py3-none-any.whl.metadata (4.7 kB)\n","Collecting segregation>=2.5.1 (from pysal)\n","  Downloading segregation-2.5.2-py3-none-any.whl.metadata (2.2 kB)\n","Collecting spaghetti>=1.7.6 (from pysal)\n","  Downloading spaghetti-1.7.6-py3-none-any.whl.metadata (12 kB)\n","Collecting mgwr>=2.2.1 (from pysal)\n","  Downloading mgwr-2.2.1-py3-none-any.whl.metadata (1.5 kB)\n","Collecting momepy>=0.9.1 (from pysal)\n","  Downloading momepy-0.9.1-py3-none-any.whl.metadata (1.4 kB)\n","Collecting spglm>=1.1.0 (from pysal)\n","  Downloading spglm-1.1.0-py3-none-any.whl.metadata (3.9 kB)\n","Collecting spint>=1.0.7 (from pysal)\n","  Downloading spint-1.0.7.tar.gz (28 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting spreg>=1.8.1 (from pysal)\n","  Downloading spreg-1.8.2-py3-none-any.whl.metadata (1.7 kB)\n","Collecting tobler>=0.12.1 (from pysal)\n","  Downloading tobler-0.12.1-py3-none-any.whl.metadata (1.9 kB)\n","Collecting mapclassify>=2.8.1 (from pysal)\n","  Downloading mapclassify-2.8.1-py3-none-any.whl.metadata (2.8 kB)\n","Collecting splot>=1.1.7 (from pysal)\n","  Downloading splot-1.1.7-py3-none-any.whl.metadata (8.9 kB)\n","Collecting spopt>=0.6.1 (from pysal)\n","  Downloading spopt-0.6.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.10->pysal) (2.6)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.10->pysal) (4.12.2)\n","Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas>=0.10.0->pysal) (0.10.0)\n","Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas>=0.10.0->pysal) (3.7.0)\n","Collecting quantecon>=0.7 (from giddy>=2.3.6->pysal)\n","  Downloading quantecon-0.7.2-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from inequality>=1.1.1->pysal) (3.10.0)\n","Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.11/dist-packages (from mapclassify>=2.8.1->pysal) (3.4.2)\n","Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.11/dist-packages (from momepy>=0.9.1->pysal) (4.67.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->pysal) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->pysal) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->pysal) (2025.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->pysal) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->pysal) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->pysal) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->pysal) (2025.1.31)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->pysal) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->pysal) (3.5.0)\n","Collecting deprecation (from segregation>=2.5.1->pysal)\n","  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from segregation>=2.5.1->pysal) (0.13.2)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from segregation>=2.5.1->pysal) (0.61.0)\n","Collecting rtree>=1.0 (from spaghetti>=1.7.6->pysal)\n","  Downloading Rtree-1.3.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n","Collecting pulp>=2.7 (from spopt>=0.6.1->pysal)\n","  Downloading PuLP-2.9.0-py3-none-any.whl.metadata (5.4 kB)\n","Collecting rasterio (from tobler>=0.12.1->pysal)\n","  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from tobler>=0.12.1->pysal) (0.14.4)\n","Collecting rasterstats (from tobler>=0.12.1->pysal)\n","  Downloading rasterstats-0.20.0-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->inequality>=1.1.1->pysal) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->inequality>=1.1.1->pysal) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->inequality>=1.1.1->pysal) (4.55.8)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->inequality>=1.1.1->pysal) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->inequality>=1.1.1->pysal) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->inequality>=1.1.1->pysal) (3.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->pysal) (1.17.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from quantecon>=0.7->giddy>=2.3.6->pysal) (1.13.1)\n","Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->segregation>=2.5.1->pysal) (0.44.0)\n","Collecting affine (from rasterio->tobler>=0.12.1->pysal)\n","  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio->tobler>=0.12.1->pysal) (25.1.0)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio->tobler>=0.12.1->pysal) (8.1.8)\n","Collecting cligj>=0.5 (from rasterio->tobler>=0.12.1->pysal)\n","  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n","Collecting click-plugins (from rasterio->tobler>=0.12.1->pysal)\n","  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n","Collecting fiona (from rasterstats->tobler>=0.12.1->pysal)\n","  Downloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting simplejson (from rasterstats->tobler>=0.12.1->pysal)\n","  Downloading simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->tobler>=0.12.1->pysal) (1.0.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->quantecon>=0.7->giddy>=2.3.6->pysal) (1.3.0)\n","Downloading pysal-25.1-py3-none-any.whl (17 kB)\n","Downloading esda-2.7.0-py3-none-any.whl (142 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.8/142.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading libpysal-4.12.1-py3-none-any.whl (2.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading access-1.1.9-py3-none-any.whl (21 kB)\n","Downloading giddy-2.3.6-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading inequality-1.1.1-py3-none-any.whl (29 kB)\n","Downloading mapclassify-2.8.1-py3-none-any.whl (59 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mgwr-2.2.1-py3-none-any.whl (47 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading momepy-0.9.1-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pointpats-2.5.1-py3-none-any.whl (59 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading segregation-2.5.2-py3-none-any.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.6/141.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading spaghetti-1.7.6-py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading spglm-1.1.0-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading splot-1.1.7-py3-none-any.whl (39 kB)\n","Downloading spopt-0.6.1-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.1/243.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading spreg-1.8.2-py3-none-any.whl (388 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.2/388.2 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tobler-0.12.1-py3-none-any.whl (28 kB)\n","Downloading PuLP-2.9.0-py3-none-any.whl (17.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading quantecon-0.7.2-py3-none-any.whl (215 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.4/215.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Rtree-1.3.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (543 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m543.2/543.2 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n","Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rasterstats-0.20.0-py3-none-any.whl (17 kB)\n","Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n","Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n","Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n","Downloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: spint\n","  Building wheel for spint (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for spint: filename=spint-1.0.7-py3-none-any.whl size=31355 sha256=0118ca7dd7e3a9489ba84775499bac438cb4bbfe8fd4667672229b5eb86f9259\n","  Stored in directory: /root/.cache/pip/wheels/32/dc/2e/400caaa67e697355772a82b77b8c2ac7cd61633f595c477fd8\n","Successfully built spint\n","Installing collected packages: simplejson, rtree, pulp, deprecation, cligj, click-plugins, affine, rasterio, quantecon, fiona, rasterstats, mapclassify, libpysal, access, tobler, spreg, segregation, pointpats, momepy, inequality, esda, spglm, spaghetti, giddy, spopt, splot, spint, mgwr, pysal\n","Successfully installed access-1.1.9 affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 deprecation-2.1.0 esda-2.7.0 fiona-1.10.1 giddy-2.3.6 inequality-1.1.1 libpysal-4.12.1 mapclassify-2.8.1 mgwr-2.2.1 momepy-0.9.1 pointpats-2.5.1 pulp-2.9.0 pysal-25.1 quantecon-0.7.2 rasterio-1.4.3 rasterstats-0.20.0 rtree-1.3.0 segregation-2.5.2 simplejson-3.19.3 spaghetti-1.7.6 spglm-1.1.0 spint-1.0.7 splot-1.1.7 spopt-0.6.1 spreg-1.8.2 tobler-0.12.1\n","Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/scipy/sparse/_data.py:133: RuntimeWarning: divide by zero encountered in reciprocal\n","  return self._with_data(data ** n)\n"]},{"output_type":"stream","name":"stdout","text":["96\n","0\n","0.41244611144065857 3.234973192214966 0.004845290444791317 0.09295616510137938 15.31489821805663\n","1\n","0.4756087362766266 5.137279033660889 0.006605035159736872 0.1126303458528205 17.35967700445132\n","2\n","0.46411845088005066 4.807016849517822 0.0052484008483588696 0.11867170937686833 20.030145238013347\n","3\n","0.4109136164188385 1.8713587522506714 0.004753814544528723 0.07353335109748516 14.004296591346023\n","4\n","0.4574815332889557 2.1204352378845215 0.006328877527266741 0.15942173560237372 18.163600598456103\n","5\n","0.43217960000038147 3.619762897491455 0.005394412204623222 0.1320820239490239 16.253458495174357\n","6\n","0.42654043436050415 2.1800849437713623 0.006932769902050495 0.1588270256383646 20.865191616641322\n","7\n","0.42983168363571167 2.1762712001800537 0.006385400891304016 0.20187627314401457 23.051021066221715\n","8\n","0.4283880591392517 3.2283921241760254 0.006746976636350155 0.21023120641420479 19.8738785977948\n","9\n","0.44991153478622437 3.997163772583008 0.007419520057737827 0.29695198016152613 21.904517181174498\n","10\n","0.3941279947757721 2.4841554164886475 0.006417983211576939 0.25798652575751657 17.769080005411876\n","11\n","0.44676414132118225 3.3951523303985596 0.006945568136870861 0.3934038359784162 19.889020622312458\n","12\n","0.3598659932613373 4.169217109680176 0.006979999132454395 0.6425893417646292 16.162526428437417\n","13\n","0.36467841267585754 2.4213860034942627 0.007378054782748222 0.47541960046597687 21.87771451262387\n","14\n","0.4218708574771881 3.230780601501465 0.005886307917535305 0.3831159356150008 24.786682550320077\n","15\n","0.4816887378692627 3.1528449058532715 0.009235923178493977 1.163270735237397 41.704191250800875\n","16\n","0.4611056447029114 2.7095885276794434 0.009230555966496468 1.250213126749905 35.80435199546308\n","17\n","0.44560807943344116 5.741547107696533 0.00912991352379322 1.6122905027091867 35.044368714158345\n","18\n","0.4423445463180542 2.067880153656006 0.007125630043447018 0.7683852311790873 32.66082371740361\n","19\n","0.4218602478504181 2.611858367919922 0.008776954375207424 1.360872692805147 30.109898555315482\n","20\n","0.3865087628364563 2.687741279602051 0.009796200320124626 1.0324858657922826 33.71961687579176\n","21\n","0.33140870928764343 2.6594619750976562 0.0063969772309064865 0.3907496696066436 17.43150716828341\n","22\n","0.40511554479599 5.187912940979004 0.007054209243506193 0.4869095147379624 20.977475664583224\n","23\n","0.3487064838409424 2.3282620906829834 0.0072744074277579784 0.4387045489900152 23.1641392004802\n","24\n","0.3412771224975586 1.3407573699951172 0.007435886189341545 0.2973473303590951 20.121872897072414\n","25\n","0.369843453168869 4.493628025054932 0.007225976325571537 0.3604954839699037 16.59608666094155\n","26\n","0.4698840379714966 6.283966064453125 0.008180261589586735 0.26515468487304883 15.907097445292731\n","27\n","0.46904873847961426 6.123181343078613 0.007145993411540985 0.23625059490019618 25.68548557428067\n","28\n","0.428096741437912 2.077699661254883 0.00545652536675334 0.20917914047804043 18.85116815480331\n","29\n","0.42920422554016113 2.5483570098876953 0.0051825521513819695 0.10764707803704521 18.581290588114953\n","30\n","0.4985601305961609 3.6947622299194336 0.008122703060507774 0.22632411360852922 31.55237412627412\n","31\n","0.43521806597709656 6.306893825531006 0.0057897972874343395 0.1496019708407914 28.044751170221524\n","32\n","0.4352589249610901 4.1965131759643555 0.007576377131044865 0.14279421689438188 26.941593044807586\n","33\n","0.44630858302116394 1.816482663154602 0.006301568355411291 0.09294393315379856 21.31644045173861\n","34\n","0.43931329250335693 1.8784970045089722 0.0050447494722902775 0.07850395257499398 16.56365576045333\n","35\n","0.4522015452384949 2.7519521713256836 0.007122991606593132 0.1544796773072622 21.44255804913187\n","36\n","0.45721548795700073 4.885073661804199 0.008135588839650154 0.2422522710223236 22.927586556279124\n","37\n","0.4638046324253082 3.3125929832458496 0.00829769391566515 0.34962887103051177 22.93805871592859\n","38\n","0.43708086013793945 2.682868003845215 0.009672919288277626 0.6170371442998871 21.065294791774686\n","39\n","0.39787113666534424 1.1418354511260986 0.004917202517390251 0.20608387513679716 16.62059415908573\n","40\n","0.3652319312095642 2.392781972885132 0.0071289753541350365 0.41904102768954044 20.082037110390676\n","41\n","0.40638700127601624 5.773066520690918 0.006953163538128138 0.5986539748656559 21.781317412229814\n","42\n","0.38875579833984375 2.6331028938293457 0.007220414467155933 0.46965913392544345 17.336852102960982\n","43\n","0.356387197971344 2.3521547317504883 0.008091885596513748 1.1950205099963305 20.26130687120841\n","44\n","0.40484529733657837 3.4134762287139893 0.00710390554741025 0.4451225398425218 26.432536653679836\n","45\n","0.3586617708206177 4.676624774932861 0.008812790736556053 1.6457379971890789 25.777835857023987\n","46\n","0.4686894416809082 3.8956990242004395 0.009370166808366776 1.5016495635545795 36.41273774453231\n","47\n","0.43667176365852356 3.056952476501465 0.006427236367017031 0.6561184189850662 30.738941270403807\n","48\n","0.40843498706817627 1.7479639053344727 0.008339582942426205 1.4436209211765971 27.655197469917415\n","49\n","0.4471176862716675 4.77644157409668 0.005988475400954485 0.6534045515536315 24.78693111784206\n","50\n","0.4148247241973877 3.4765748977661133 0.006441840901970863 0.39234989425352085 22.721648889912387\n","51\n","0.42475414276123047 2.1219632625579834 0.006374214310199022 0.5918308675786025 27.114974686259096\n","52\n","0.34901684522628784 1.3414348363876343 0.006919693201780319 0.7172200785168995 23.21392838542473\n","53\n","0.4220622777938843 2.269475221633911 0.006972248665988445 0.5055212667607205 31.918447005898052\n","54\n","0.34919777512550354 3.3902392387390137 0.007128242868930101 0.518773016878028 18.278922944285654\n","55\n","0.39924031496047974 1.4540507793426514 0.0056517599150538445 0.23417032877258637 21.70095592548784\n","56\n","0.42246758937835693 2.800673723220825 0.007594043854624033 0.5144708307740117 22.442609279920795\n","57\n","0.414855420589447 2.5350613594055176 0.005398905370384455 0.15392888514229797 18.996859079402377\n","58\n","0.40910956263542175 2.8094983100891113 0.005723228678107262 0.14481235316211116 18.851808347292952\n","59\n","0.44112223386764526 2.1593472957611084 0.006335506681352854 0.14379477139714303 17.08008389352601\n","60\n","0.4665743410587311 1.6140737533569336 0.006396716460585594 0.11003462321706325 19.06052487195887\n","61\n","0.4107055068016052 2.972895860671997 0.004636074882000685 0.054745914268777174 12.396380376646965\n","62\n","0.4478922486305237 3.723876714706421 0.0053622107952833176 0.07620315750107753 18.086411873855592\n","63\n","0.4822740852832794 4.197506904602051 0.008226284757256508 0.16947336503398425 19.435321026316423\n","64\n","0.4024684429168701 2.1994516849517822 0.00475392397493124 0.10470319164211916 12.34188357174055\n","65\n","0.42768213152885437 3.311250686645508 0.004445837344974279 0.06493334087452263 11.50382789415521\n","66\n","0.4034596085548401 4.13340950012207 0.004793204832822084 0.09614149596039556 11.935299050468014\n","67\n","0.4156860411167145 1.8235363960266113 0.005949028767645359 0.2242230879230136 20.571492168100043\n","68\n","0.4385666251182556 5.219823837280273 0.00684693269431591 0.24448414736334434 15.466153292984256\n","69\n","0.40657851099967957 2.9676003456115723 0.006156843155622482 0.14236656956002713 17.75665974038585\n","70\n","0.419765830039978 2.468510627746582 0.005668335594236851 0.18235687742422568 17.278484311339255\n","71\n","0.3627575933933258 5.1482133865356445 0.007028794381767511 0.39969029624952646 17.94918346601846\n","72\n","0.337994784116745 2.4670650959014893 0.006052922457456589 0.29960337975175066 18.564662774545337\n","73\n","0.43145090341567993 3.4907937049865723 0.006594017613679171 0.42304294593802416 23.52468615500992\n","74\n","0.3998275399208069 3.5863561630249023 0.006506297737360001 0.336787385464554 21.040286337115337\n","75\n","0.3742761015892029 3.631664514541626 0.009643803350627422 1.1798034851700794 24.8625056533689\n","76\n","0.4744490683078766 2.067966938018799 0.006868577096611261 0.5699552974878764 30.241866063596937\n","77\n","0.45240095257759094 2.4620919227600098 0.006942878011614084 0.7678499696954703 34.82381990574912\n","78\n","0.4347692131996155 3.0563883781433105 0.008389642462134361 1.219979207778065 33.2425043598823\n","79\n","0.43903863430023193 4.3468241691589355 0.006097395904362202 0.5808630370237201 25.451040075713955\n","80\n","0.4097672998905182 2.5388479232788086 0.008479333482682705 1.7084648047642672 36.3717402987502\n","81\n","0.4312686324119568 2.698007822036743 0.0060629588551819324 0.4898839375252111 26.24496749281507\n","82\n","0.41867512464523315 2.2195427417755127 0.006811829749494791 0.5455300060429439 26.519094891134884\n","83\n","0.3178476393222809 3.0655269622802734 0.0061523159965872765 0.5150438308258809 23.22806680573526\n","84\n","0.36136505007743835 1.5652427673339844 0.007266707252711058 0.5458956291799816 27.308479354573755\n","85\n","0.3674929440021515 2.256939172744751 0.006201399490237236 0.3116376920796411 15.450021670513422\n","86\n","0.3266189396381378 7.139486789703369 0.006597362924367189 0.4781894339595507 16.259983477272264\n","87\n","0.4877210259437561 11.235217094421387 0.007610163185745478 0.2797954529926069 17.344385483649972\n","88\n","0.43506938219070435 1.725226640701294 0.005695212632417679 0.26356124975347495 21.65735672222901\n","89\n","0.4424996078014374 5.485968112945557 0.006074059288948774 0.1282910004101241 19.14938436832498\n","90\n","0.4844810962677002 3.393848180770874 0.007871430367231369 0.16716899322567325 19.6008048830786\n","91\n","0.4346456527709961 1.3659700155258179 0.006460635457187891 0.22324064861964066 20.055654740804957\n","92\n","0.426139235496521 1.8454711437225342 0.005246882326900959 0.13533981823676244 18.793009802523798\n","93\n","0.45395487546920776 5.091089725494385 0.0069558266550302505 0.19262775216036054 20.203918240822816\n","94\n","0.450228214263916 2.226306676864624 0.005529338028281927 0.18592363766725273 14.61413297168049\n","95\n","0.4472568929195404 3.5665626525878906 0.007285528350621462 0.20108416455656475 17.445642372786818\n","0.4195910878479481 0.040011568159005216\n","3.2722992238899073 1.525010975905557\n","0.006807655799396646 0.0012330499997629638\n","0.4428450316828724 0.39592589668691636\n","22.10878706883504 6.150573590522514\n"]}],"source":["# @title Implement evaluation\n","dim = 0 # dim=0 (or 1 only for 'WeatherBench-surface wind components')\n","\n","!pip install pysal esda libpysal\n","\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import os\n","import torch\n","import pickle\n","\n","from libpysal.weights import DistanceBand\n","from scipy.spatial import distance_matrix\n","from esda.moran import Moran\n","import statsmodels.api as sm\n","from scipy.stats import zscore\n","from libpysal.weights import DistanceBand, block_weights\n","import copy\n","\n","dataName = 'tmp2m'\n","with_concatenate = 0\n","\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir('/content/drive/My Drive/CLCRN-main')\n","\n","if wavelet_decomposition_dimension == 2:\n","    fileName = f\"FABLE_tmp2m_{clampEpsilon}_{stepNum}_2d_{fix}_{implementations}\"\n","elif wavelet_decomposition_dimension == 3:\n","    fileName = f\"FABLE_tmp2m_{clampEpsilon}_{stepNum}_3d_{fix}_{implementations}\"\n","\n","df = pd.read_csv('results/' + fileName + \".csv\")\n","\n","gPrecision = []\n","inPrecision = []\n","outPrecision = []\n","closeness = []\n","sMoranDiffAvgX = []\n","tAutoDiffAvgX = []\n","\n","sMoranDiffConcatX = []\n","tAutoDiffConcatX = []\n","\n","sMoranXListAll = []\n","sMoranXAdvListAll = []\n","\n","def spatialMoranI(X, W):\n","    moran = Moran(X, W)\n","\n","    return moran.I\n","\n","def temporalAutocorrelation(timeSeries):\n","    acfValues = sm.tsa.acf(timeSeries)\n","\n","    if not np.isnan(acfValues).any():\n","        return acfValues\n","    else:\n","        return np.zeros(len(acfValues))\n","\n","def expand_lonlat(lonlat, T, delta=1000):\n","    N = lonlat.shape[0]\n","    lonlat_expanded = np.tile(lonlat, (T, 1))  # shape: (N*T, 2)\n","\n","    for t in range(T):\n","        lonlat_expanded[t*N:(t+1)*N, 1] += t * delta\n","\n","    return lonlat_expanded\n","\n","lonlatAddr = 'data/' + dataName + '/position_info.pkl'\n","with open(lonlatAddr, 'rb') as lonLatPkl:\n","    lonLatDict = pickle.load(lonLatPkl)\n","lonlat = lonLatDict['lonlat']\n","lonlat_expanded = expand_lonlat(lonlat, 12)\n","\n","threshold = 1                                                                   # This is a hyperparameter\n","W = DistanceBand(lonlat, threshold=threshold, binary=False, silence_warnings=True) # w.n: 1320*1320\n","if with_concatenate == 1:\n","    W_concat = DistanceBand(lonlat_expanded, threshold=threshold, binary=False, silence_warnings=True)\n","\n","print(len(df['thisX']))\n","for sampleNum in range(len(df['thisX'])): # a list of tensors\n","    print(sampleNum)\n","\n","    ### used for DEBUG\n","    # if sampleNum == 48:\n","    #     break\n","\n","    ### in-target precision\n","    # format of 'inPositions': tensor([[timestep, location], [], ..., []])\n","    inPositions =  torch.nonzero(\n","        torch.tensor(eval(df['yTarget'][sampleNum]))[:, :, dim]\n","        - torch.tensor(eval(df['yPred'][sampleNum]))[:, :, dim])\n","    thisInPrecision = torch.sum(\n","        torch.abs(\n","            torch.tensor(eval(df['yTarget'][sampleNum]))[inPositions[:, 0], inPositions[:, 1], dim]\n","            - torch.tensor(eval(df['yPerturbed'][sampleNum]))[inPositions[:, 0], inPositions[:, 1], dim])\n","        )\n","    inPrecision.append(thisInPrecision.item())\n","\n","    ### out-target precision.\n","    outPositions = torch.nonzero(\n","        (torch.tensor(eval(df['yTarget'][sampleNum]))[:, :, dim]\n","        - torch.tensor(eval(df['yPred'][sampleNum]))[:, :, dim])==0)\n","    thisOutPrecision = torch.sum(\n","        torch.abs(\n","            torch.tensor(eval(df['yTarget'][sampleNum]))[outPositions[:, 0], outPositions[:, 1], dim]\n","            - torch.tensor(eval(df['yPerturbed'][sampleNum]))[outPositions[:, 0], outPositions[:, 1], dim])\n","        )\n","    outPrecision.append(thisOutPrecision.item())\n","\n","    ### closeness\n","    thisCloseness = torch.mean(\n","        torch.abs(torch.tensor(eval(df['thisX'][sampleNum]))[:, :, dim]\n","        - torch.tensor(eval(df['xPerturbed'][sampleNum]))[:, :, dim]))\n","    closeness.append(thisCloseness.item())\n","\n","    ### spatial autocorrelation-averaged\n","    X = torch.tensor(eval(df['thisX'][sampleNum]))[:, :, dim].numpy()           # X.shape: 1320*T\n","    XAdv = torch.tensor(eval(df['xPerturbed'][sampleNum]))[:, :, dim].numpy()\n","    sMoranXList = [spatialMoranI(X[i,:].flatten(), W) for i in range(X.shape[0])] # for each element in the list, the shape is (1320,)\n","    sMoranXAdvList = [spatialMoranI(XAdv[i,:].flatten(), W) for i in range(XAdv.shape[0])]\n","    sMoranXDiffList = [abs(a - b) for a, b in zip(sMoranXList, sMoranXAdvList)]\n","    thisSMoranDiffAvgX = np.sum(sMoranXDiffList)\n","    sMoranDiffAvgX.append(thisSMoranDiffAvgX)\n","    sMoranXListAll.append(sMoranXList)\n","    sMoranXAdvListAll.append(sMoranXAdvList)\n","\n","    ### spatial autocorrelation-concatenated\n","    if with_concatenate == 1:\n","        X_concat = X.flatten()\n","        XAdv_concat = XAdv.flatten()\n","        moran_all_X = Moran(X_concat, W_concat)\n","        moran_all_XAdv = Moran(XAdv_concat, W_concat)\n","        thisSMoranDiffConcatX = abs(moran_all_X.I - moran_all_XAdv.I)\n","        sMoranDiffConcatX.append(thisSMoranDiffConcatX)\n","\n","    ### temporal autocorrelation-averaged\n","    tAutoXList = [temporalAutocorrelation(X[:,i]) for i in range(X.shape[1])]   # For NLDAS-precipitation dataset, there are some n.a. outcomes for temporal autocorrelation. So, we first replace them as all zeros and then take those invalid out\n","    tAutoXNp = np.array(tAutoXList)                                             # len(tAutoXList): 1320. tAutoXNp.shape: (1320, T)\n","    tAutoXAdvList = [temporalAutocorrelation(XAdv[:,i]) for i in range(XAdv.shape[1])]\n","    tAutoXAdvNp = np.array(tAutoXAdvList)\n","    tAutoDiffNp = np.abs(tAutoXNp - tAutoXAdvNp)                                # tAutoDiffNp.shape: (1320, T)\n","    thisTAutoDiffAvgX = np.mean(tAutoDiffNp, axis=1)\n","    thisTAutoDiffAvgX = np.sum(thisTAutoDiffAvgX)\n","    tAutoDiffAvgX.append(thisTAutoDiffAvgX)\n","\n","    ### temporal autocorrelation-concatenated\n","    if with_concatenate == 1:\n","        X_concat    = X.flatten()                                                   # shape: (N*T,)\n","        XAdv_concat = XAdv.flatten()                                                # shape: (N*T,)\n","        acf_X    = temporalAutocorrelation(X_concat)\n","        acf_XAdv = temporalAutocorrelation(XAdv_concat)\n","        acf_diff = np.abs(acf_X - acf_XAdv)\n","        thisTAutoDiffConcatX = np.sum(acf_diff)\n","        tAutoDiffConcatX.append(thisTAutoDiffConcatX)\n","\n","    if with_concatenate == 1:\n","        print(thisInPrecision.item(), thisOutPrecision.item(), thisCloseness.item(), thisSMoranDiffAvgX, thisSMoranDiffConcatX, thisTAutoDiffAvgX, thisTAutoDiffConcatX)\n","    else:\n","        print(thisInPrecision.item(), thisOutPrecision.item(), thisCloseness.item(), thisSMoranDiffAvgX, thisTAutoDiffAvgX)\n","\n","print(np.mean(inPrecision), np.std(inPrecision))\n","print(np.mean(outPrecision), np.std(outPrecision))\n","print(np.mean(closeness), np.std(closeness))\n","print(np.mean(sMoranDiffAvgX), np.std(sMoranDiffAvgX))\n","print(np.mean(tAutoDiffAvgX), np.std(tAutoDiffAvgX))\n","\n","if with_concatenate == 1:\n","    print(np.mean(sMoranDiffConcatX), np.std(sMoranDiffConcatX))\n","    print(np.mean(tAutoDiffConcatX), np.std(tAutoDiffConcatX))\n","\n","if with_concatenate == 1:\n","    dict = {\n","        'In-Target Precision': inPrecision,\n","        'Out-Target Precision': outPrecision,\n","        'Closeness': closeness,\n","        'Spatial Autocorrelation X Averaged': sMoranDiffAvgX,\n","        'Spatial Autocorrelation X Concatenated': sMoranDiffConcatX,\n","        'Temporal Autocorrelation X Averaged': tAutoDiffAvgX,\n","        'Temporal Autocorrelation X Concatenated': tAutoDiffConcatX,\n","        'Moran X': sMoranXListAll,\n","        'Moran XAdv': sMoranXAdvListAll\n","    }\n","else:\n","    dict = {\n","      'In-Target Precision': inPrecision,\n","      'Out-Target Precision': outPrecision,\n","      'Closeness': closeness,\n","      'Spatial Autocorrelation X Averaged': sMoranDiffAvgX,\n","      'Temporal Autocorrelation X Averaged': tAutoDiffAvgX,\n","      'Moran X': sMoranXListAll,\n","      'Moran XAdv': sMoranXAdvListAll\n","    }\n","\n","df = pd.DataFrame(dict)\n","df.to_csv('results/' + fileName + \"_evaluation\" + \".csv\", index = False)\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyNG3f9bBehtD3s9RkNJKGMR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}